<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>content on Kubernetes Community</title>
    <link>/</link>
    <description>Recent content in content on Kubernetes Community</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>0000-kep-template</title>
      <link>/keps/0000-kep-template/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/keps/0000-kep-template/</guid>
      <description>kep-number: 0 title: My First KEP authors: - &amp;ldquo;@janedoe&amp;rdquo; owning-sig: sig-xxx participating-sigs: - sig-aaa - sig-bbb reviewers: - TBD - &amp;ldquo;@alicedoe&amp;rdquo; approvers: - TBD - &amp;ldquo;@oscardoe&amp;rdquo; editor: TBD creation-date: yyyy-mm-dd last-updated: yyyy-mm-dd status: provisional see-also: - KEP-1 - KEP-2 replaces: - KEP-3 superseded-by:
- KEP-100 Title This is the title of the KEP. Keep it simple and descriptive. A good title can help communicate what the KEP is and should be considered as part of any review.</description>
    </item>
    
    <item>
      <title>0001-kubernetes-enhancement-proposal-process</title>
      <link>/keps/0001-kubernetes-enhancement-proposal-process/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/keps/0001-kubernetes-enhancement-proposal-process/</guid>
      <description>kep-number: 1 title: Kubernetes Enhancement Proposal Process authors: - &amp;ldquo;@calebamiles&amp;rdquo; - &amp;ldquo;@jbeda&amp;rdquo; owning-sig: sig-architecture participating-sigs: - kubernetes-wide reviewers: - name: &amp;ldquo;@timothysc&amp;rdquo; approvers: - name: &amp;ldquo;@bgrant0607&amp;rdquo; editor: name: &amp;ldquo;@jbeda&amp;rdquo; creation-date: 2017-08-22
status: implementable Kubernetes Enhancement Proposal Process Table of Contents  Kubernetes Enhancement Proposal Process  Metadata Table of Contents Summary Motivation Reference-level explanation  What type of work should be tracked by a KEP KEP Template KEP Metadata KEP Workflow Git and GitHub Implementation KEP Editor Role Important Metrics Prior Art  Graduation Criteria Drawbacks Alternatives Unresolved Questions Mentors   Summary A standardized development process for Kubernetes is proposed in order to</description>
    </item>
    
    <item>
      <title>0002-cloud-controller-manager</title>
      <link>/keps/sig-cloud-provider/0002-cloud-controller-manager/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/keps/sig-cloud-provider/0002-cloud-controller-manager/</guid>
      <description>kep-number: 2 title: Cloud Provider Controller Manager authors: - &amp;ldquo;@cheftako&amp;rdquo; - &amp;ldquo;@calebamiles&amp;rdquo; - &amp;ldquo;@hogepodge&amp;rdquo; owning-sig: sig-apimachinery participating-sigs: - sig-apps - sig-aws - sig-azure - sig-cloud-provider - sig-gcp - sig-network - sig-openstack - sig-storage reviewers: - &amp;ldquo;@andrewsykim&amp;rdquo; - &amp;ldquo;@calebamiles&amp;rdquo; - &amp;ldquo;@hogepodge&amp;rdquo; - &amp;ldquo;@jagosan&amp;rdquo; approvers: - &amp;ldquo;@thockin&amp;rdquo; editor: TBD status: provisional replaces:
- contributors/design-proposals/cloud-provider/cloud-provider-refactoring.md Remove Cloud Provider Code From Kubernetes Core Table of Contents  Remove Cloud Provider Code From Kubernetes Core  Table of Contents Summary Motivation  Goals Intermediary Goals Non-Goals  Proposal  Controller Manager Changes Kubelet Changes API Server Changes Volume Management Changes Deployment Changes Implementation Details/Notes/Constraints  Repository Requirements  Notes for Repository Requirements Repository Timeline   Security Considerations  Graduation Criteria  Graduation to Beta  Process Goals   Implementation History Alternatives   Summary We want to remove any cloud provider specific logic from the kubernetes/kubernetes repo.</description>
    </item>
    
    <item>
      <title>0003-cluster-api</title>
      <link>/keps/sig-cluster-lifecycle/0003-cluster-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/keps/sig-cluster-lifecycle/0003-cluster-api/</guid>
      <description>kep-number: 3 title: Kubernetes Cluster Management API status: provisional authors: - &amp;ldquo;@roberthbailey&amp;rdquo; - &amp;ldquo;@pipejakob&amp;rdquo; owning-sig: sig-cluster-lifecycle reviewers: - &amp;ldquo;@thockin&amp;rdquo; approvers: - &amp;ldquo;@roberthbailey&amp;rdquo; editor: - &amp;ldquo;@roberthbailey&amp;rdquo; creation-date: 2018-01-19
last-updated: 2018-01-22 Kubernetes Cluster Management API Table of Contents  Kubernetes Cluster Management API  Metadata Table of Contents Summary Motivation Goals Non-goals Challenges and Open Questions Proposal Driving Use Cases Cluster-level API Machine API  Capabilities Overview In-place vs. Replace Omitted Capabilities Conditions Types  Graduation Criteria Implementation History Drawbacks Alternatives   Summary We are building a set of Kubernetes cluster management APIs to enable common cluster lifecycle operations (install, upgrade, repair, delete) across disparate environments.</description>
    </item>
    
    <item>
      <title>0003-testgrid-conformance-e2e</title>
      <link>/keps/sig-cloud-provider/0003-testgrid-conformance-e2e/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/keps/sig-cloud-provider/0003-testgrid-conformance-e2e/</guid>
      <description>kep-number: 003 title: Reporting Conformance Test Results to Testgrid authors: - &amp;ldquo;@andrewsykim&amp;rdquo; owning-sig: sig-cloud-provider participating-sigs: - sig-testing - sig-release - sig-aws - sig-azure - sig-gcp - sig-ibmcloud - sig-openstack - sig-vmware reviewers: - TBD approvers: - TBD editor: TBD creation-date: 2018-06-06 last-updated: 2018-06-06 status: provisional
Reporting Conformance Test Results to Testgrid Table of Contents  Summary Motivation  Goals Non-Goals  Proposal  Implementation Details/Notes/Constraints Risks and Mitigations  Graduation Criteria Implementation History  Summary This is a KEP outlining the motivation behind why cloud providers should periodically upload E2E conformance test results to Testgrid and how a cloud provider can go about doing this.</description>
    </item>
    
    <item>
      <title>0004-bootstrap-checkpointing</title>
      <link>/keps/sig-cluster-lifecycle/0004-bootstrap-checkpointing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/keps/sig-cluster-lifecycle/0004-bootstrap-checkpointing/</guid>
      <description>kep-number: 4 title: Kubernetes Bootstrap Checkpointing Proposal status: implemented authors: - &amp;ldquo;@timothysc&amp;rdquo; owning-sig: sig-cluster-lifecycle participating-sigs: - sig-node reviewers: - &amp;ldquo;@yujuhong&amp;rdquo; - &amp;ldquo;@luxas&amp;rdquo; - &amp;ldquo;@roberthbailey&amp;rdquo; approvers: - &amp;ldquo;@yujuhong&amp;rdquo; - &amp;ldquo;@roberthbailey&amp;rdquo; editor: name: &amp;ldquo;@timothysc&amp;rdquo; creation-date: 2017-10-20
last-updated: 2018-01-23 Kubernetes Bootstrap Checkpointing Proposal Table of Contents  Summary Objectives  Goals Non-Goals  Proposal  User Stories  Graduation Criteria Implementation History Unresolved Questions  Summary There are several methods to deploy a kubernetes cluster, one method that offers some unique advantages is self hosting.</description>
    </item>
    
    <item>
      <title>0004-cloud-provider-template</title>
      <link>/keps/sig-cloud-provider/providers/0004-cloud-provider-template/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/keps/sig-cloud-provider/providers/0004-cloud-provider-template/</guid>
      <description>kep-number: 4 title: Cloud Provider Template authors: - &amp;ldquo;@janedoe&amp;rdquo; owning-sig: sig-cloud-provider participating-sigs: - sig-aaa - sig-bbb reviewers: - TBD - &amp;ldquo;@alicedoe&amp;rdquo; approvers: - &amp;ldquo;@andrewsykim&amp;rdquo; - &amp;ldquo;@hogepodge&amp;rdquo; - &amp;ldquo;@jagosan&amp;rdquo; editor: TBD creation-date: yyyy-mm-dd last-updated: yyyy-mm-dd status: provisional see-also: - KEP-1 - KEP-2 replaces: - KEP-3 superseded-by:
- KEP-100 Cloud Provider FooBar This is a KEP template, outlining how to propose a new cloud provider into the Kubernetes ecosystem.
Table of Contents  Table of Contents Summary Motivation  Goals Non-Goals  Requirements Proposal  Summary This is where you add a summary of your cloud provider and other additional information about your cloud provider that others may find useful.</description>
    </item>
    
    <item>
      <title>0005-contributor-site</title>
      <link>/keps/sig-contributor-experience/0005-contributor-site/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/keps/sig-contributor-experience/0005-contributor-site/</guid>
      <description>kep-number: 5 title: Contributor Site authors: - &amp;ldquo;@jbeda&amp;rdquo; owning-sig: sig-contributor-experience participating-sigs: - sig-architecture - sig-docs reviewers: - &amp;ldquo;@castrojo&amp;rdquo; approvers: - &amp;ldquo;@parispittman&amp;rdquo; editor: TBD creation-date: &amp;ldquo;2018-02-19&amp;rdquo; last-updated: &amp;ldquo;2018-03-07&amp;rdquo;
status: implementable Contributor Site Table of Contents  Table of Contents Summary Motivation  Goals Non-Goals  Proposal  Risks and Mitigations  Graduation Criteria Implementation History Drawbacks Alternatives  Summary We need a way to organize and publish information targeted at contributors.</description>
    </item>
    
    <item>
      <title>0006-apply</title>
      <link>/keps/0006-apply/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/keps/0006-apply/</guid>
      <description>kep-number: 6 title: Apply authors: - &amp;ldquo;@lavalamp&amp;rdquo; owning-sig: sig-api-machinery participating-sigs: - sig-api-machinery - sig-cli reviewers: - &amp;ldquo;@pwittrock&amp;rdquo; - &amp;ldquo;@erictune&amp;rdquo; approvers: - &amp;ldquo;@bgrant0607&amp;rdquo; editor: TBD creation-date: 2018-03-28 last-updated: 2018-03-28 status: provisional see-also: - n/a replaces: - n/a superseded-by:
- n/a Apply Table of Contents  Apply  Table of Contents Summary Motivation  Goals Non-Goals  Proposal  Implementation Details/Notes/Constraints [optional] Risks and Mitigations  Graduation Criteria Implementation History Drawbacks Alternatives   Summary kubectl apply is a core part of the Kubernetes config workflow, but it is buggy and hard to fix.</description>
    </item>
    
    <item>
      <title>0007-20180403-community-forum</title>
      <link>/keps/sig-contributor-experience/0007-20180403-community-forum/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/keps/sig-contributor-experience/0007-20180403-community-forum/</guid>
      <description>kep-number: 0007 title: A community forum for Kubernetes authors: - &amp;ldquo;@castrojo&amp;rdquo; owning-sig: sig-contributor-experience participating-sigs: reviewers: - &amp;ldquo;@jberkus&amp;rdquo; - &amp;ldquo;@joebeda&amp;rdquo; - &amp;ldquo;@cblecker&amp;rdquo; approvers: - &amp;ldquo;@parispittman&amp;rdquo; - &amp;ldquo;@grodrigues3&amp;rdquo;
editor: TBD creation-date: 2018-04-03 last-updated: 2018-04-17
status: provisional A community forum for Kubernetes Table of Contents  Table of Contents Summary Motivation  Goals Non-Goals  Proposal  User Stories [optional]  Story 1 Story 2  Implementation Details/Notes/Constraints Risks and Mitigations  Graduation Criteria Implementation History Drawbacks  Summary Kubernetes is large enough that we should take a more active role in growing our community.</description>
    </item>
    
    <item>
      <title>0007-pod-ready&#43;&#43;</title>
      <link>/keps/sig-network/0007-pod-ready&#43;&#43;/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/keps/sig-network/0007-pod-ready&#43;&#43;/</guid>
      <description>kep-number: 1 title: Pod Ready++ authors: - &amp;ldquo;freehan@&amp;rdquo; owning-sig: sig-network participating-sigs: - sig-node - sig-cli reviewers: - thockin@ - dchen1107@ approvers: - thockin@ - dchen1107@ editor: freehan@ creation-date: 2018-04-01 last-updated: 2018-04-01 status: provisional
Pod Ready++ Table of Contents  Table of Contents Summary Motivation  Goals Non-Goals  Proposal  Implementation Details/Notes/Constraints Risks and Mitigations  Graduation Criteria Implementation History Alternatives  Summary This proposal aims to add extensibility to pod readiness.</description>
    </item>
    
    <item>
      <title>0008-20180430-promote-sysctl-annotations-to-fields</title>
      <link>/keps/sig-node/0008-20180430-promote-sysctl-annotations-to-fields/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/keps/sig-node/0008-20180430-promote-sysctl-annotations-to-fields/</guid>
      <description>kep-number: 8 title: Protomote sysctl annotations to fields authors: - &amp;ldquo;@ingvagabund&amp;rdquo; owning-sig: sig-node participating-sigs: - sig-auth reviewers: - &amp;ldquo;@sjenning&amp;rdquo; - &amp;ldquo;@derekwaynecarr&amp;rdquo; approvers: - &amp;ldquo;@sjenning &amp;ldquo; - &amp;ldquo;@derekwaynecarr&amp;rdquo; editor: creation-date: 2018-04-30 last-updated: 2018-05-02 status: provisional see-also: replaces:
superseded-by: Promote sysctl annotations to fields Table of Contents  Promote sysctl annotations to fields  Table of Contents Summary Motivation  Promote annotations to fields Promote &amp;ndash;experimental-allowed-unsafe-sysctls kubelet flag to kubelet config api option Gate the feature  Proposal  User Stories Implementation Details/Notes/Constraints Risks and Mitigations  Graduation Criteria Implementation History   Summary Setting the sysctl parameters through annotations provided a successful story for defining better constraints of running applications.</description>
    </item>
    
    <item>
      <title>0008-20180504-kubeadm-config-beta</title>
      <link>/keps/sig-cluster-lifecycle/0008-20180504-kubeadm-config-beta/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/keps/sig-cluster-lifecycle/0008-20180504-kubeadm-config-beta/</guid>
      <description>kep-number: draft-20180412 title: Kubeadm Config Draft authors: - &amp;ldquo;@liztio&amp;rdquo; owning-sig: sig-cluster-lifecycle participating-sigs: [] reviewers: - &amp;ldquo;@timothysc&amp;rdquo; approvers: - TBD editor: TBD creation-date: 2018-04-12 last-updated: 2018-04-12 status: draft see-also: [] replaces: []
superseded-by: [] Kubeadm Config to Beta Table of Contents A table of contents is helpful for quickly jumping to sections of a KEP and for highlighting any additional information provided beyond the standard KEP template.
Table of Contents</description>
    </item>
    
    <item>
      <title>0008-kustomize</title>
      <link>/keps/sig-cli/0008-kustomize/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/keps/sig-cli/0008-kustomize/</guid>
      <description>kep-number: 8 title: Kustomize authors: - &amp;ldquo;@pwittrock&amp;rdquo; - &amp;ldquo;@monopole&amp;rdquo; owning-sig: sig-cli participating-sigs: - sig-cli reviewers: - &amp;ldquo;@droot&amp;rdquo; approvers: - &amp;ldquo;@soltysh&amp;rdquo; editor: &amp;ldquo;@droot&amp;rdquo; creation-date: 2018-05-05 last-updated: 2018-05-23 status: implemented see-also: - n/a replaces: - kinflate # Old name for kustomize superseded-by:
- n/a Kustomize Table of Contents  Kustomize  Table of Contents Summary Motivation  Goals Non-Goals  Proposal  Implementation Details/Notes/Constraints [optional] Risks and Mitigations Risks of Not Having a Solution  Graduation Criteria Implementation History Drawbacks Alternatives FAQ   Summary Declarative specification of Kubernetes objects is the recommended way to manage Kubernetes production workloads, however gaps in the kubectl tooling force users to write their own scripting and tooling to augment the declarative tools with preprocessing transformations.</description>
    </item>
    
    <item>
      <title>0009-node-heartbeat</title>
      <link>/keps/sig-node/0009-node-heartbeat/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/keps/sig-node/0009-node-heartbeat/</guid>
      <description>kep-number: 8 title: Efficient Node Heartbeat authors: - &amp;ldquo;@wojtek-t&amp;rdquo; - &amp;ldquo;with input from @bgrant0607, @dchen1107, @yujuhong, @lavalamp&amp;rdquo; owning-sig: sig-node participating-sigs: - sig-scalability - sig-apimachinery - sig-scheduling reviewers: - &amp;ldquo;@deads2k&amp;rdquo; - &amp;ldquo;@lavalamp&amp;rdquo; approvers: - &amp;ldquo;@dchen1107&amp;rdquo; - &amp;ldquo;@derekwaynecarr&amp;rdquo; editor: TBD creation-date: 2018-04-27 last-updated: 2018-04-27 status: implementable see-also: - https://github.com/kubernetes/kubernetes/issues/14733 - https://github.com/kubernetes/kubernetes/pull/14735 replaces: - n/a superseded-by:
- n/a Efficient Node Heartbeats Table of Contents Table of Contents  Efficient Node Heartbeats  Table of Contents Summary Motivation  Goals Non-Goals  Proposal  Risks and Mitigations  Graduation Criteria Implementation History Alternatives  Dedicated “heartbeat” object instead of “leader election” one Events instead of dedicated heartbeat object Reuse the Component Registration mechanisms Split Node object into two parts at etcd level Delta compression in etcd Replace etcd with other database    Summary Node heartbeats are necessary for correct functioning of Kubernetes cluster.</description>
    </item>
    
    <item>
      <title>0010-20180314-coredns-GA-proposal</title>
      <link>/keps/sig-network/0010-20180314-coredns-ga-proposal/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/keps/sig-network/0010-20180314-coredns-ga-proposal/</guid>
      <description>kep-number: 10 title: Graduate CoreDNS to GA authors: - &amp;ldquo;@johnbelamaric&amp;rdquo; - &amp;ldquo;@rajansandeep&amp;rdquo; owning-sig: sig-network participating-sigs: - sig-cluster-lifecycle reviewers: - &amp;ldquo;@bowei&amp;rdquo; - &amp;ldquo;@thockin&amp;rdquo; approvers: - &amp;ldquo;@thockin&amp;rdquo; editor: &amp;ldquo;@rajansandeep&amp;rdquo; creation-date: 2018-03-21 last-updated: 2018-05-18 status: provisional
see-also: https://github.com/kubernetes/community/pull/2167 Graduate CoreDNS to GA Table of Contents  Summary Motivation  Goals Non-Goals  Proposal  User Cases  Graduation Criteria Implementation History  Summary CoreDNS is sister CNCF project and is the successor to SkyDNS, on which kube-dns is based.</description>
    </item>
    
    <item>
      <title>0011-ipvs-proxier</title>
      <link>/keps/sig-network/0011-ipvs-proxier/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/keps/sig-network/0011-ipvs-proxier/</guid>
      <description>kep-number: TBD title: IPVS Load Balancing Mode in Kubernetes status: implemented authors: - &amp;ldquo;@rramkumar1&amp;rdquo; owning-sig: sig-network reviewers: - &amp;ldquo;@thockin&amp;rdquo; - &amp;ldquo;@m1093782566&amp;rdquo; approvers: - &amp;ldquo;@thockin&amp;rdquo; - &amp;ldquo;@m1093782566&amp;rdquo; editor: - &amp;ldquo;@thockin&amp;rdquo; - &amp;ldquo;@m1093782566&amp;rdquo;
creation-date: 2018-03-21 IPVS Load Balancing Mode in Kubernetes Note: This is a retroactive KEP. Credit goes to @m1093782566, @haibinxie, and @quinton-hoole for all information &amp;amp; design in this KEP.
Important References: https://github.com/kubernetes/community/pull/692/files
Table of Contents  Summary Motivation  Goals Non-goals  Proposal  Kube-Proxy Parameter Changes Build Changes Deployment Changes Design Considerations IPVS service network topology Port remapping Falling back to iptables Supporting NodePort service Supporting ClusterIP service Supporting LoadBalancer service Session Affinity Cleaning up inactive rules Sync loop pseudo code  Graduation Criteria Implementation History Drawbacks Alternatives  Summary We are building a new implementation of kube proxy built on top of IPVS (IP Virtual Server).</description>
    </item>
    
    <item>
      <title>0012-20180518-coredns-default-proposal</title>
      <link>/keps/sig-network/0012-20180518-coredns-default-proposal/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/keps/sig-network/0012-20180518-coredns-default-proposal/</guid>
      <description>kep-number: 11 title: Switch CoreDNS to the default DNS authors: - &amp;ldquo;@johnbelamaric&amp;rdquo; - &amp;ldquo;@rajansandeep&amp;rdquo; owning-sig: sig-network participating-sigs: - sig-cluster-lifecycle reviewers: - &amp;ldquo;@bowei&amp;rdquo; - &amp;ldquo;@thockin&amp;rdquo; approvers: - &amp;ldquo;@thockin&amp;rdquo; editor: &amp;ldquo;@rajansandeep&amp;rdquo; creation-date: 2018-05-18 last-updated: 2018-05-18
status: provisional Switch CoreDNS to the default DNS Table of Contents  Summary Goals Proposal  User Cases  Graduation Criteria Implementation History  Summary CoreDNS is now well-established in Kubernetes as the DNS service, with CoreDNS starting as an alpha feature from Kubernetes v1.</description>
    </item>
    
    <item>
      <title>0015-kubeadm-join-master</title>
      <link>/keps/sig-cluster-lifecycle/0015-kubeadm-join-master/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/keps/sig-cluster-lifecycle/0015-kubeadm-join-master/</guid>
      <description>kubeadm join &amp;ndash;master workflow Metadata --- kep-number: 15 title: kubeadm join --master workflow status: accepted authors: - &amp;#34;@fabriziopandini&amp;#34; owning-sig: sig-cluster-lifecycle reviewers: - &amp;#34;@chuckha” - &amp;#34;@detiber&amp;#34; - &amp;#34;@luxas&amp;#34; approvers: - &amp;#34;@luxas&amp;#34; - &amp;#34;@timothysc&amp;#34; editor: - &amp;#34;@fabriziopandini&amp;#34; creation-date: 2018-01-28 last-updated: 2018-06-29 see-also: - KEP 0004 Table of Contents  kubeadm join &amp;ndash;master workflow  Metadata Table of Contents Summary Motivation  Goals Non-goals Challenges and Open Questions  Proposal  User Stories  Create a cluster with more than one master nodes (static workflow) Add a new master node (dynamic workflow)  Implementation Details  Initialize the Kubernetes cluster Preparing for execution of kubeadm join &amp;ndash;master The kubeadm join &amp;ndash;master workflow dynamic workflow (advertise-address == controlplaneAddress) Static workflow (advertise-address !</description>
    </item>
    
    <item>
      <title>0300-0345_CODEORGANIZATION</title>
      <link>/events/2017/05-leadership-summit/session-notes/0300-0345_codeorganization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/05-leadership-summit/session-notes/0300-0345_codeorganization/</guid>
      <description>Code Organization and Release Process Improvement **Identify the following before beginning your session. Do note move forward until these are decided / assigned: **
 Session Topic: Code Organization and Release Process Improvement Topic Facilitator(s): bgrant0607@ Note-taker(s) (Collaborating on this doc): jdumars@ Person responsible for converting to Markdown &amp;amp; uploading to Github: michellen@  Session Notes Background from November dev summit:  Slides Notes  Github orgs: Github supports 2 levels of hierarchy, orgs and repos, and we need to use both effectively.</description>
    </item>
    
    <item>
      <title>0905-0930_STATE_OF_THE_KUBE</title>
      <link>/events/2017/05-leadership-summit/session-notes/0905-0930_state_of_the_kube/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/05-leadership-summit/session-notes/0905-0930_state_of_the_kube/</guid>
      <description>State of the Kube Identify the following before beginning your session. Do not move forward until these are decided / assigned:
 Session Topic: STATE OF THE KUBE Topic Facilitator(s): Tim Hockin Note-taker(s) (Collaborating on this doc): Jason Singer DuMars, Nilay Yener Person responsible for converting to Markdown &amp;amp; uploading to Github: nyener@  Session Notes  789 companies 15 timezones 41.3 commits daily 2505 devs 100 days between releases 3549 commits since v1.</description>
    </item>
    
    <item>
      <title>0940-1000_PROJECT_GOALS_ROADMAP</title>
      <link>/events/2017/05-leadership-summit/session-notes/0940-1000_project_goals_roadmap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/05-leadership-summit/session-notes/0940-1000_project_goals_roadmap/</guid>
      <description>Project Goals &amp;amp; Roadmap Identify the following before beginning your session. Do not move forward until these are decided / assigned:
 Session Topic: Project Goals and Roadmap Topic Facilitator(s): Apart Sinha, Google &amp;amp; Ihop Dvoretskyi, Mirantis Note-taker(s) (Collaborating on this doc): Jason Singer DuMars, Nilay Yener Person responsible for converting to Markdown &amp;amp; uploading to Github: nyener@  Session Notes (primarily discussion, for the presentation see: Link )</description>
    </item>
    
    <item>
      <title>1015-1115_UPDATES_GOVERNANCE_AND_CNCF</title>
      <link>/events/2017/05-leadership-summit/session-notes/1015-1115_updates_governance_and_cncf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/05-leadership-summit/session-notes/1015-1115_updates_governance_and_cncf/</guid>
      <description>Updates : Governance &amp;amp; CNCF Identify the following before beginning your session. Do not move forward until these are decided / assigned:
 Session Topic: Updates : Governance &amp;amp; CNCF Topic Facilitator(s): Brian Grant, Google &amp;amp; Brandon Philips, CoreOS Note-taker(s) (Collaborating on this doc): Jason Singer DuMars, Nilay Yener Person responsible for converting to Markdown &amp;amp; uploading to Github: nyener@  Session Notes Link
BGrant: Please do not rathole on minutiae, but please comment on the governance documents, we need to understand that this formalizes the escalation path B Philips: Steering committee is focused on watching the details</description>
    </item>
    
    <item>
      <title>1145_1245_ARCHITECTURAL_ROADMAP</title>
      <link>/events/2017/05-leadership-summit/session-notes/1145_1245_architectural_roadmap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/05-leadership-summit/session-notes/1145_1245_architectural_roadmap/</guid>
      <description>Architectural Roadmap Identify the following before beginning your session. Do not move forward until these are decided / assigned:
 Session Topic: Architectural Roadmap Topic Facilitator(s): Brian Grant Note-taker(s) (Collaborating on this doc): Jason Singer DuMars, Nilay Yener Person responsible for converting to Markdown &amp;amp; uploading to Github: nyener@  Session Notes Link
Not using “core” because it is overloaded - substituting “nucleus” instead ~ absolute necessities for Kubernetes to function</description>
    </item>
    
    <item>
      <title>1345-1430_SIGGOVERNANCE</title>
      <link>/events/2017/05-leadership-summit/session-notes/1345-1430_siggovernance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/05-leadership-summit/session-notes/1345-1430_siggovernance/</guid>
      <description>SIG Governance and Check-in Identify the following before beginning your session. Do not move forward until these are decided / assigned:
 Session Topic: SIG Governance and check-in Topic Facilitator(s): brendandburns@ Note-taker(s) (Collaborating on this doc): jbeda@ Person responsible for converting to Markdown &amp;amp; uploading to Github: philips@  Session Notes  Core discussions about SIG leadership  Is the SIG lead role more like an on-call position or a team lead?</description>
    </item>
    
    <item>
      <title>1500-1545_COMMUNITY_LADDER</title>
      <link>/events/2017/05-leadership-summit/session-notes/1500-1545_community_ladder/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/05-leadership-summit/session-notes/1500-1545_community_ladder/</guid>
      <description>Community Building &amp;amp; Ladder Session Original Title: “Building community effectiveness (Contributor ladder, Code of Conduct &amp;amp; Committee)”
 Session Area: Community Topic Facilitator(s): Sarah Novotny Note-taker(s) (Collaborating on this doc):  Rob Hirschfeld @zehicle Jess Frazzelle  Person responsible for converting to Markdown &amp;amp; uploading to Github: Rob Hirschfeld @zehicle  Session Notes Classes of Communities being discussed: * Developer / User - using the API * Developer / Operator - running the Infrastructure / DevOps * Developer / Contributor - writing the Kubernetes code, docs, any sort of non-technical contribution</description>
    </item>
    
    <item>
      <title>1600-1740_PRESENTATIONS_FROM_BREAKOUTS_AND_CALL_TO_ACTIONS</title>
      <link>/events/2017/05-leadership-summit/session-notes/1600-1740_presentations_from_breakouts_and_call_to_actions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/05-leadership-summit/session-notes/1600-1740_presentations_from_breakouts_and_call_to_actions/</guid>
      <description>Presentations From Breakouts with Call to Actions Identify the following before beginning your session. Do not move forward until these are decided / assigned:
 Session Topic: Presentations From Breakouts with Call to Actions Topic Facilitator(s): sarahnovotny@ Note-taker(s) (Collaborating on this doc): Jason Singer DuMars, Nilay Yener Person responsible for converting to Markdown &amp;amp; uploading to Github: nyener@  Session Notes SIG GOVERNANCE AND SIG CHECK IN  Discussed cross cutting SIG concerns and how to accomplish Create certificate for SIGs SIG Leads office hours SIG Leads mailing list ~ open vs.</description>
    </item>
    
    <item>
      <title>2016-05-18</title>
      <link>/sig-apps/minutes/2016-05-18/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-apps/minutes/2016-05-18/</guid>
      <description>May 18, 2016  The first SIG Apps meeting Intro from Michelle Discussion on the future of the SIG:  Mike from Rackspace offered to do a demo of the recursive functionality (issue) Idea: solicit the community for cases where their use cases aren&amp;rsquo;t met.  Demo from Prashanth B on PetSets (issue)  Supposed to make deploying and managing stateful apps easier. Will be alpha in 1.3. Zookeeper, mysql, cassandra are example apps to run in this A group of elements in the identity of a thing.</description>
    </item>
    
    <item>
      <title>2016-05-25</title>
      <link>/sig-apps/minutes/2016-05-25/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-apps/minutes/2016-05-25/</guid>
      <description>May 25, 2016  Intro by Michelle Mike Metral from Rackspace demo  Recursively process configuration files with the -R flag  Discuss the survey  Gabe from Deis, Mike from Red Hat, and Doug Davis from IBM are willing to chip in.  How can we provide Prashanth and pet sets feedback?  We need some docs on how to use it with some examples (2) that you can copy and paste.</description>
    </item>
    
    <item>
      <title>2016-06-08</title>
      <link>/sig-apps/minutes/2016-06-08/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-apps/minutes/2016-06-08/</guid>
      <description>June 8, 2016  Intro by Michelle Noorali Adnan Abdulhussein, Stacksmith lead at Bitnami, did a demo of Stacksmith  In the container world, updates to your application&amp;rsquo;s stack or environment are rolled out by bringing down outdated containers and replacing them with an updated container image. Tools like Docker and Kubernetes make it incredibly easy to do this, however, knowing when your stack is outdated or vulnerable and starting the upgrade process is still a manual step.</description>
    </item>
    
    <item>
      <title>2016-06-15</title>
      <link>/sig-apps/minutes/2016-06-15/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-apps/minutes/2016-06-15/</guid>
      <description>June 15, 2016  Michelle Noorali gave an introduction Ben Hall, founder of katacoda, did a demo of katacoda  Katacoda is an interactive learning platform for software engineers. Ben showed off the current lessons on Kubernetes which are available now. Ben also talked about how companies/people can make their own interactive lessons and embed them. Contact Ben on Twitter: @Ben_Hall  Eric Chiang demoed new Role Based Access Control features and capability in Kubernetes 1.</description>
    </item>
    
    <item>
      <title>2016-06-22</title>
      <link>/sig-apps/minutes/2016-06-22/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-apps/minutes/2016-06-22/</guid>
      <description>June 22, 2016  Michelle Noorali gave a quick introduction Antoine Legrand did a demo of [KPM]((https://github.com/kubespray/kpm) Matt Farina gave an update on the SIG-apps survey  The survey ends this Friday, so there are only two more days to collect data. The survey can be found here. Please help us share the survey and get as much feedback on running apps in kubernetes as possible! If you&amp;rsquo;d like to help analyze the data, please contact @mattfarina.</description>
    </item>
    
    <item>
      <title>2016-06-29</title>
      <link>/sig-apps/minutes/2016-06-29/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-apps/minutes/2016-06-29/</guid>
      <description>June 29, 2016  Bart Spaans gave a demo and talked about KubeFuse  Work with your Kubernetes cluster like it&amp;rsquo;s a file system You can also edit resources and update them in cluster on writes Find more details in this post Contact Bart on Twitter: @Work_of_Bart  The DRUD team showed off how they use Kubernetes Jobs  DRUD is a drupal and wordpress management platform built on Kubernetes KubeJobWatcher can be found here For help or questions, reach out to Erin at erin@newmediadenver.</description>
    </item>
    
    <item>
      <title>2016-07-06</title>
      <link>/sig-apps/minutes/2016-07-06/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-apps/minutes/2016-07-06/</guid>
      <description>July 6, 2016  Matt Farina gave an introduction and overview of the agenda Chris Love gave a demo of his Cassandra cluster  It was a great example of PetSet in action His demo application is located at https://github.com/k8s-for-greeks/gpmr  Matt Farina gave an update on the analysis of the sig-apps survey  He will reach out to those interested in helping analyze the data It&amp;rsquo;s not too late to volunteer to help.</description>
    </item>
    
    <item>
      <title>2016-07-13</title>
      <link>/sig-apps/minutes/2016-07-13/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-apps/minutes/2016-07-13/</guid>
      <description>July 13, 2016  Michelle Noorali gave an introduction and overview of the agenda Christian Simon from JetStack gave a demo of Kube-Lego  Kube-Lego automatically requests certificates for Kubernetes Ingress resources Let&amp;rsquo;s Encrypt Here are the slides  Matt Farina gave an update on the analysis of the sig-apps survey  He has formed a team and should have results from the survey soon A blog post with the results will follow  Clayton Coleman called to attention that there are many open discussions on PetSet  Now is the time to get involved in key decision making conversations especially if you have already implemented a PetSet Follow along in the &amp;ldquo;Pet Set in Beta&amp;rdquo; issue (#28718) which also points to other relevant PetSet topics.</description>
    </item>
    
    <item>
      <title>2016-07-20</title>
      <link>/sig-apps/minutes/2016-07-20/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-apps/minutes/2016-07-20/</guid>
      <description>July 20, 2016  Michelle Noorali gave an introduction and overview of the agenda Janet Kuo gave an overview of Deployment features  See her blog post See used minikube for the local cluster set up during her demo  Saad Ali gave an overview of Volume features and things to look forward to around Volumes  Check out his presentation  We are looking forward to the sig-apps survey results next week  Watch the recording.</description>
    </item>
    
    <item>
      <title>2016-07-27</title>
      <link>/sig-apps/minutes/2016-07-27/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-apps/minutes/2016-07-27/</guid>
      <description>July 27, 2016  Intro by Matt Gerred Dillon presented a demo of Deis including features in the 2.2 release.  To turn an application into a container it uses Heroku buildpacks. slugbuilder converts an application and buildpack into a container and puts the image into a private registry. Pod manifest, service manifest, etc are also created. Q: Is slugbuilder a separate microservice others can use? A: Yes. It&amp;rsquo;s a plugin to builder that can be used with other builders as well.</description>
    </item>
    
    <item>
      <title>2016-08-03</title>
      <link>/sig-apps/minutes/2016-08-03/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-apps/minutes/2016-08-03/</guid>
      <description>August 03, 2016  Intro by Michelle Ryan J talked about &amp;ldquo;Defining Applications in Kubernetes(and Openshift).  His presentation can be found here and includes lots of helpful links. He went through some OpenShift primitives that are not in Kubernetes but may show up soon if the community sees a need. There was some in depth discussion about Deployments. Kubernetes has some concepts that came out of OpenShift Deployment Config.</description>
    </item>
    
    <item>
      <title>2017_goals</title>
      <link>/sig-aws/2017_goals/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-aws/2017_goals/</guid>
      <description>Kubernetes sig-aws Goals (2017) IAM improvements  We hope to offer a more seamless integration path to taking advantage of IAM roles in Kubernetes We are looking at implementations such as kube2iam  Statement Kubernetes components now have the ability to take advantage of IAM roles. The process is intuitive and easy.
Networking in AWS  We hope to offer a knowledge base for networking in Kubernetes on AWS We will consolidate networking reports and metrics around Kubernetes e2e tests and grow our metric list over time  Statement A user can go to a single public location, and see a side by side comparison of networking avenues in Kubernetes on AWS.</description>
    </item>
    
    <item>
      <title>CHARTER</title>
      <link>/sig-cloud-provider/charter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-cloud-provider/charter/</guid>
      <description>SIG Cloud Provider Charter Mission The Cloud Provider SIG ensures that the Kubernetes ecosystem is evolving in a way that is neutral to all (public and private) cloud providers. It will be responsible for establishing standards and requirements that must be met by all providers to ensure optimal integration with Kubernetes.
Subprojects &amp;amp; Areas of Focus  Maintaining parts of the Kubernetes project that allows Kubernetes to integrate with the underlying provider.</description>
    </item>
    
    <item>
      <title>CLA</title>
      <link>/cla/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/cla/</guid>
      <description>The Contributor License Agreement The Cloud Native Computing Foundation defines the legal status of the contributed code in a Contributor License Agreement (CLA).
Only original source code from CLA signatories can be accepted into kubernetes.
This policy does not apply to third_party and vendor.
What am I agreeing to? There are two versions of the CLA:
 One for individual contributors submitting contributions on their own behalf. One for corporations to sign for contributions submitted by their employees.</description>
    </item>
    
    <item>
      <title>CONTRIBUTING</title>
      <link>/contributing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributing/</guid>
      <description>Contributing to the Community Repo Welcome to the Kubernetes Community contributing guide. We are excited about the prospect of you joining our community!
Getting Started We have full documentation on how to get started contributing here:
 Kubernetes Contributor Guide - Main contributor documentation Contributor Cheat Sheet - Common resources for existing developers  Mentorship  Mentoring Initiatives - We have a diverse set of mentorship programs available that are always looking for volunteers!</description>
    </item>
    
    <item>
      <title>CONTRIBUTING</title>
      <link>/sig-apps/contributing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-apps/contributing/</guid>
      <description>Contributing to SIG Apps Welcome to contributing to SIG Apps.
SIG Apps has multiple areas you can contribute to. Those contributions can be in the form of code, documentation, support, being involved in mailing list discussions, attending meetings, and more. This guide describes different major functional areas SIG Apps is involved in, provides an overview of the areas, and gives pointers on getting more involved in each area. Consider this a launching point or the start of a choose your own adventure for SIG Apps.</description>
    </item>
    
    <item>
      <title>CONTRIBUTING</title>
      <link>/sig-aws/contributing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-aws/contributing/</guid>
      <description>Contributing The process for contributing code to Kubernetes via SIG-aws community.
Note: This page is focused on helping new contributors become active members of the community through sustained contributions.
If you are interested in contributing to Kubernetes as a whole there is a top level contributor&amp;rsquo;s guide.
Introduction Welcome to the Kubernetes sig-aws contributing guide. We are excited about the prospect of you joining our community! Mentoring and on-boarding new contributors is critical to the success of the project.</description>
    </item>
    
    <item>
      <title>CONTRIBUTING</title>
      <link>/sig-cli/contributing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-cli/contributing/</guid>
      <description>Contributing Welcome to the Kubernetes sig-cli contributing guide. We are excited about the prospect of you joining our community!
Before You Begin We strongly recommend you to understand the main Kubernetes Contributor Guide and adhere to the contribution rules (specially signing the CLA).
You can also check the Contributor Cheat Sheet, with common resources for existing developers.
The process for contributing code to Kubernetes via SIG-CLI community.
Please be aware that all contributions to Kubernetes require time and commitment from project maintainers to direct and review work.</description>
    </item>
    
    <item>
      <title>CONTRIBUTING</title>
      <link>/sig-multicluster/contributing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-multicluster/contributing/</guid>
      <description>Contributing The process for contributing code to Kubernetes via the sig-multicluster community.
TL;DR  The sig-multicluster community page lists sig-multicluster leads and group meeting times. Request a feature by making an issue and mentioning @kubernetes/sig-multicluster-feature-requests. Write a design proposal before starting work on a new feature. Write tests!  Before You Begin Welcome to the Kubernetes sig-multicluster contributing guide. We are excited about the prospect of you joining our community!</description>
    </item>
    
    <item>
      <title>ContribSummitInformation</title>
      <link>/events/2017/12-contributor-summit/contribsummitinformation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/12-contributor-summit/contribsummitinformation/</guid>
      <description>2017 Kubernetes Contributor Summit Information fka DevSummit
What: The Contributor Summit provides an avenue for Kubernetes contributors to connect face to face and mindshare about future community development and community governance endeavors.
In some sense, the summit is a real-life extension of the community meetings and SIG meetings.
When: December 5th, 2017 (before KubeCon)
All day event with happy hour reception to close
Agenda:
Times in CST
- 08:00 am - Registration and light breakfast</description>
    </item>
    
    <item>
      <title>Design_Proposal_TEMPLATE</title>
      <link>/contributors/design-proposals/design_proposal_template/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/design_proposal_template/</guid>
      <description>  Status: Pending
Version: Alpha | Beta | GA
Implementation Owner: TBD
Motivation Proposal User Experience Use Cases &amp;lt;include full examples&amp;gt;
Implementation Client/Server Backwards/Forwards compatibility Alternatives considered </description>
    </item>
    
    <item>
      <title>FAQ</title>
      <link>/committee-steering/governance/faq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/committee-steering/governance/faq/</guid>
      <description>Frequently Asked Questions Why the new structure? Why split the SIG Lead Role into 2 distinct roles? While developing the SIG governance requirements, 3 distinct sets of responsibilities were identified:
 Setting technical direction and resolving technical discussions in subprojects of a SIG (e.g. intra-subproject escalation point) Setting pure-technical scope across the SIG and resolving technical discussions that span subprojects (e.g. inter-subproject escalation point) Managing the operational aspects of the SIG - defining and evolving process and structure.</description>
    </item>
    
    <item>
      <title>K8sYoutubeCollaboration</title>
      <link>/communication/k8syoutubecollaboration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/communication/k8syoutubecollaboration/</guid>
      <description>Kubernetes Youtube Channel Collaboration Meeting Playlists The Kubernetes Youtube Channel has separate playlists for each SIG’s meeting recordings, as well as recordings of other meetings (i.e. Cloud Native, Kubernetes Community meetings).
To better serve the community, we have set up collaboration on these playlists, so that anyone with the appropriate link to the particular playlist can upload videos to that particular playlist (links &amp;amp; playlists are 1:1).
Each SIG playlist’s link will be shared with the SIG’s leadership, and other playlists&amp;rsquo; links (i.</description>
    </item>
    
    <item>
      <title>KubDevSummitVoting</title>
      <link>/events/2016/developer-summit-2016/kubdevsummitvoting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2016/developer-summit-2016/kubdevsummitvoting/</guid>
      <description>###Kubernetes Developer&amp;rsquo;s Summit Discussion Topics Voting ### A voting poll for discussion topic proposals has been created, and the link to the poll can be found here.
The poll will close on 10/07/16 at 23:59:59 PDT.
How Does it Work? #### The voting uses the Condorcet method, which relies on relative rankings to pick winners. You can read more about the Condorcet method and the voting service we&amp;rsquo;re using on the CIVS website.</description>
    </item>
    
    <item>
      <title>Kubernetes_1st_Bday</title>
      <link>/events/2016/kubernetes_1st_bday/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2016/kubernetes_1st_bday/</guid>
      <description>Kubernetes 1st Birthday Announcement Dear Kubernauts,
Happy birthday! The Kubernetes community is celebrating the first anniversary for the v1.0 release on July 21st, 2016. We are reaching out to you as the chief point(s) of contact for your local Kubernetes meetup to encourage you to throw a Kubernetes birthday party!
This is very exciting for the Kubernetes community, and we are happy to support your party planning efforts. We have swag we can provide (including awesome Kubernetes party hats!</description>
    </item>
    
    <item>
      <title>Kubernetes_Dev_Summit</title>
      <link>/events/2016/developer-summit-2016/kubernetes_dev_summit/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2016/developer-summit-2016/kubernetes_dev_summit/</guid>
      <description>Kubernetes Dev Summit Edit - Event Location ## The event is on the 4th Floor of the Union Street Tower of the Sheraton Seattle Hotel.
About the Event # The Kubernetes Developers&amp;rsquo; Summit provides an avenue for Kubernetes developers to connect face to face and mindshare about future community development and community governance endeavors.
In some sense, the summit is a real-life extension of the community meetings and SIG meetings.</description>
    </item>
    
    <item>
      <title>OBSOLETE_templates</title>
      <link>/contributors/design-proposals/apps/obsolete_templates/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/apps/obsolete_templates/</guid>
      <description>Templates+Parameterization: Repeatedly instantiating user-customized application topologies. Motivation Addresses https://github.com/kubernetes/kubernetes/issues/11492
There are two main motivators for Template functionality in Kubernetes: Controller Instantiation and Application Definition
Controller Instantiation Today the replication controller defines a PodTemplate which allows it to instantiate multiple pods with identical characteristics. This is useful but limited. Stateful applications have a need to instantiate multiple instances of a more sophisticated topology than just a single pod (e.g. they also need Volume definitions).</description>
    </item>
    
    <item>
      <title>ONCALL</title>
      <link>/sig-multicluster/oncall/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-multicluster/oncall/</guid>
      <description>Overview We have an oncall rotation for Federation in the SIG. The role description is as follows:
 Ensure that the testgrid (https://k8s-testgrid.appspot.com/sig-multicluster) is green. This person will be the point of contact if testgrid turns red. Will identify the problem and fix it (most common scenarios: find culprit PR and revert it or free quota by deleting leaked resources). Will also report most common failure scenarios and suggest improvements.</description>
    </item>
    
    <item>
      <title>RESULTS</title>
      <link>/events/elections/2017/results/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/elections/2017/results/</guid>
      <description>Results of the 2017 Steering Committee Election Number of seats open: 3 (2 year term), 3 (1 year term)
Number of eligible voters: 392
Number of votes cast: 309
Turnout: 78.8%
Raw ballot data
Results The final ranking, using the &amp;ldquo;Schulze&amp;rdquo; Condorcet completion, is as follows:
 Derek Carr Michelle Noorali Phillip Wittrock Michael Rubin Timothy St. Clair Quinton Hoole Aaron Crickenberger Caleb Miles Jaice Singer DuMars Kris Nova Justin Santa Barbara Alex Pollitt Sebastien Goasguen Ihor Dvoretskyi Adnan Abdulhussein Ilya Dmitrichenko Matt Farina Aaron Schlesinger Rob Hirschfeld Doug Davis  According to the limits on company representation, Google would be over-represented with this result, so Michael Rubin must be excluded.</description>
    </item>
    
    <item>
      <title>aaroncrickenberger_bio</title>
      <link>/events/elections/2017/aaroncrickenberger_bio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/elections/2017/aaroncrickenberger_bio/</guid>
      <description>Aaron Crickenberger I can be reached as @spiffxp on github, slack, gmail, linkedin, twitter, soundcloud, etc
What I&amp;rsquo;ve done I have been involved in open source projects since 2007, cloud related projects since 2009, and Kubernetes since 2015.
I co-founded SIG Testing. I actively contribute in SIG Contributor Experience, Release, Scale. If you attend the weekly Kubernetes Community meetings, chances are you&amp;rsquo;ve seen me (or at least my beard.)</description>
    </item>
    
    <item>
      <title>aaronschlesinger_bio</title>
      <link>/events/elections/2017/aaronschlesinger_bio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/elections/2017/aaronschlesinger_bio/</guid>
      <description>Aaron Schlesinger  Github: arschles Twitter: @arschles  About I&amp;rsquo;m a Sr. Software Engineer, Microsoft Azure Containers Group.
I&amp;rsquo;m a passionate engineer, teacher and leader with over 10 years software engineering and architecture experience.
I believe that Kubernetes will truly succeed when we improve the developer and user experience.
Experience I&amp;rsquo;ve made contributions to helm, helm charts, minikube, service-catalog and I am a co-lead of SIG-Service-Catalog.
Outside of Kubernetes, I&amp;rsquo;ve spent over 10 years speaking at conferences, teaching Go &amp;amp; Scala, building large systems, sitting on standards bodies, and contributing to other open source projects.</description>
    </item>
    
    <item>
      <title>accelerator-monitoring</title>
      <link>/contributors/design-proposals/node/accelerator-monitoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/accelerator-monitoring/</guid>
      <description>Monitoring support for hardware accelerators Version: Alpha
Owner: @mindprince (agarwalrohit@google.com)
Motivation We have had alpha support for running containers with GPUs attached in Kubernetes for a while. To take this to beta and GA, we need to provide GPU monitoring, so that users can get insights into how their GPU jobs are performing.
Detailed Design The current metrics pipeline for Kubernetes is: - Container level metrics are collected by cAdvisor.</description>
    </item>
    
    <item>
      <title>access</title>
      <link>/contributors/design-proposals/auth/access/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/auth/access/</guid>
      <description>K8s Identity and Access Management Sketch This document suggests a direction for identity and access management in the Kubernetes system.
Background High level goals are: - Have a plan for how identity, authentication, and authorization will fit in to the API. - Have a plan for partitioning resources within a cluster between independent organizational units. - Ease integration with existing enterprise and hosted scenarios.
Actors Each of these can act as normal users or attackers.</description>
    </item>
    
    <item>
      <title>add-new-patchStrategy-to-clear-fields-not-present-in-patch</title>
      <link>/contributors/design-proposals/api-machinery/add-new-patchstrategy-to-clear-fields-not-present-in-patch/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/add-new-patchstrategy-to-clear-fields-not-present-in-patch/</guid>
      <description>Add new patchStrategy to clear fields not present in the patch We introduce a new struct tag patchStrategy:&amp;quot;retainKeys&amp;quot; and a new optional directive $retainKeys: &amp;lt;list of fields&amp;gt; in the patch.
The proposal of Full Union is in kubernetes/community#388.
   Capability Supported By This Proposal Supported By Full Union     Auto clear missing fields on patch X X   Merge union fields on patch X X   Validate only 1 field set on type  X   Validate discriminator field matches one-of field  X   Support non-union patchKey X TBD   Support arbitrary combinations of set fields X     Use cases  As a user patching a map, I want keys mutually exclusive with those that I am providing to automatically be cleared.</description>
    </item>
    
    <item>
      <title>adding-an-APIGroup</title>
      <link>/contributors/devel/adding-an-apigroup/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/adding-an-apigroup/</guid>
      <description>Adding an API Group Please refer to api_changes.md.</description>
    </item>
    
    <item>
      <title>admission-control-webhooks</title>
      <link>/contributors/design-proposals/api-machinery/admission-control-webhooks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/admission-control-webhooks/</guid>
      <description>Webhooks Beta PUBLIC Authors: @erictune, @caesarxuchao, @enisoc Thanks to: {@dbsmith, @smarterclayton, @deads2k, @cheftako, @jpbetz, @mbohlool, @mml, @janetkuo} for comments, data, prior designs, etc.
[TOC]
Summary This document proposes a detailed plan for bringing Webhooks to Beta. Highlights include (incomplete, see rest of doc for complete list) :
 Adding the ability for webhooks to mutate.
 Bootstrapping Monitoring Versioned rather than Internal data sent on hook Ordering behavior within webhooks, and with other admission phases, is better defined  This plan is compatible with the original design doc.</description>
    </item>
    
    <item>
      <title>admission-webhook-bootstrapping</title>
      <link>/contributors/design-proposals/api-machinery/admission-webhook-bootstrapping/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/admission-webhook-bootstrapping/</guid>
      <description>Webhook Bootstrapping Background Admission webhook is a feature that dynamically extends Kubernetes admission chain. Because the admission webhooks are in the critical path of admitting REST requests, broken webhooks could block the entire cluster, even blocking the reboot of the webhooks themselves. This design presents a way to avoid such bootstrap deadlocks.
Objective  If one or more webhooks are down, it should be able restart them automatically. If a core system component that supports webhooks is down, the component should be able to restart.</description>
    </item>
    
    <item>
      <title>admission_control</title>
      <link>/contributors/design-proposals/api-machinery/admission_control/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/admission_control/</guid>
      <description>Kubernetes Proposal - Admission Control Related PR:
   Topic Link     Separate validation from RESTStorage http://issue.k8s.io/2977    Background High level goals: * Enable an easy-to-use mechanism to provide admission control to cluster. * Enable a provider to support multiple admission control strategies or author their own. * Ensure any rejected request can propagate errors back to the caller with why the request failed.</description>
    </item>
    
    <item>
      <title>admission_control_event_rate_limit</title>
      <link>/contributors/design-proposals/api-machinery/admission_control_event_rate_limit/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/admission_control_event_rate_limit/</guid>
      <description>Admission control plugin: EventRateLimit Background This document proposes a system for using an admission control to enforce a limit on the number of event requests that the API Server will accept in a given time slice. In a large cluster with many namespaces managed by disparate administrators, there may be a small percentage of namespaces that have pods that are always in some type of error state, for which the kubelets and controllers in the cluster are producing a steady stream of error event requests.</description>
    </item>
    
    <item>
      <title>admission_control_extension</title>
      <link>/contributors/design-proposals/api-machinery/admission_control_extension/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/admission_control_extension/</guid>
      <description>Extension of Admission Control via Initializers and External Admission Enforcement Admission control is the primary business-logic policy and enforcement subsystem in Kubernetes. It provides synchronous hooks for all API operations and allows an integrator to impose additional controls on the system - rejecting, altering, or reacting to changes to core objects. Today each of these plugins must be compiled into Kubernetes. As Kubernetes grows, the requirement that all policy enforcement beyond coarse grained access control be done through in-tree compilation and distribution becomes unwieldy and limits administrators and the growth of the ecosystem.</description>
    </item>
    
    <item>
      <title>admission_control_limit_range</title>
      <link>/contributors/design-proposals/resource-management/admission_control_limit_range/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/resource-management/admission_control_limit_range/</guid>
      <description>Admission control plugin: LimitRanger Background This document proposes a system for enforcing resource requirements constraints as part of admission control.
Use cases  Ability to enumerate resource requirement constraints per namespace Ability to enumerate min/max resource constraints for a pod Ability to enumerate min/max resource constraints for a container Ability to specify default resource limits for a container Ability to specify default resource requests for a container Ability to enforce a ratio between request and limit for a resource.</description>
    </item>
    
    <item>
      <title>admission_control_resource_quota</title>
      <link>/contributors/design-proposals/resource-management/admission_control_resource_quota/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/resource-management/admission_control_resource_quota/</guid>
      <description>Admission control plugin: ResourceQuota Background This document describes a system for enforcing hard resource usage limits per namespace as part of admission control.
Use cases  Ability to enumerate resource usage limits per namespace. Ability to monitor resource usage for tracked resources. Ability to reject resource usage exceeding hard quotas.  Data Model The ResourceQuota object is scoped to a Namespace.
// The following identify resource constants for Kubernetes object types const ( // Pods, number  ResourcePods ResourceName = &amp;#34;pods&amp;#34; // Services, number  ResourceServices ResourceName = &amp;#34;services&amp;#34; // ReplicationControllers, number  ResourceReplicationControllers ResourceName = &amp;#34;replicationcontrollers&amp;#34; // ResourceQuotas, number  ResourceQuotas ResourceName = &amp;#34;resourcequotas&amp;#34; // ResourceSecrets, number  ResourceSecrets ResourceName = &amp;#34;secrets&amp;#34; // ResourcePersistentVolumeClaims, number  ResourcePersistentVolumeClaims ResourceName = &amp;#34;persistentvolumeclaims&amp;#34; ) // ResourceQuotaSpec defines the desired hard limits to enforce for Quota type ResourceQuotaSpec struct { // Hard is the set of desired hard limits for each named resource  Hard ResourceList `json:&amp;#34;hard,omitempty&amp;#34; description:&amp;#34;hard is the set of desired hard limits for each named resource; see http://releases.</description>
    </item>
    
    <item>
      <title>adnanabdulhussein_bio</title>
      <link>/events/elections/2017/adnanabdulhussein_bio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/elections/2017/adnanabdulhussein_bio/</guid>
      <description>Adnan Abdulhussein  GitHub/Slack/Twitter: @prydonius Email: adnan@bitnami.com  Background I joined the Kubernetes community in 2016, and have contributed in a number of capacities since:
 Core contributor of Helm Co-leading the Kubernetes Charts project Kicked off the Helm incubation process and community docs Co-leading SIG Apps Regularly speaking and advocating for Kubernetes  Prior to my involvement in Kubernetes, I have been an active maintainer of Bitnami&amp;rsquo;s open source container images.</description>
    </item>
    
    <item>
      <title>agenda</title>
      <link>/sig-apps/agenda/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-apps/agenda/</guid>
      <description>Agenda Note, the minutes and agenda have moved to Google Docs.
August 17, 2016  Intro / Agenda Brian Hardock of the Deis Helm team will demo Helm.  Helm is a tool for creating and managing Kubernetes native applications.  Adnan Abdulhussein from Bitnami will walk us through example helm charts for Kubernetes native applications  August 10, 2016  Intro / Agenda PoC demo and discussion of AppController Working discussion of Pet Set beta steps led by Clayton Coleman and Prashanth B.</description>
    </item>
    
    <item>
      <title>aggregated-api-servers</title>
      <link>/contributors/design-proposals/api-machinery/aggregated-api-servers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/aggregated-api-servers/</guid>
      <description>Aggregated API Servers Abstract We want to divide the single monolithic API server into multiple aggregated servers. Anyone should be able to write their own aggregated API server to expose APIs they want. Cluster admins should be able to expose new APIs at runtime by bringing up new aggregated servers.
Motivation  Extensibility: We want to allow community members to write their own API servers to expose APIs they want.</description>
    </item>
    
    <item>
      <title>alexpollitt_bio</title>
      <link>/events/elections/2017/alexpollitt_bio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/elections/2017/alexpollitt_bio/</guid>
      <description>Alex Pollitt I am excited to have been nominated for the steering committee. I recognize that doing the role justice will be a demanding and time consuming commitment, but I would like to do my part to help maintain and foster the growth of this amazing community of contributors and users.
For those of you who don’t know me, I am one of the founders of Tigera, the company behind Calico, and active contributors to flannel, CNI, Kubernetes, and Istio.</description>
    </item>
    
    <item>
      <title>all-in-one-volume</title>
      <link>/contributors/design-proposals/node/all-in-one-volume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/all-in-one-volume/</guid>
      <description>Abstract Describes a proposal for a new volume type that can project secrets, configmaps, and downward API items.
Motivation Users often need to build directories that contain multiple types of configuration and secret data. For example, a configuration directory for some software package may contain both config files and credentials. Currently, there is no way to achieve this in Kubernetes without scripting inside of a container.
Constraints and Assumptions  The volume types must remain unchanged for backward compatibility There will be a new volume type for this proposed functionality, but no other API changes The new volume type should support atomic updates in the event of an input change  Use Cases  As a user, I want to automatically populate a single volume with the keys from multiple secrets, configmaps, and with downward API information, so that I can synthesize a single directory with various sources of information As a user, I want to populate a single volume with the keys from multiple secrets, configmaps, and with downward API information, explicitly specifying paths for each item, so that I can have full control over the contents of that volume  Populating a single volume without pathing A user should be able to map any combination of resources mentioned above into a single directory.</description>
    </item>
    
    <item>
      <title>alternate-api-representations</title>
      <link>/contributors/design-proposals/api-machinery/alternate-api-representations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/alternate-api-representations/</guid>
      <description>Alternate representations of API resources Abstract Naive clients benefit from allowing the server to returning resource information in a form that is easy to represent or is more efficient when dealing with resources in bulk. It should be possible to ask an API server to return a representation of one or more resources of the same type in a way useful for:
 Retrieving a subset of object metadata in a list or watch of a resource, such as the metadata needed by the generic Garbage Collector or the Namespace Lifecycle Controller Dealing with generic operations like Scale correctly from a client across multiple API groups, versions, or servers Return a simple tabular representation of an object or list of objects for naive web or command-line clients to display (for kubectl get) Return a simple description of an object that can be displayed in a wide range of clients (for kubectl describe) Return the object with fields set by the server cleared (as kubectl export) which is dependent on the schema, not on user input.</description>
    </item>
    
    <item>
      <title>annotations-downward-api</title>
      <link>/contributors/design-proposals/node/annotations-downward-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/annotations-downward-api/</guid>
      <description>Exposing annotations via environment downward API Author: Michal Rostecki &amp;lt;michal@kinvolk.io&amp;gt;
Introduction Annotations of the pod can be taken through the Kubernetes API, but currently there is no way to pass them to the application inside the container. This means that annotations can be used by the core Kubernetes services and the user outside of the Kubernetes cluster.
Of course using Kubernetes API from the application running inside the container managed by Kubernetes is technically possible, but that&amp;rsquo;s an idea which denies the principles of microservices architecture.</description>
    </item>
    
    <item>
      <title>announcement</title>
      <link>/events/2017/05-leadership-summit/announcement/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/05-leadership-summit/announcement/</guid>
      <description>This is an announcement for the 2017 Kubernetes Leadership Summit, which will occur on June 2nd, 2017 in San Jose, CA. This event will be similar to the Kubernetes Developer&amp;rsquo;s Summit in November 2016, but involving a smaller smaller audience comprised solely of leaders and influencers of the community. These leaders and influences include the SIG leads, release managers, and representatives from several companies, including (but not limited to) Google, Red Hat, CoreOS, WeaveWorks, Deis, and Mirantis.</description>
    </item>
    
    <item>
      <title>api-chunking</title>
      <link>/contributors/design-proposals/api-machinery/api-chunking/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/api-chunking/</guid>
      <description>Allow clients to retrieve consistent API lists in chunks On large clusters, performing API queries that return all of the objects of a given resource type (GET /api/v1/pods, GET /api/v1/secrets) can lead to significant variations in peak memory use on the server and contribute substantially to long tail request latency.
When loading very large sets of objects &amp;ndash; some clusters are now reaching 100k pods or equivalent numbers of supporting resources &amp;ndash; the system must:</description>
    </item>
    
    <item>
      <title>api-conventions</title>
      <link>/contributors/devel/api-conventions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/api-conventions/</guid>
      <description>API Conventions Updated: 3/7/2017
This document is oriented at users who want a deeper understanding of the Kubernetes API structure, and developers wanting to extend the Kubernetes API. An introduction to using resources with kubectl can be found in the object management overview.
Table of Contents
 Types (Kinds)  Resources Objects  Metadata Spec and Status Typical status properties References to related objects Lists of named subobjects preferred over maps Primitive types Constants Unions  Lists and Simple kinds  Differing Representations Verbs on Resources  PATCH operations  Strategic Merge Patch   Idempotency Optional vs.</description>
    </item>
    
    <item>
      <title>api-design</title>
      <link>/contributors/design-proposals/multicluster/cluster-registry/api-design/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/multicluster/cluster-registry/api-design/</guid>
      <description>Cluster Registry API @perotinus, @madhusudancs
Original draft: 08/16/2017
Reviewed in SIG multi-cluster meeting on 8&amp;frasl;29
This doc is a Markdown conversion of the original Cluster Registry API Google doc. That doc is deprecated, and this one is canonical; however, the old doc will be preserved so as not to lose comment and revision history that it contains.
Table of Contents  Purpose Motivating use cases API Authorization-based filtering of the list of clusters Status Auth Key differences vs existing Federation API Cluster object Open questions  Purpose The cluster registry API is intended to provide a common abstraction for other tools that will perform operations on multiple clusters.</description>
    </item>
    
    <item>
      <title>api-extensions-position-statement</title>
      <link>/sig-api-machinery/api-extensions-position-statement/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-api-machinery/api-extensions-position-statement/</guid>
      <description>API Extensions (SIG API Machinery position statement) Authors: Daniel Smith, David Eads (SIG API Machinery co-leads)
Last edit: Feb 23
Status: RELEASED
Background We have observed a lot of confusion in the community around the general topic of ThirdPartyResources (TPRs) and apiserver aggregation (AA). We want to document the current position of the API Machinery SIG.
Extremely briefly, TPR is a mechanism for lightweight, easy extension of the kubernetes API, which has collected a significant userbase.</description>
    </item>
    
    <item>
      <title>api-group</title>
      <link>/contributors/design-proposals/api-machinery/api-group/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/api-group/</guid>
      <description>Supporting multiple API groups Goal  Breaking the monolithic v1 API into modular groups and allowing groups to be enabled/disabled individually. This allows us to break the monolithic API server to smaller components in the future.
 Supporting different versions in different groups. This allows different groups to evolve at different speed.
 Supporting identically named kinds to exist in different groups. This is useful when we experiment new features of an API in the experimental group while supporting the stable API in the original group at the same time.</description>
    </item>
    
    <item>
      <title>api-tracking</title>
      <link>/sig-windows/api-tracking/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-windows/api-tracking/</guid>
      <description>Windows &amp;amp; Kubernetes APIs This document will grow into an API by API list of work that needs to be done to clarify Windows &amp;amp; Linux differences. This will be used to help clarify what needs to be eventually implemented (need a tracking issue), or not implemented (need a doc note).
Volumes V1.Pod.Volumes
Out of the various volume types, these should all be possible on Windows but tests are lacking:</description>
    </item>
    
    <item>
      <title>api_call_latency</title>
      <link>/sig-scalability/slos/api_call_latency/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-scalability/slos/api_call_latency/</guid>
      <description>API call latency SLIs/SLOs details User stories  As a user of vanilla Kubernetes, I want some guarantee how quickly I get the response from an API call. As an administrator of Kubernetes cluster, if I know characteristics of my external dependencies of apiserver (e.g custom admission plugins, webhooks and initializers) I want to be able to provide guarantees for API calls latency to users of my cluster.  Other notes  We obviously can’t give any guarantee in general, because cluster administrators are allowed to register custom admission plugins, webhooks and/or initializers, which we don’t have any control about and they obviously impact API call latencies.</description>
    </item>
    
    <item>
      <title>api_changes</title>
      <link>/contributors/devel/api_changes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/api_changes/</guid>
      <description>*This document is oriented at developers who want to change existing APIs. A set of API conventions, which applies to new APIs and to changes, can be found at API Conventions.
Table of Contents
 So you want to change the API?  Operational overview On compatibility Backward compatibility gotchas Incompatible API changes Changing versioned APIs Edit types.go Edit defaults.go Edit conversion.go Changing the internal structures Edit types.go Edit validation.</description>
    </item>
    
    <item>
      <title>api_extensions_latency</title>
      <link>/sig-scalability/slos/api_extensions_latency/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-scalability/slos/api_extensions_latency/</guid>
      <description> API call extension points latency SLIs details User stories  As an administrator, if API calls are slow, I would like to know if this is because slow extension points (admission plugins, webhooks, initializers) and if so which ones are responsible for it.  </description>
    </item>
    
    <item>
      <title>apiserver-build-in-admission-plugins</title>
      <link>/contributors/design-proposals/api-machinery/apiserver-build-in-admission-plugins/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/apiserver-build-in-admission-plugins/</guid>
      <description>Build some Admission Controllers into the Generic API server library Related PR:
   Topic Link     Admission Control https://git.k8s.io/community/contributors/design-proposals/api-machinery/admission_control.md    Introduction An admission controller is a piece of code that intercepts requests to the Kubernetes API - think a middleware. The API server lets you have a whole chain of them. Each is run in sequence before a request is accepted into the cluster.</description>
    </item>
    
    <item>
      <title>apiserver-count-fix</title>
      <link>/contributors/design-proposals/api-machinery/apiserver-count-fix/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/apiserver-count-fix/</guid>
      <description>apiserver-count fix proposal Authors: @rphillips
Table of Contents  Overview Known Issues Proposal Alternate Proposals  Custom Resource Definitions Refactor Old Reconciler   Overview Proposal to fix Issue #22609
kube-apiserver currently has a command-line argument --apiserver-count specifying the number of api servers. This masterCount is used in the MasterCountEndpointReconciler on a 10 second interval to potentially cleanup stale API Endpoints. The issue is when the number of kube-apiserver instances gets below or above the masterCount.</description>
    </item>
    
    <item>
      <title>apiserver-watch</title>
      <link>/contributors/design-proposals/api-machinery/apiserver-watch/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/apiserver-watch/</guid>
      <description>Abstract In the current system, most watch requests sent to apiserver are redirected to etcd. This means that for every watch request the apiserver opens a watch on etcd.
The purpose of the proposal is to improve the overall performance of the system by solving the following problems:
 having too many open watches on etcd avoiding deserializing/converting the same objects multiple times in different watch results  In the future, we would also like to add an indexing mechanism to the watch.</description>
    </item>
    
    <item>
      <title>apparmor</title>
      <link>/contributors/design-proposals/auth/apparmor/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/auth/apparmor/</guid>
      <description>Overview  Motivation Related work  Alpha Design  Overview Prerequisites API Changes Pod Security Policy Deploying profiles Testing  Beta Design  API Changes  Future work  System component profiles Deploying profiles Custom app profiles Security plugins Container Runtime Interface Alerting Profile authoring  Appendix  Overview AppArmor is a mandatory access control (MAC) system for Linux that supplements the standard Linux user and group based permissions.</description>
    </item>
    
    <item>
      <title>application_service_definition_notes</title>
      <link>/events/2016/developer-summit-2016/application_service_definition_notes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2016/developer-summit-2016/application_service_definition_notes/</guid>
      <description>Service/Application Definition We think we need to help out the developers in how do we organize our services and how do I define them nicely and deploy on our orchestrator of choice. Writing the Kube files is a steep learning curve. So can we have something which is a little bit easier?
Helm solves one purpose for this.
Helm contrib: one of the things folks as us is they start from a dockerfile, and they want to have a workflow where they go from dockerfile&amp;ndash;&amp;gt;imagebuild&amp;ndash;&amp;gt;registry&amp;ndash;&amp;gt;resource def.</description>
    </item>
    
    <item>
      <title>apply_refactor</title>
      <link>/contributors/design-proposals/cli/apply_refactor/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/cli/apply_refactor/</guid>
      <description>Apply v2 Background kubectl apply reads a file or set of files, and updates the cluster state based off the file contents. It does a couple things:
 Create / Update / (Delete) the live resources based on the file contents Update currently and previously configured fields, without clobbering fields set by other means, such as imperative kubectl commands, other deployment and management tools, admission controllers, initializers, horizontal and vertical autoscalers, operators, and other controllers.</description>
    </item>
    
    <item>
      <title>architectural-roadmap</title>
      <link>/contributors/devel/architectural-roadmap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/architectural-roadmap/</guid>
      <description>Kubernetes Architectural Roadmap Shared with the community
Status: First draft
Last update: 4/20/2017
Authors: Brian Grant, Tim Hockin, and Clayton Coleman
Intended audience: Kubernetes contributors
Table of Contents
 Kubernetes Architectural Roadmap  Summary/TL;DR Background System Layers  The Nucleus: API and Execution  The API and cluster control plane Execution  The Application Layer: Deployment and Routing The Governance Layer: Automation and Policy Enforcement The Interface Layer: Libraries and Tools The Ecosystem  Managing the matrix Layering of the system as it relates to security Next Steps   Summary/TL;DR This document describes the ongoing architectural development of the Kubernetes system, and the motivations behind it.</description>
    </item>
    
    <item>
      <title>architecture</title>
      <link>/contributors/design-proposals/architecture/architecture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/architecture/architecture/</guid>
      <description>Kubernetes Design and Architecture A much more detailed and updated Architectural Roadmap is also available.
Overview Kubernetes is a production-grade, open-source infrastructure for the deployment, scaling, management, and composition of application containers across clusters of hosts, inspired by previous work at Google. Kubernetes is more than just a “container orchestrator”. It aims to eliminate the burden of orchestrating physical/virtual compute, network, and storage infrastructure, and enable application operators and developers to focus entirely on container-centric primitives for self-service operation.</description>
    </item>
    
    <item>
      <title>auditing</title>
      <link>/contributors/design-proposals/api-machinery/auditing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/auditing/</guid>
      <description>Auditing Maciej Szulik (@soltysh) Dr. Stefan Schimanski (@sttts) Tim St. Clair (@timstclair)
Abstract This proposal aims at extending the auditing log capabilities of the apiserver.
Motivation and Goals With #27087 basic audit logging was added to Kubernetes. It basically implements access.log like http handler based logging of all requests in the apiserver API. It does not do deeper inspection of the API calls or of their payloads. Moreover, it has no specific knowledge of the API objects which are modified.</description>
    </item>
    
    <item>
      <title>automation</title>
      <link>/contributors/devel/automation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/automation/</guid>
      <description>Kubernetes Development Automation Overview Kubernetes uses a variety of automated tools in an attempt to relieve developers of repetitive, low brain power work. This document attempts to describe these processes.
Submit Queue In an effort to * reduce load on core developers * maintain end-to-end test stability * load test github&amp;rsquo;s label feature
We have added an automated submit-queue to the github &amp;ldquo;munger&amp;rdquo; for kubernetes.
The submit-queue does the following:</description>
    </item>
    
    <item>
      <title>aws_under_the_hood</title>
      <link>/contributors/design-proposals/aws/aws_under_the_hood/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/aws/aws_under_the_hood/</guid>
      <description>Peeking under the hood of Kubernetes on AWS This document provides high-level insight into how Kubernetes works on AWS and maps to AWS objects. We assume that you are familiar with AWS.
We encourage you to use kube-up to create clusters on AWS. We recommend that you avoid manual configuration but are aware that sometimes it&amp;rsquo;s the only option.
Tip: You should open an issue and let us know what enhancements can be made to the scripts to better suit your needs.</description>
    </item>
    
    <item>
      <title>backlog</title>
      <link>/sig-architecture/backlog/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-architecture/backlog/</guid>
      <description>Backlog Processes  KEP process rollout and iteration New repo process API review process Conformance test suite change review process Escalation process Branching policy  SIG-related  Improve charter to meet expectations of Steering Committee Clarify criteria for areas out of scope for the SIG Formalize decision processes Identify subprojects/subefforts Determine initial reviewers and approvers How are SIG leads chosen  Documentation  Update What Is Kubernetes on the web site Architectural diagram Document who owns client library, build, and release artifacts Document who owns conformance definition, profiles, etc.</description>
    </item>
    
    <item>
      <title>bazel</title>
      <link>/contributors/devel/bazel/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/bazel/</guid>
      <description>Build and test with Bazel Building and testing Kubernetes with Bazel is supported but not yet default.
Go rules are managed by the gazelle tool, with some additional rules managed by the kazel tool. These tools are called via the hack/update-bazel.sh script.
Instructions for installing Bazel can be found here.
Several make rules have been created for common operations:
 make bazel-build: builds all binaries in tree make bazel-test: runs all unit tests make bazel-test-integration: runs all integration tests make bazel-release: builds release tarballs, Docker images (for server components), and Debian images  You can also interact with Bazel directly; for example, to run all kubectl unit tests, run</description>
    </item>
    
    <item>
      <title>bootstrap-discovery</title>
      <link>/contributors/design-proposals/cluster-lifecycle/bootstrap-discovery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/cluster-lifecycle/bootstrap-discovery/</guid>
      <description>Super Simple Discovery API Overview It is surprisingly hard to figure out how to talk to a Kubernetes cluster. Not only do clients need to know where to look on the network, they also need to identify the set of root certificates to trust when talking to that endpoint.
This presents a set of problems: * It should be super easy for users to configure client systems with a minimum of effort kubectl or kubeadm init (or other client systems).</description>
    </item>
    
    <item>
      <title>bound-service-account-tokens</title>
      <link>/contributors/design-proposals/auth/bound-service-account-tokens/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/auth/bound-service-account-tokens/</guid>
      <description>Bound Service Account Tokens Author: @mikedanese
Objective This document describes an API that would allow workloads running on Kubernetes to request JSON Web Tokens that are audience, time and eventually key bound.
Background Kubernetes already provisions JWTs to workloads. This functionality is on by default and thus widely deployed. The current workload JWT system has serious issues:
 Security: JWTs are not audience bound. Any recipient of a JWT can masquerade as the presenter to anyone else.</description>
    </item>
    
    <item>
      <title>breaking-up-the-monolith</title>
      <link>/events/2017/12-contributor-summit/breaking-up-the-monolith/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/12-contributor-summit/breaking-up-the-monolith/</guid>
      <description>Breaking Up The Monolith Note Takers: Josh Berkus(@jberkus), Aaron Crickenberger (@spiffxp)
Here&amp;rsquo;s what we&amp;rsquo;re covering: build process, issue tracking, logistics.
Question: are we committed emotionally 100% to splitting the monolith Opinion: committed, but need to define what that means (past provides the impetus)
We&amp;rsquo;ve been trying [the monolith] for so long and it&amp;rsquo;s too expensive.
 erictune: as the project matures the core can&amp;rsquo;t keep growing at the same rate mattfarina: reason to break it up - developer experience,difficulty to contribute pushes people away from contributing (hard to step into issue queue, etc) clayton: I&amp;rsquo;m scared by this because kubectl is never going to be simpler, it may be easier to just write a new one erictune: ecosystem yes, more contributors&amp;hellip; narrowly defined core no bburns: we don&amp;rsquo;t do a good job of measuring the cost of this, we love breaking things up because it&amp;rsquo;s &amp;ldquo;cleaner&amp;rdquo; but we&amp;rsquo;re not estimating the cost in terms of the human effort and complexity lavalamp: respectfully disagree, we have estimated the cost, knew it was going to be painfu but we haven&amp;rsquo;t had a choice timstclair: nobody has ever answered the question to be of how we&amp;rsquo;re going to deliver a set of binaries as part of a release, there isn&amp;rsquo;t a cohesive plan for doing this timstclair: how are you actually going to get test infra to test all the things bburns: how do you actually find the commit that causes a failure in e2e lavalamp: have you see the doc I have posted everywhere this comes up?</description>
    </item>
    
    <item>
      <title>bulk_watch</title>
      <link>/contributors/design-proposals/api-machinery/bulk_watch/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/bulk_watch/</guid>
      <description>Background As part of increasing security of a cluster, we are planning to limit the ability of a given Kubelet (in general: node), to be able to read only resources associated with it. Those resources, in particular means: secrets, configmaps &amp;amp; persistentvolumeclaims. This is needed to avoid situation when compromising node de facto means compromising a cluster. For more details &amp;amp; discussions see https://github.com/kubernetes/kubernetes/issues/40476.
However, by some extension to this effort, we would like to improve scalability of the system, by significantly reducing amount of api calls coming from kubelets.</description>
    </item>
    
    <item>
      <title>calebamiles_bio</title>
      <link>/events/elections/2017/calebamiles_bio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/elections/2017/calebamiles_bio/</guid>
      <description>caleb miles Contact Information  GitHub/Slack: @calebamiles Email: caleb.miles@coreos.com  Problem solving style I am a huge fan of ideas which simplify and unify, possibly due to my undergraduate study of physics and mathematics. I believe that collaboration is incredibly important and that continuous feedback on incremental solutions are likely to produce the best outcome. I therefore try spend a lot of time talking with people about the problems that are important to them before stepping back to try to introduce a minimal solution based on their concerns.</description>
    </item>
    
    <item>
      <title>charter</title>
      <link>/sig-architecture/charter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-architecture/charter/</guid>
      <description>SIG Architecture Charter This charter is a WIP.
The Architecture SIG maintains and evolves the design principles of Kubernetes, and provides a consistent body of expertise necessary to ensure architectural consistency over time.
The scope covers issues that span all the system&amp;rsquo;s components, how they fit together, how they interact, etc.
Specific areas of focus include:
 Defining the scope of the Kubernetes project  What is (and is not) Kubernetes  Maintaining, evolving, and enforcing the deprecation policy  Deprecation policy  Documenting and evolving the system architecture  Kubernetes Design and Architecture  Defining and driving necessary extensibility points Establishing and documenting design principles  Design principles  Establishing and documenting conventions for system and user-facing APIs  API conventions  Developing necessary technical review processes, such as the proposal and API review processes Driving improvement of overall code organization, including github orgs and repositories Educating approvers/owners of other SIGs (e.</description>
    </item>
    
    <item>
      <title>charter</title>
      <link>/sig-azure/charter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-azure/charter/</guid>
      <description>SIG Azure Charter The following is a charter for the Kubernetes Special Interest Group for Azure. It delineates the roles of SIG leadership, SIG members, as well as the organizational processes for the SIG, both as they relate to project management and technical processes for SIG subprojects.
Roles SIG Chairs  Run operations and processes governing the SIG Seed members established at SIG founding Chairs MAY decide to step down at anytime and propose a replacement.</description>
    </item>
    
    <item>
      <title>charter</title>
      <link>/sig-contributor-experience/charter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-contributor-experience/charter/</guid>
      <description>Contributor Experience Special Interest Group Charter Chairs:  Elsie Phillips (@Phillels) Paris Pittman (@parispittman)  Technical Leads (TL):  Garrett Rodrigues (@grodrigues3) Christoph Blecker (@cblecker)  Subprojects and OWNERs Can be found on our [README]()
Processes and Responsibilities  Chairs run weekly meetings in accordance with SIG governance procedures unless TLs need to fill in Chairs and TLs run quarterly planning meetings where we decide goals for the next release, to be published in the SIG’s [README]()</description>
    </item>
    
    <item>
      <title>cherry-picks</title>
      <link>/contributors/devel/cherry-picks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/cherry-picks/</guid>
      <description>Overview This document explains how cherry picks are managed on release branches within the Kubernetes projects.
Prerequisites  Contributor License Agreement is considered implicit for all code within cherry-pick pull requests, unless there is a large conflict. A pull request merged against the master branch. Release branch exists. The normal git and GitHub configured shell environment for pushing to your kubernetes origin fork on GitHub and making a pull request against a configured remote upstream that tracks &amp;ldquo;https://github.</description>
    </item>
    
    <item>
      <title>client-libraries</title>
      <link>/contributors/devel/client-libraries/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/client-libraries/</guid>
      <description>Kubernetes API client libraries This document has been moved to https://kubernetes.io/docs/reference/using-api/client-libraries/.</description>
    </item>
    
    <item>
      <title>client-package-structure</title>
      <link>/contributors/design-proposals/api-machinery/client-package-structure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/client-package-structure/</guid>
      <description>Client: layering and package structure  Desired layers Transport RESTClient/request.go Mux layer High-level: Individual typed  High-level, typed: Discovery  High-level: Dynamic High-level: Client Sets Package Structure Client Guarantees (and testing)   Client: layering and package structure Desired layers Transport The transport layer is concerned with round-tripping requests to an apiserver somewhere. It consumes a Config object with options appropriate for this. (That&amp;rsquo;s most of the current client.</description>
    </item>
    
    <item>
      <title>clientgo-notes</title>
      <link>/events/2018/05-contributor-summit/clientgo-notes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2018/05-contributor-summit/clientgo-notes/</guid>
      <description>Client-go Lead: munnerz with assist from lavalamp
Slides: combined with the CRD session here (CRD is first; client-go is after)
Thanks to our notetakers: kragniz, mrbobbytales, directxman12, onyiny-ang
Goals for the Session  What is currently painful when building a controller Questions around best practices As someone new:  What is hard to grasp?  As someone experienced:  What important bits of info do you think are critical   Pain points when building controller  A lot of boilerplate  Work queues HasSynced functions Re-queuing  Lack of deep documentation in these areas  Some documentation exists, bot focused on k/k core  Securing webhooks &amp;amp; APIServers Validation schemas TLS, the number of certs is a pain point  It is hard right now, the internal k8s CA has been used a bit.</description>
    </item>
    
    <item>
      <title>cloud-native-design-refactoring-across-k8s</title>
      <link>/events/2017/12-contributor-summit/cloud-native-design-refactoring-across-k8s/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/12-contributor-summit/cloud-native-design-refactoring-across-k8s/</guid>
      <description>Contributor Summit - KubeCon 2017
Cloud Native Design/Refactoring across Kubernetes
*Lead: Joe Beda(*@jbeda)*
*Co-lead: Amit Kumar Jaiswal(*@AMIT_GKP)*
Abstract
Explore how to cleanly support cloud-native behaviors, such as standardized Kubernetes logs, injectable configuration and common queryable APIs in Kubernetes components. While this discussion isn&amp;rsquo;t only about containers, orchestrating OpenStack deployment/management within Kubernetes via OpenStack-Helm or Kolla-Kubernetes paves the way for better upgrade capabilities. They will also improve the ability to run individual Kubernetes services independently or in combination with other adjacent technologies.</description>
    </item>
    
    <item>
      <title>cloud-provider</title>
      <link>/events/2017/12-contributor-summit/cloud-provider/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/12-contributor-summit/cloud-provider/</guid>
      <description>CloudProvider Update Lead: Walter Fender / cheftako
Notetaker: Jago Macleod / jagosan 4pm
Goals:
Make it easy for any new cloud provider to be able to add support for new cloud provider without having to submit code to k/k repo. [good shape!] For currently in-tree cloud providers to be able to build and release without having to ‘wait’ for upstream kubernetes. (e.g. security patches and features) Also suboptimal that for technical reasons, other cloudprovider’s ‘init’s are being run in the hosting cloud provider’s environments.</description>
    </item>
    
    <item>
      <title>cloud-provider-refactoring</title>
      <link>/contributors/design-proposals/cloud-provider/cloud-provider-refactoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/cloud-provider/cloud-provider-refactoring/</guid>
      <description>Refactor Cloud Provider out of Kubernetes Core As kubernetes has evolved tremendously, it has become difficult for different cloudproviders (currently 7) to make changes and iterate quickly. Moreover, the cloudproviders are constrained by the kubernetes build/release life-cycle. This proposal aims to move towards a kubernetes code base where cloud providers specific code will move out of the core repository and into &amp;ldquo;official&amp;rdquo; repositories, where it will be maintained by the cloud providers themselves.</description>
    </item>
    
    <item>
      <title>cloudprovider-storage-metrics</title>
      <link>/contributors/design-proposals/cloud-provider/cloudprovider-storage-metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/cloud-provider/cloudprovider-storage-metrics/</guid>
      <description>Cloud Provider (specifically GCE and AWS) metrics for Storage API calls Goal Kubernetes should provide metrics such as - count &amp;amp; latency percentiles for cloud provider API it uses to provision persistent volumes.
In a ideal world - we would want these metrics for all cloud providers and for all API calls kubernetes makes but to limit the scope of this feature we will implement metrics for:
 GCE AWS  We will also implement metrics only for storage API calls for now.</description>
    </item>
    
    <item>
      <title>cluster-deployment</title>
      <link>/contributors/design-proposals/cluster-lifecycle/cluster-deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/cluster-lifecycle/cluster-deployment/</guid>
      <description>Objective Simplify the cluster provisioning process for a cluster with one master and multiple worker nodes. It should be secured with SSL and have all the default add-ons. There should not be significant differences in the provisioning process across deployment targets (cloud provider + OS distribution) once machines meet the node specification.
Overview Cluster provisioning can be broken into a number of phases, each with their own exit criteria. In some cases, multiple phases will be combined together to more seamlessly automate the cluster setup, but in all cases the phases can be run sequentially to provision a functional cluster.</description>
    </item>
    
    <item>
      <title>cluster-role-aggregation</title>
      <link>/contributors/design-proposals/auth/cluster-role-aggregation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/auth/cluster-role-aggregation/</guid>
      <description>Cluster Role Aggregation In order to support easy RBAC integration for CustomResources and Extension APIServers, we need to have a way for API extenders to add permissions to the &amp;ldquo;normal&amp;rdquo; roles for admin, edit, and view.
These roles express an intent for the namespaced power of administrators of the namespace (manage ownership), editors of the namespace (manage content like pods), and viewers of the namespace (see what is present). As new APIs are made available, these roles should reflect that intent to prevent migration concerns every time a new API is added.</description>
    </item>
    
    <item>
      <title>cluster_federation_notes</title>
      <link>/events/2016/developer-summit-2016/cluster_federation_notes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2016/developer-summit-2016/cluster_federation_notes/</guid>
      <description>Cluster Federation There&amp;rsquo;s a whole bunch of reasons why federation is interesting. There&amp;rsquo;s HA, there&amp;rsquo;s geographic locality, there&amp;rsquo;s just managing very large clusters. Use cases:
 HA Hybrid Cloud Geo/latency Scalability (many large clusters instead of one gigantic one) visibility of multiple clusters  You don&amp;rsquo;t actually need federation for geo-location now, but it helps. The mental model for this is kind of like Amazon AZ or Google zones. Sometimes we don&amp;rsquo;t care where a resource is but sometimes we do.</description>
    </item>
    
    <item>
      <title>cluster_lifecycle_notes</title>
      <link>/events/2016/developer-summit-2016/cluster_lifecycle_notes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2016/developer-summit-2016/cluster_lifecycle_notes/</guid>
      <description>Cluster Lifecycle Deployment &amp;amp; Upgrade Roadmap Moderator: Mike Danese
Note taker: Robert Bailey
Date: 2016-11-10
Goals Discuss HA, upgrades, and config management beyond kubeadm/kops &amp;amp; try to identify things that are currently underserved (upgrade testing, version skew policy, security upgrades)
Discussion kubeadm - not destined for production? * Doing resource provisioning (cloud VMs) is out of scope * Should be a toolbox that does the common parts of cluster lifecycle * And should be able to break out just the pieces that you want * Found a bunch of common parts of existing cluster deployment and want to build more of it into the core</description>
    </item>
    
    <item>
      <title>clustering</title>
      <link>/contributors/design-proposals/cluster-lifecycle/clustering/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/cluster-lifecycle/clustering/</guid>
      <description>Clustering in Kubernetes Overview The term &amp;ldquo;clustering&amp;rdquo; refers to the process of having all members of the Kubernetes cluster find and trust each other. There are multiple different ways to achieve clustering with different security and usability profiles. This document attempts to lay out the user experiences for clustering that Kubernetes aims to address.
Once a cluster is established, the following is true:
 Master -&amp;gt; Node The master needs to know which nodes can take work and what their current status is wrt capacity.</description>
    </item>
    
    <item>
      <title>code-of-conduct</title>
      <link>/code-of-conduct/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/code-of-conduct/</guid>
      <description>Kubernetes Community Code of Conduct Kubernetes follows the CNCF Code of Conduct.</description>
    </item>
    
    <item>
      <title>coding-conventions</title>
      <link>/contributors/guide/coding-conventions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/guide/coding-conventions/</guid>
      <description>Coding Conventions Updated: 1/24/2018
Table of Contents
 Coding Conventions  Code conventions Testing conventions Directory and file conventions   Code conventions  Bash
 https://google.github.io/styleguide/shell.xml
 Ensure that build, release, test, and cluster-management scripts run on macOS
  Go
 Go Code Review Comments
 Effective Go
 Know and avoid Go landmines
 Comment your code.
 Go&amp;rsquo;s commenting conventions If reviewers ask questions about why the code is the way it is, that&amp;rsquo;s a sign that comments might be helpful.</description>
    </item>
    
    <item>
      <title>collab</title>
      <link>/contributors/devel/collab/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/collab/</guid>
      <description>On Collaborative Development Code reviews All changes must be code reviewed. For non-maintainers this is obvious, since you can&amp;rsquo;t commit anyway. But even for maintainers, we want all changes to get at least one review, preferably (for non-trivial changes obligatorily) from someone who knows the areas the change touches. For non-trivial changes we may want two reviewers. The primary reviewer will make this decision and nominate a second reviewer, if needed.</description>
    </item>
    
    <item>
      <title>command_execution_port_forwarding</title>
      <link>/contributors/design-proposals/network/command_execution_port_forwarding/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/network/command_execution_port_forwarding/</guid>
      <description>Container Command Execution &amp;amp; Port Forwarding in Kubernetes Abstract This document describes how to use Kubernetes to execute commands in containers, with stdin/stdout/stderr streams attached and how to implement port forwarding to the containers.
Background See the following related issues/PRs:
 Support attach Real container ssh Provide easy debug network access to services OpenShift container command execution proposal  Motivation Users and administrators are accustomed to being able to access their systems via SSH to run remote commands, get shell access, and do port forwarding.</description>
    </item>
    
    <item>
      <title>communication</title>
      <link>/communication/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/communication/</guid>
      <description>NOTE: This document has moved to a new location.</description>
    </item>
    
    <item>
      <title>community-expectations</title>
      <link>/contributors/guide/community-expectations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/guide/community-expectations/</guid>
      <description>Community Expectations Kubernetes is a community project. Consequently, it is wholly dependent on its community to provide a productive, friendly and collaborative environment.
The first and foremost goal of the Kubernetes community is to develop orchestration technology that radically simplifies the process of creating reliable distributed systems. However a second, equally important goal is the creation of a community that fosters easy, agile development of such orchestration systems.
We therefore describe the expectations for members of the Kubernetes community.</description>
    </item>
    
    <item>
      <title>community-maintenance</title>
      <link>/sig-contributor-experience/community-maintenance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-contributor-experience/community-maintenance/</guid>
      <description>Community Maintenance This document outlines maintenance tasks that SIG Contributor Experience should be doing on a regular basis. Copy this list into a new Github issue, for example &amp;ldquo;Community Maintenance Tasks for 2Q 2018&amp;rdquo;. Then people can volunteer to audit different parts of the project at once. These tasks should be done at a minimum of twice a year, but ideally once a quarter.
If any of these tasks can be automated, then do so, however most of these require a human to make a judgement decision.</description>
    </item>
    
    <item>
      <title>community-meeting</title>
      <link>/events/community-meeting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/community-meeting/</guid>
      <description>Kubernetes Weekly Community Meeting We have PUBLIC and RECORDED weekly meeting every Thursday at 5pm UTC.
See it on the web at calendar.google.com , or paste this iCal url into any iCal client. Do NOT copy the meetings over to a your personal calendar, you will miss meeting updates. Instead use your client&amp;rsquo;s calendaring feature to say you are attending the meeting so that any changes made to meetings will be reflected on your personal calendar.</description>
    </item>
    
    <item>
      <title>community-membership</title>
      <link>/community-membership/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/community-membership/</guid>
      <description>Community membership Note: This document is in progress
This doc outlines the various responsibilities of contributor roles in Kubernetes. The Kubernetes project is subdivided into subprojects under SIGs. Responsibilities for most roles are scoped to these subprojects.
   Role Responsibilities Requirements Defined by     member active contributor in the community sponsored by 2 reviewers. multiple contributions to the project. Kubernetes GitHub org member.   reviewer review contributions from other members history of review and authorship in a subproject OWNERS file reviewer entry.</description>
    </item>
    
    <item>
      <title>component-config-conventions</title>
      <link>/contributors/devel/component-config-conventions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/component-config-conventions/</guid>
      <description>Component Configuration Conventions Objective This document concerns the configuration of Kubernetes system components (as opposed to the configuration of user workloads running on Kubernetes). Component configuration is a major operational burden for operators of Kubernetes clusters. To date, much literature has been written on and much effort expended to improve component configuration. Despite this, the state of component configuration remains dissonant. This document attempts to aggregate that literature and propose a set of guidelines that component owners can follow to improve consistency across the project.</description>
    </item>
    
    <item>
      <title>configmap</title>
      <link>/contributors/design-proposals/apps/configmap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/apps/configmap/</guid>
      <description>Generic Configuration Object Abstract The ConfigMap API resource stores data used for the configuration of applications deployed on Kubernetes.
The main focus of this resource is to:
 Provide dynamic distribution of configuration data to deployed applications. Encapsulate configuration information and simplify Kubernetes deployments. Create a flexible configuration model for Kubernetes.  Motivation A Secret-like API resource is needed to store configuration data that pods can consume.
Goals of this design:</description>
    </item>
    
    <item>
      <title>container-identity</title>
      <link>/events/2017/12-contributor-summit/container-identity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/12-contributor-summit/container-identity/</guid>
      <description>Container identity
Lead: Greg Castle
How are people using k8s service accounts
 Default service accounts?
 User:
 (Zalando)
 55 clusters
 Only allow pre-set service accounts postgres &amp;ldquo;operator service accounts&amp;rdquo;
 Don’t enable RBAC
 Namespaces: usually use the default namespace, more sophisticated clients can use namespaces
 Cluster per product
 CRD that defines request for an OAuth2 token
 Q: Would you want to tie it to a Kubernetes service account.</description>
    </item>
    
    <item>
      <title>container-init</title>
      <link>/contributors/design-proposals/node/container-init/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/container-init/</guid>
      <description>Pod initialization @smarterclayton
March 2016
Proposal and Motivation Within a pod there is a need to initialize local data or adapt to the current cluster environment that is not easily achieved in the current container model. Containers start in parallel after volumes are mounted, leaving no opportunity for coordination between containers without specialization of the image. If two containers need to share common initialization data, both images must be altered to cooperate using filesystem or network semantics, which introduces coupling between images.</description>
    </item>
    
    <item>
      <title>container-runtime-interface</title>
      <link>/contributors/devel/container-runtime-interface/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/container-runtime-interface/</guid>
      <description>CRI: the Container Runtime Interface What is CRI? CRI (Container Runtime Interface) consists of a protobuf API, specifications/requirements (to-be-added), and libraries for container runtimes to integrate with kubelet on a node. CRI is currently in Alpha.
In the future, we plan to add more developer tools such as the CRI validation tests.
Why develop CRI? Prior to the existence of CRI, container runtimes (e.g., docker, rkt) were integrated with kubelet through implementing an internal, high-level interface in kubelet.</description>
    </item>
    
    <item>
      <title>container-runtime-interface-v1</title>
      <link>/contributors/design-proposals/node/container-runtime-interface-v1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/container-runtime-interface-v1/</guid>
      <description>Redefine Container Runtime Interface The umbrella issue: #28789
Motivation Kubelet employs a declarative pod-level interface, which acts as the sole integration point for container runtimes (e.g., docker and rkt). The high-level, declarative interface has caused higher integration and maintenance cost, and also slowed down feature velocity for the following reasons. 1. Not every container runtime supports the concept of pods natively. When integrating with Kubernetes, a significant amount of work needs to go into implementing a shim of significant size to support all pod features.</description>
    </item>
    
    <item>
      <title>container-storage-interface</title>
      <link>/contributors/design-proposals/storage/container-storage-interface/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/container-storage-interface/</guid>
      <description>CSI Volume Plugins in Kubernetes Design Doc Status: Pending
Version: Alpha
Author: Saad Ali (@saad-ali, saadali@google.com)
This document was drafted here.
Terminology    Term Definition     Container Storage Interface (CSI) A specification attempting to establish an industry standard interface that Container Orchestration Systems (COs) can use to expose arbitrary storage systems to their containerized workloads.   in-tree Code that exists in the core Kubernetes repository.</description>
    </item>
    
    <item>
      <title>containerized-mounter</title>
      <link>/contributors/design-proposals/storage/containerized-mounter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/containerized-mounter/</guid>
      <description>Containerized Mounter with Chroot for Container-Optimized OS Goal Due security and management overhead, our new Container-Optimized OS used by GKE does not carry certain storage drivers and tools needed for such as nfs and glusterfs. This project takes a containerized mount approach to package mount binaries into a container. Volume plugin will execute mount inside of container and share the mount with the host.
Design  A docker image has storage tools (nfs and glusterfs) pre-installed and uploaded to gcs.</description>
    </item>
    
    <item>
      <title>containerized-mounter-pod</title>
      <link>/contributors/design-proposals/storage/containerized-mounter-pod/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/containerized-mounter-pod/</guid>
      <description>Containerized mounter using volume utilities in pods Goal Kubernetes should be able to run all utilities that are needed to provision/attach/mount/unmount/detach/delete volumes in pods instead of running them on the host. The host can be a minimal Linux distribution without tools to create e.g. Ceph RBD or mount GlusterFS volumes.
Secondary objectives These are not requirements per se, just things to consider before drawing the final design. * CNCF designs Container Storage Interface (CSI).</description>
    </item>
    
    <item>
      <title>contributing</title>
      <link>/sig-storage/contributing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-storage/contributing/</guid>
      <description>Ramping up on Kubernetes Storage We recommend the following presentations, docs, and videos to help get familiar with Kubernetes Storage concepts.
   Date Title Link Description     - Persistent Volume Framework Doc Public user docs for Kubenretes Persistent Volume framework.   2018 May 03 SIG Storage Intro Video An overview of SIG Storage By Saad Ali at Kubecon EU 2018.   2018 May 04 Kubernetes Storage Lingo 101 Video An overview of various terms used in Kubernetes storage and what they mean by Saad Ali at Kubecon EU 2018.</description>
    </item>
    
    <item>
      <title>contributor-cheatsheet</title>
      <link>/contributors/guide/contributor-cheatsheet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/guide/contributor-cheatsheet/</guid>
      <description>Kubernetes Cheat Sheet A list of common resources when contributing to Kubernetes.
   Repo PRs Issues Notes     Kubernetes PRs Issues Meeting Notes   Community PRs Issues    Docs PRs Issues     Getting Started  Contributor Guide  Workflow  Gubernator Dashboard - k8s.reviews Submit Queue Bot commands GitHub labels Release Buckets Developer Guide  Cherry Picking Guide - Queue  Kubernetes Code Search, maintained by @dims  SIGs and Working Groups  Master SIG list  Community  Calendar kubernetes-dev kubernetes-users Slack channels StackOverflow YouTube Channel  Tests  Current Test Status Aggregated Failures Test Grid Test Health Test History  Email Aliases  community@kubernetes.</description>
    </item>
    
    <item>
      <title>contributor-guide-building-your-kubernetes-tool</title>
      <link>/sig-cli/migrated-from-wiki/contributor-guide-building-your-kubernetes-tool/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-cli/migrated-from-wiki/contributor-guide-building-your-kubernetes-tool/</guid>
      <description>Client Tool Release Publishing Guidelines Projects should publish releases for client side tools.
Go Projects Static Linking See Go executables are statically linked, except when they are not.
 How to compile a statically linked binary: go file must be compiled without cgo support.  # Disable cgo export CGO_ENABLED=0  How to check if a binary is statically linked  # List dynamic dependencies (shared libraries): # 1. if it&amp;#39;s dynamically linked, you&amp;#39;ll see $ ldd &amp;lt;your_tool&amp;gt; linux-vdso.</description>
    </item>
    
    <item>
      <title>control-plane-resilience</title>
      <link>/contributors/design-proposals/multicluster/control-plane-resilience/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/multicluster/control-plane-resilience/</guid>
      <description>Kubernetes and Cluster Federation Control Plane Resilience Long Term Design and Current Status by Quinton Hoole, Mike Danese and Justin Santa-Barbara December 14, 2015 Summary Some amount of confusion exists around how we currently, and in future want to ensure resilience of the Kubernetes (and by implication Kubernetes Cluster Federation) control plane. This document is an attempt to capture that definitively. It covers areas including self-healing, high availability, bootstrapping and recovery.</description>
    </item>
    
    <item>
      <title>controller-ref</title>
      <link>/contributors/design-proposals/api-machinery/controller-ref/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/controller-ref/</guid>
      <description>ControllerRef proposal  Authors: gmarek, enisoc Last edit: 2017-02-06 Status: partially implemented  Approvers: * [ ] briangrant * [ ] dbsmith
Table of Contents
 Goals Non-goals API Behavior Upgrading Implementation Alternatives History  Goals  The main goal of ControllerRef (controller reference) is to solve the problem of controllers that fight over controlled objects due to overlapping selectors (e.g. a ReplicaSet fighting with a ReplicationController over Pods because both controllers have label selectors that match those Pods).</description>
    </item>
    
    <item>
      <title>controller_history</title>
      <link>/contributors/design-proposals/apps/controller_history/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/apps/controller_history/</guid>
      <description>Controller History Author: kow3ns@
Status: Proposal
Abstract In Kubernetes, in order to update and rollback the configuration and binary images of controller managed Pods, users mutate DaemonSet, StatefulSet, and Deployment Objects, and the corresponding controllers attempt to transition the current state of the system to the new declared target state.
To facilitate update and rollback for these controllers, and to provide a primitive that third-party controllers can build on, we propose a mechanism that allows controllers to manage a bounded history of revisions to the declared target state of their generated Objects.</description>
    </item>
    
    <item>
      <title>controllers</title>
      <link>/contributors/devel/controllers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/controllers/</guid>
      <description>Writing Controllers A Kubernetes controller is an active reconciliation process. That is, it watches some object for the world&amp;rsquo;s desired state, and it watches the world&amp;rsquo;s actual state, too. Then, it sends instructions to try and make the world&amp;rsquo;s current state be more like the desired state.
The simplest implementation of this is a loop:
for { desired := getDesiredState() current := getCurrentState() makeChanges(desired, current) } Watches, etc, are all merely optimizations of this logic.</description>
    </item>
    
    <item>
      <title>core-metrics-pipeline</title>
      <link>/contributors/design-proposals/instrumentation/core-metrics-pipeline/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/instrumentation/core-metrics-pipeline/</guid>
      <description>Core Metrics in kubelet Author: David Ashpole (@dashpole)
Last Updated: 1/31/2017
Status: Proposal
This document proposes a design for the set of metrics included in an eventual Core Metrics Pipeline.
 Core Metrics in kubelet  Introduction Definitions Background Motivations Proposal Non Goals Design Metric Requirements: Proposed Core Metrics: On-Demand Design: Future Work   Introduction Definitions &amp;ldquo;Kubelet&amp;rdquo;: The daemon that runs on every kubernetes node and controls pod and container lifecycle, among many other things.</description>
    </item>
    
    <item>
      <title>coredns</title>
      <link>/contributors/design-proposals/network/coredns/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/network/coredns/</guid>
      <description>Add CoreDNS for DNS-based Service Discovery Status: Pending
Version: Alpha
Implementation Owner: @johnbelamaric
Motivation CoreDNS is another CNCF project and is the successor to SkyDNS, which kube-dns is based on. It is a flexible, extensible authoritative DNS server and directly integrates with the Kubernetes API. It can serve as cluster DNS, complying with the dns spec.
CoreDNS has fewer moving parts than kube-dns, since it is a single executable and single process.</description>
    </item>
    
    <item>
      <title>cpu-manager</title>
      <link>/contributors/design-proposals/node/cpu-manager/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/cpu-manager/</guid>
      <description>CPU Manager Authors:
 @ConnorDoyle - Connor Doyle &amp;lt;connor.p.doyle@intel.com&amp;gt; @flyingcougar - Szymon Scharmach &amp;lt;szymon.scharmach@intel.com&amp;gt; @sjenning - Seth Jennings &amp;lt;sjenning@redhat.com&amp;gt;  Contents:
 Overview Proposed changes Operations and observability Practical challenges Implementation roadmap Appendix A: cpuset pitfalls  Overview Problems to solve:
 Poor or unpredictable performance observed compared to virtual machine based orchestration systems. Application latency and lower CPU throughput compared to VMs due to cpu quota being fulfilled across all cores, rather than exclusive cores, which results in fewer context switches and higher cache affinity.</description>
    </item>
    
    <item>
      <title>crds-notes</title>
      <link>/events/2018/05-contributor-summit/crds-notes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2018/05-contributor-summit/crds-notes/</guid>
      <description>CRDs - future and painpoints Lead: sttts
Slides: combined with the client-go session here
Thanks to our notetakers: mrbobbytales, kragniz, tpepper, and onyiny-ang
outlook - aggregation  API stable since 1.10. There is a lack of tools and library support. GSoC project with @xmudrii: share etcd storage  kubectl create etcdstorage your api-server  Store custom data in etcd  outlook custom resources 1.11: * alpha: multiplier versions with/without conversion * alpha: pruning - blocker for GA - unspecified fields are removed * deep change of semantics of custom resources * from JSON blob store to schema based storage * alpha: defaulting - defaults from openapi validation schema are applied * alpha: graceful deletion - (maybe?</description>
    </item>
    
    <item>
      <title>cri-container-stats</title>
      <link>/contributors/devel/cri-container-stats/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/cri-container-stats/</guid>
      <description>Container Runtime Interface: Container Metrics Container runtime interface (CRI) provides an abstraction for container runtimes to integrate with Kubernetes. CRI expects the runtime to provide resource usage statistics for the containers.
Background Historically Kubelet relied on the cAdvisor library, an open-source project hosted in a separate repository, to retrieve container metrics such as CPU and memory usage. These metrics are then aggregated and exposed through Kubelet&amp;rsquo;s Summary API for the monitoring pipeline (and other components) to consume.</description>
    </item>
    
    <item>
      <title>cri-dockershim-checkpoint</title>
      <link>/contributors/design-proposals/node/cri-dockershim-checkpoint/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/cri-dockershim-checkpoint/</guid>
      <description>CRI: Dockershim PodSandbox Checkpoint Umbrella Issue #34672
Background Container Runtime Interface (CRI) is an ongoing project to allow container runtimes to integrate with kubernetes via a newly-defined API. Dockershim is the Docker CRI implementation. This proposal aims to introduce checkpoint mechanism in dockershim.
Motivation Why do we need checkpoint? With CRI, Kubelet only passes configurations (SandboxConfig, ContainerConfig and ImageSpec) when creating sandbox, container and image, and only use the reference id to manage them after creation.</description>
    </item>
    
    <item>
      <title>cri-testing-policy</title>
      <link>/contributors/devel/cri-testing-policy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/cri-testing-policy/</guid>
      <description>Container Runtime Interface: Testing Policy Owner: SIG-Node
This document describes testing policy and process for runtimes implementing the Container Runtime Interface (CRI) to publish test results in a federated dashboard. The objective is to provide the Kubernetes community an easy way to track the conformance, stability, and supported features of a CRI runtime.
This document focuses on Kubernetes node/cluster end-to-end (E2E) testing because many features require integration of runtime, OS, or even the cloud provider.</description>
    </item>
    
    <item>
      <title>cri-validation</title>
      <link>/contributors/devel/cri-validation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/cri-validation/</guid>
      <description>Container Runtime Interface (CRI) Validation Testing CRI validation testing provides a test framework and a suite of tests to validate that the Container Runtime Interface (CRI) server implementation meets all the requirements. This allows the CRI runtime developers to verify that their runtime conforms to CRI, without needing to set up Kubernetes components or run Kubernetes end-to-end tests.
CRI validation testing is GA since v1.11.0 and is hosted at the cri-tools repository.</description>
    </item>
    
    <item>
      <title>cri-windows</title>
      <link>/contributors/design-proposals/node/cri-windows/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/cri-windows/</guid>
      <description>CRI: Windows Container Configuration Authors: Jiangtian Li (@JiangtianLi), Pengfei Ni (@feiskyer), Patrick Lang(@PatrickLang)
Status: Proposed
Background Container Runtime Interface (CRI) defines APIs and configuration types for kubelet to integrate various container runtimes. The Open Container Initiative (OCI) Runtime Specification defines platform specific configuration, including Linux, Windows, and Solaris. Currently CRI only supports Linux container configuration. This proposal is to bring the Memory &amp;amp; CPU resource restrictions already specified in OCI for Windows to CRI.</description>
    </item>
    
    <item>
      <title>cronjob</title>
      <link>/contributors/design-proposals/apps/cronjob/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/apps/cronjob/</guid>
      <description>CronJob Controller (previously ScheduledJob) Abstract A proposal for implementing a new controller - CronJob controller - which will be responsible for managing time based jobs, namely: * once at a specified point in time, * repeatedly at a specified point in time.
There is already a discussion regarding this subject: * Distributed CRON jobs #2156
There are also similar solutions available, already: * Mesos Chronos * Quartz
Use Cases  Be able to schedule a job execution at a given point in time.</description>
    </item>
    
    <item>
      <title>csi-client-structure-proposal</title>
      <link>/contributors/design-proposals/api-machinery/csi-client-structure-proposal/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/csi-client-structure-proposal/</guid>
      <description>Overall Kubernetes Client Structure Status: Approved by SIG API Machinery on March 29th, 2017
Authors: @lavalamp, @mbohlool
last edit: 2017-3-22
Goals  Users can build production-grade programmatic use of Kubernetes-style APIs in their language of choice.  New Concept Today, Kubernetes has the concept of an API Group. Sometimes it makes sense to package multiple groups together in a client, for example, the core APIs we publish today. I’ll call this a &amp;ldquo;group collection&amp;rdquo; as it sounds a bit better than “group group.</description>
    </item>
    
    <item>
      <title>csi-new-client-library-procedure</title>
      <link>/contributors/design-proposals/api-machinery/csi-new-client-library-procedure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/csi-new-client-library-procedure/</guid>
      <description>Kubernetes: New Client Library Procedure Status: Approved by SIG API Machinery on March 29th, 2017
Authors: @mbohlool, @lavalamp
Last Updated: 2017-03-06
Background Kubernetes currently officially supports both Go and Python client libraries. The go client is developed and extracted from main kubernetes repositories in a complex process. On the other hand, the python client is based on OpenAPI, and is mostly generated code (via swagger-codegen). By generating the API Operations and Data Models, updating the client and tracking changes from main repositories becomes much more sustainable.</description>
    </item>
    
    <item>
      <title>current-state-of-client-libraries</title>
      <link>/events/2017/12-contributor-summit/current-state-of-client-libraries/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/12-contributor-summit/current-state-of-client-libraries/</guid>
      <description>Story so far is described at /contributors/design-proposals/api-machinery/csi-client-structure-proposal.md - linked from https://github.com/kubernetes-client/community/blob/master/design-docs/clients-library-structure.md
Would like libs to be more &amp;ldquo;fluent&amp;rdquo; - should join up with the native language features e.g. JavaDoc, per-platform
OpenAPI (Swagger) has limitations which force ugly workarounds. - e.g. doesn&amp;rsquo;t cover returning different types in response to a call e.g. a result or an error. - OpenAPI 3 fixes a number of these limitations
Key difficulty is which verbs apply to which objects - Go doesn&amp;rsquo;t natively have this concept.</description>
    </item>
    
    <item>
      <title>custom-metrics-api</title>
      <link>/contributors/design-proposals/instrumentation/custom-metrics-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/instrumentation/custom-metrics-api/</guid>
      <description>Custom Metrics API The new metrics monitoring vision proposes an API that the Horizontal Pod Autoscaler can use to access arbitrary metrics.
Similarly to the master metrics API, the new API should be structured around accessing metrics by referring to Kubernetes objects (or groups thereof) and a metric name. For this reason, the API could be useful for other consumers (most likely controllers) that want to consume custom metrics (similarly to how the master metrics API is generally useful to multiple cluster components).</description>
    </item>
    
    <item>
      <title>custom_content</title>
      <link>/generator/testdata/custom_content/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/generator/testdata/custom_content/</guid>
      <description> Auth SIG Covers improvements to Kubernetes authorization, authentication, and cluster security policy.
Meetings  Wednesdays at 18:00 UTC (biweekly). Convert to your timezone.  Meeting notes and Agenda can be found here.
Leads  Eric Chiang, CoreOS Jordan Liggitt, Red Hat David Eads, Red Hat  Contact  Slack Mailing list  FOO BAR BAZ </description>
    </item>
    
    <item>
      <title>customresources-subresources</title>
      <link>/contributors/design-proposals/api-machinery/customresources-subresources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/customresources-subresources/</guid>
      <description>Subresources for CustomResources Authors: @nikhita, @sttts
Table of Contents  Abstract Goals Non-Goals Proposed Extension of CustomResourceDefinition  API Types Feature Gate  Semantics  Validation Behavior  Status Scale  Status Behavior Scale Behavior  Status Replicas Behavior Selector Behavior   Implementation Plan Alternatives  Scope   Abstract CustomResourceDefinitions (CRDs) were introduced in 1.7. The objects defined by CRDs are called CustomResources (CRs). Currently, we do not provide subresources for CRs.</description>
    </item>
    
    <item>
      <title>customresources-validation</title>
      <link>/contributors/design-proposals/api-machinery/customresources-validation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/customresources-validation/</guid>
      <description>Validation for CustomResources Authors: @nikhita, @sttts, some ideas integrated from @xiao-zhou’s proposal1
Table of Contents  Overview Background  Goals Non-Goals  Proposed Extension of CustomResourceDefinition  API Types Examples  JSON-Schema Error messages   Validation Behavior  Metadata Server-Side Validation Client-Side Validation Comparison between server-side and client-side Validation Existing Instances and changing the Schema Outlook to Status Sub-Resources Outlook Admission Webhook  Implementation Plan Appendix  Expressiveness of JSON-Schema JSON-Schema Validation Runtime Complexity Alternatives  Direct Embedding of the Schema into the Spec External CustomResourceSchema Type    Overview This document proposes the design and describes a way to add JSON-Schema based validation for Custom Resources.</description>
    </item>
    
    <item>
      <title>customresources-versioning</title>
      <link>/contributors/design-proposals/api-machinery/customresources-versioning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/customresources-versioning/</guid>
      <description>CRD Versioning The objective of this design document is to provide a machinery for Custom Resource Definition authors to define different resource version and a conversion mechanism between them.
Background Custom Resource Definitions (CRDs) are a popular mechanism for extending Kubernetes, due to their ease of use compared with the main alternative of building an Aggregated API Server. They are, however, lacking a very important feature that all other kubernetes objects support: Versioning.</description>
    </item>
    
    <item>
      <title>daemon</title>
      <link>/contributors/design-proposals/apps/daemon/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/apps/daemon/</guid>
      <description>DaemonSet in Kubernetes Author: Ananya Kumar (@AnanyaKumar)
Status: Implemented.
This document presents the design of the Kubernetes DaemonSet, describes use cases, and gives an overview of the code.
Motivation Many users have requested for a way to run a daemon on every node in a Kubernetes cluster, or on a certain set of nodes in a cluster. This is essential for use cases such as building a sharded datastore, or running a logger on every node.</description>
    </item>
    
    <item>
      <title>daemonset-update</title>
      <link>/contributors/design-proposals/apps/daemonset-update/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/apps/daemonset-update/</guid>
      <description>DaemonSet Updates Author: @madhusudancs, @lukaszo, @janetkuo
Status: Proposal
Abstract A proposal for adding the update feature to DaemonSet. This feature will be implemented on server side (in DaemonSet API).
Users already can update a DaemonSet today (Kubernetes release 1.5), which will not cause changes to its subsequent pods, until those pods are killed. In this proposal, we plan to add a &amp;ldquo;RollingUpdate&amp;rdquo; strategy which allows DaemonSet to downstream its changes to pods.</description>
    </item>
    
    <item>
      <title>dashboard-ux-breakout</title>
      <link>/events/2017/12-contributor-summit/dashboard-ux-breakout/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/12-contributor-summit/dashboard-ux-breakout/</guid>
      <description>Kubernetes Dashboard UX breakout session 12.5.17, led by Rahul Dhide (rahuldhide) and Dan Romlein
 Resources:
 Dashboard User Types #975
 Dashboard User Types and Use Cases
 SIG UI weekly
  Notes
 2018 Dashboard strategy.
 Deck
 Github issue
  Kubectl access via Dashboard.
 Dhaval: Provide context to issues.  Third-party Widgets.
 Custom Views.
 Dhaval: Custom views will be very useful to share the event details, contextual information, and logs with specific time ranges.</description>
    </item>
    
    <item>
      <title>declarative-application-management</title>
      <link>/contributors/design-proposals/architecture/declarative-application-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/architecture/declarative-application-management/</guid>
      <description>Declarative application management in Kubernetes  This article was authored by Brian Grant (bgrant0607) on 8/2/2017. The original Google Doc can be found here: https://goo.gl/T66ZcD
 Most users will deploy a combination of applications they build themselves, also known as bespoke applications, and common off-the-shelf (COTS) components. Bespoke applications are typically stateless application servers, whereas COTS components are typically infrastructure (and frequently stateful) systems, such as databases, key-value stores, caches, and messaging systems.</description>
    </item>
    
    <item>
      <title>default-storage-class</title>
      <link>/contributors/design-proposals/storage/default-storage-class/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/default-storage-class/</guid>
      <description>Deploying a default StorageClass during installation Goal Usual Kubernetes installation tools should deploy a default StorageClass where it makes sense.
&amp;ldquo;Usual installation tools&amp;rdquo; are:
 cluster/kube-up.sh kops kubeadm  Other &amp;ldquo;installation tools&amp;rdquo; can (and should) deploy default StorageClass following easy steps described in this document, however we won&amp;rsquo;t touch them during implementation of this proposal.
&amp;ldquo;Where it makes sense&amp;rdquo; are:
 AWS Azure GCE Photon OpenStack vSphere  Explicitly, there is no default storage class on bare metal.</description>
    </item>
    
    <item>
      <title>deploy</title>
      <link>/contributors/design-proposals/apps/deploy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/apps/deploy/</guid>
      <description>Deploy through CLI  Motivation Requirements Related kubectl Commands kubectl run kubectl scale and kubectl autoscale kubectl rollout kubectl set Mutating Operations Example Support in Deployment Deployment Status Deployment Revision Pause Deployments Failed Deployments   Deployment rolling update design proposal Author: @janetkuo
Status: implemented
Deploy through CLI Motivation Users can use Deployments or kubectl rolling-update to deploy in their Kubernetes clusters. A Deployment provides declarative update for Pods and ReplicationControllers, whereas rolling-update allows the users to update their earlier deployment without worrying about schemas and configurations.</description>
    </item>
    
    <item>
      <title>deployment</title>
      <link>/contributors/design-proposals/apps/deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/apps/deployment/</guid>
      <description>Deployment Authors: - Brian Grant (@bgrant0607) - Clayton Coleman (@smarterclayton) - Dan Mace (@ironcladlou) - David Oppenheimer (@davidopp) - Janet Kuo (@janetkuo) - Michail Kargakis (@kargakis) - Nikhil Jindal (@nikhiljindal)
Abstract A proposal for implementing a new resource - Deployment - which will enable declarative config updates for ReplicaSets. Users will be able to create a Deployment, which will spin up a ReplicaSet to bring up the desired Pods. Users can also target the Deployment to an existing ReplicaSet either by rolling back an existing Deployment or creating a new Deployment that can adopt an existing ReplicaSet.</description>
    </item>
    
    <item>
      <title>derekcarr_bio</title>
      <link>/events/elections/2017/derekcarr_bio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/elections/2017/derekcarr_bio/</guid>
      <description>Derek Carr  GitHub: @derekwaynecarr Employer: Red Hat  Roles Held  Co-founder, lead: sig-node, wg-resource-management Participate: api-machinery, autoscaling, federation, scheduling, service-catalog
  About me I joined in 2014 as an early external contributor. After the first developer summit, I realized I had a lot to learn from the experience of others. My first PR enabled people to contribute to the project by running Kubernetes in a VM. Mentors in the community helped multiply the quality of my contributions.</description>
    </item>
    
    <item>
      <title>development</title>
      <link>/contributors/devel/development/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/development/</guid>
      <description>Development Guide This document is the canonical source of truth for things like supported toolchain versions for building Kubernetes.
Please submit an issue on Github if you * Notice a requirement that this doc does not capture. * Find a different doc that specifies requirements (the doc should instead link here).
Development branch requirements will change over time, but release branch requirements are frozen.
Pre submit flight checks Determine whether your issue or pull request is improving Kubernetes&amp;rsquo; architecture or whether it&amp;rsquo;s simply fixing a bug.</description>
    </item>
    
    <item>
      <title>device-plugin</title>
      <link>/contributors/design-proposals/resource-management/device-plugin/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/resource-management/device-plugin/</guid>
      <description>Device Manager Proposal  Motivation Use Cases Objectives Non Objectives Vendor story End User story Device Plugin  Introduction Registration Unix Socket Protocol Overview API specification HealthCheck and Failure Recovery API Changes  Upgrading your cluster Installation Versioning References  Authors:
 @RenaudWasTaken - Renaud Gaubert &amp;lt;rgaubert@NVIDIA.com&amp;gt; @jiayingz - Jiaying Zhang &amp;lt;jiayingz@google.com&amp;gt;  Motivation Kubernetes currently supports discovery of CPU and Memory primarily to a minimal extent. Very few devices are handled natively by Kubelet.</description>
    </item>
    
    <item>
      <title>devtools-notes</title>
      <link>/events/2018/05-contributor-summit/devtools-notes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2018/05-contributor-summit/devtools-notes/</guid>
      <description>Developer Tools: Leads: errordeveloper, r2d4
Slides: n/a
Thanks to our notetakers: mrbobbytales, onyiny-ang
What APIs should we target, what parts of the developer workflow haven&amp;rsquo;t been covered yet?
 Do you think the Developer tools for Kubernetes is a solved problem?  A: No   Long form responses from SIG Apps survey  Need to talk about developer experience Kubernetes Community can do a lot more in helping evangelize Software development workflow, including CI/CD.</description>
    </item>
    
    <item>
      <title>disk-accounting</title>
      <link>/contributors/design-proposals/node/disk-accounting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/disk-accounting/</guid>
      <description>Author: Vishnu Kannan
Last Updated: 11/16/2015
Status: Pending Review
This proposal is an attempt to come up with a means for accounting disk usage in Kubernetes clusters that are running docker as the container runtime. Some of the principles here might apply for other runtimes too.
Why is disk accounting necessary? As of kubernetes v1.1 clusters become unusable over time due to the local disk becoming full. The kubelets on the node attempt to perform garbage collection of old containers and images, but that doesn&amp;rsquo;t prevent running pods from using up all the available disk space.</description>
    </item>
    
    <item>
      <title>dougdavis_bio</title>
      <link>/events/elections/2017/dougdavis_bio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/elections/2017/dougdavis_bio/</guid>
      <description>Doug Davis  Github: @duglin Works for: IBM  Background I&amp;rsquo;ve been working on open source projects, and standards, for about 17 years, starting with being a co-initiator of the Apache Axis project; working on WS-* related projects and specifications, OpenStack, CloudFoundry, Docker and now Kubernetes, including as a co-lead of the Service-Catalog Incubator. I also founded the Web Services Testing Forum, a consortium of Web Service providers and consumers designed for interop testing of SOAP/WS-* specifications, as well as the soaphub on-line collaboration chat tool used by several communities.</description>
    </item>
    
    <item>
      <title>downward_api_resources_limits_requests</title>
      <link>/contributors/design-proposals/node/downward_api_resources_limits_requests/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/downward_api_resources_limits_requests/</guid>
      <description>Downward API for resource limits and requests Background Currently the downward API (via environment variables and volume plugin) only supports exposing a Pod&amp;rsquo;s name, namespace, annotations, labels and its IP (see details). This document explains the need and design to extend them to expose resources (e.g. cpu, memory) limits and requests.
Motivation Software applications require configuration to work optimally with the resources they&amp;rsquo;re allowed to use. Exposing the requested and limited amounts of available resources inside containers will allow these applications to be configured more easily.</description>
    </item>
    
    <item>
      <title>dramatically-simplify-cluster-creation</title>
      <link>/contributors/design-proposals/cluster-lifecycle/dramatically-simplify-cluster-creation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/cluster-lifecycle/dramatically-simplify-cluster-creation/</guid>
      <description>Proposal: Dramatically Simplify Kubernetes Cluster Creation  Please note: this proposal doesn&amp;rsquo;t reflect final implementation, it&amp;rsquo;s here for the purpose of capturing the original ideas. You should probably read kubeadm docs, to understand the end-result of this effor.
 Luke Marsden &amp;amp; many others in SIG-cluster-lifecycle.
17th August 2016
This proposal aims to capture the latest consensus and plan of action of SIG-cluster-lifecycle. It should satisfy the first bullet point required by the feature description.</description>
    </item>
    
    <item>
      <title>dynamic-admission-control-configuration</title>
      <link>/contributors/design-proposals/api-machinery/dynamic-admission-control-configuration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/dynamic-admission-control-configuration/</guid>
      <description>Background The extensible admission control proposal proposed making admission control extensible. In the proposal, the initializer admission controller and the generic webhook admission controller are the two controllers that set default initializers and external admission hooks for resources newly created. These two admission controllers are in the same binary as the apiserver. This section gave a preliminary design of the dynamic configuration of the list of the default admission controls.</description>
    </item>
    
    <item>
      <title>dynamic-kubelet-configuration</title>
      <link>/contributors/design-proposals/node/dynamic-kubelet-configuration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/dynamic-kubelet-configuration/</guid>
      <description>Dynamic Kubelet Configuration Abstract A proposal for making it possible to (re)configure Kubelets in a live cluster by providing config via the API server. Some subordinate items include local checkpointing of Kubelet configuration and the ability for the Kubelet to read config from a file on disk, rather than from command line flags.
Motivation The Kubelet is currently configured via command-line flags. This is painful for a number of reasons: - It makes it difficult to change the way Kubelets are configured in a running cluster, because it is often tedious to change the Kubelet startup configuration (without adding your own configuration management system e.</description>
    </item>
    
    <item>
      <title>e2e-node-tests</title>
      <link>/contributors/devel/e2e-node-tests/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/e2e-node-tests/</guid>
      <description>Node End-To-End tests Node e2e tests are component tests meant for testing the Kubelet code on a custom host environment.
Tests can be run either locally or against a host running on GCE.
Node e2e tests are run as both pre- and post- submit tests by the Kubernetes project.
Note: Linux only. Mac and Windows unsupported.
Note: There is no scheduler running. The e2e tests have to do manual scheduling, e.</description>
    </item>
    
    <item>
      <title>e2e-tests</title>
      <link>/contributors/devel/e2e-tests/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/e2e-tests/</guid>
      <description>End-to-End Testing in Kubernetes Table of Contents
 End-to-End Testing in Kubernetes  Overview Building Kubernetes and Running the Tests Cleaning up Advanced testing Installing/updating kubetest Extracting a specific version of kubernetes Bringing up a cluster for testing Federation e2e tests  Configuring federation e2e tests Image Push Repository Build Deploy federation control plane Run the Tests Teardown Shortcuts for test developers  Debugging clusters Local clusters  Testing against local clusters  Version-skewed and upgrade testing  Test jobs naming convention  Kinds of tests Viper configuration and hierarchichal test parameters.</description>
    </item>
    
    <item>
      <title>effective-reviewable</title>
      <link>/sig-contributor-experience/migrated-from-wiki/effective-reviewable/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-contributor-experience/migrated-from-wiki/effective-reviewable/</guid>
      <description>Or, one weird trick to make Reviewable awesome
The Kubernetes team is still new to Reviewable. As you discover new cool features and workflows, add them here. Once we have built up a good number of tricks we can reorganize this list.
 Hold off on publishing comments (using the &amp;ldquo;Publish&amp;rdquo; button) until you have completed your review. (source) When leaving comments, select a &amp;ldquo;disposition&amp;rdquo; (the button with your profile picture) to indicate whether the comment requires resolution, or is just advisory and hence requires no response.</description>
    </item>
    
    <item>
      <title>enabling-kubernetes-ecosystem</title>
      <link>/events/2017/12-contributor-summit/enabling-kubernetes-ecosystem/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/12-contributor-summit/enabling-kubernetes-ecosystem/</guid>
      <description>Kubernetes Ecosystem Notes by @directxman12
 How do we e.g. have an object validator
 Went looking, found 2, second b/c first didn’t work well enough  How do we enable people to build tools that consume and assist with working with Kubernetes
 Kube-fuse, validatators, etc
 How do we make those discoverable
 Difficult to find via GitHub search
 Difficult to find via search engines (find blog posts, etc, and not the tools)</description>
    </item>
    
    <item>
      <title>enhance-pluggable-policy</title>
      <link>/contributors/design-proposals/auth/enhance-pluggable-policy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/auth/enhance-pluggable-policy/</guid>
      <description>Enhance Pluggable Policy While trying to develop an authorization plugin for Kubernetes, we found a few places where API extensions would ease development and add power. There are a few goals: 1. Provide an authorization plugin that can evaluate a .Authorize() call based on the full content of the request to RESTStorage. This includes information like the full verb, the content of creates and updates, and the names of resources being acted upon.</description>
    </item>
    
    <item>
      <title>envvar-configmap</title>
      <link>/contributors/design-proposals/node/envvar-configmap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/envvar-configmap/</guid>
      <description>ConfigMaps as environment variables Goal Populating environment variables of a container from an entire ConfigMap.
Design Points A container can specify a set of existing ConfigMaps to populate environment variables.
There needs to be an easy way to isolate the variables introduced by a given ConfigMap. The contents of a ConfigMap may not be known in advance and it may be generated by someone or something else. Services may provide binding information via a ConfigMap.</description>
    </item>
    
    <item>
      <title>errordeveloper_bio</title>
      <link>/events/elections/2017/errordeveloper_bio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/elections/2017/errordeveloper_bio/</guid>
      <description>Ilya Dmitrchenko Manifesto Ilya has been contributing to Kubernetes since 2014. The focus of his contributions has been on solving general cluster provisioning and bootstrap problems and making networking easier. For example in 2016, he did the groundwork on the initial version of kubeadm.
Ilya is a DX engineer at Weaveworks and is based in London. From there he travels (mostly in Europe) to educate the wider community about the benefits of containers and orchestration with Kubernetes.</description>
    </item>
    
    <item>
      <title>event-style-guide</title>
      <link>/contributors/devel/event-style-guide/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/event-style-guide/</guid>
      <description>Event style guide Status: During Review
Author: Marek Grabowski (gmarek@)
Why the guide? The Event API change proposal is the first step towards having useful Events in the system. Another step is to formalize the Event style guide, i.e. set of properties that developers need to ensure when adding new Events to the system. This is necessary to ensure that we have a system in which all components emit consistently structured Events.</description>
    </item>
    
    <item>
      <title>event_compression</title>
      <link>/contributors/design-proposals/api-machinery/event_compression/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/event_compression/</guid>
      <description>Kubernetes Event Compression This document captures the design of event compression.
Background Kubernetes components can get into a state where they generate tons of events.
The events can be categorized in one of two ways:
 same - The event is identical to previous events except it varies only on timestamp. similar - The event is identical to previous events except it varies on timestamp and message.  For example, when pulling a non-existing image, Kubelet will repeatedly generate image_not_existing and container_is_waiting events until upstream components correct the image.</description>
    </item>
    
    <item>
      <title>events-redesign</title>
      <link>/contributors/design-proposals/instrumentation/events-redesign/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/instrumentation/events-redesign/</guid>
      <description>Make Kubernetes Events useful and safe Status: Pending
Version: Beta
Implementation Owner: gmarek@google.com
Approvers: - [X] thockin - API changes - [X] briangrant - API changes - [X] konryd - API changes from UI/UX side - [X] pszczesniak - logging team side - [X] wojtekt - performance side - [ ] derekwaynecarr - &amp;ldquo;I told you so&amp;rdquo; Events person:)
Overview This document describes an effort which aims at fixing few issues in current way Events are structured and implemented.</description>
    </item>
    
    <item>
      <title>example</title>
      <link>/generator/testdata/example/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/generator/testdata/example/</guid>
      <description>Hello! Example custom content!</description>
    </item>
    
    <item>
      <title>expansion</title>
      <link>/contributors/design-proposals/node/expansion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/expansion/</guid>
      <description>Variable expansion in pod command, args, and env Abstract A proposal for the expansion of environment variables using a simple $(var) syntax.
Motivation It is extremely common for users to need to compose environment variables or pass arguments to their commands using the values of environment variables. Kubernetes should provide a facility for the 80% cases in order to decrease coupling and the use of workarounds.
Goals  Define the syntax format Define the scoping and ordering of substitutions Define the behavior for unmatched variables Define the behavior for unexpected/malformed input  Constraints and Assumptions  This design should describe the simplest possible syntax to accomplish the use-cases.</description>
    </item>
    
    <item>
      <title>extending-api</title>
      <link>/contributors/design-proposals/api-machinery/extending-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/extending-api/</guid>
      <description>Adding custom resources to the Kubernetes API server This document describes the design for implementing the storage of custom API types in the Kubernetes API Server.
Resource Model The ThirdPartyResource The ThirdPartyResource resource describes the multiple versions of a custom resource that the user wants to add to the Kubernetes API. ThirdPartyResource is a non-namespaced resource; attempting to place it in a namespace will return an error.
Each ThirdPartyResource resource has the following: * Standard Kubernetes object metadata.</description>
    </item>
    
    <item>
      <title>extending-kubernetes</title>
      <link>/events/2017/12-contributor-summit/extending-kubernetes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/12-contributor-summit/extending-kubernetes/</guid>
      <description>Extending Kubernetes Note Taker: Clayton Coleman (smarterclayton)
 Questions
 Do we have enough extension mechanisms?
 See below
 Implementing network injection that isn’t a CNI injection of some form is hard
 e.g. adding in arbitrary network devices, for example   Are Flex Volumes enough?
 Maybe?  Are we doing ok on kubectl extensions?
 Yes, we’re heading in the right direction with plugins
 Kubectl itself should be developed using its own mechanisms</description>
    </item>
    
    <item>
      <title>external-lb-source-ip-preservation</title>
      <link>/contributors/design-proposals/network/external-lb-source-ip-preservation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/network/external-lb-source-ip-preservation/</guid>
      <description>Overview  Motivation  Alpha Design  Overview Traffic Steering using LB programming Traffic Steering using Health Checks Choice of traffic steering approaches by individual Cloud Provider implementations API Changes Local Endpoint Recognition Support Service Annotation to opt-in for new behaviour NodePort allocation for HealthChecks Behavior Changes expected External Traffic Blackholed on nodes with no local endpoints Traffic Balancing Changes Cloud Provider support GCE 1.4  GCE Expected Packet Source/Destination IP (Datapath) GCE Expected Packet Destination IP (HealthCheck path)  AWS TBD Openstack TBD Azure TBD Testing  Beta Design  API Changes from Alpha to Beta  Future work Appendix  Overview Kubernetes provides an external loadbalancer service type which creates a virtual external ip (in supported cloud provider environments) that can be used to load-balance traffic to the pods matching the service pod-selector.</description>
    </item>
    
    <item>
      <title>feature-roadmap-2018</title>
      <link>/events/2017/12-contributor-summit/feature-roadmap-2018/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/12-contributor-summit/feature-roadmap-2018/</guid>
      <description>Contributor summit - Kubecon 2017
@AUTHORS - CONNOR DOYLE
@SLIDE AUTHORS ARE THE PRESENTERS
@SKETCHNOTES AUTHOR DAN ROMLEIN
2018 features and roadmap update Presenters: Jaice, Aparna, Ihor, Craig, Caleb
Slidedeck: https://docs.google.com/presentation/d/10AcxtnYFT9Btg_oTV4yWGNRZy41BKK9OjK3rjwtje0g/edit?usp=sharing
What is SIG PM: the &amp;ldquo;periscope&amp;rdquo; of Kubernetes
 They look out for what’s next, translate what’s going on in the community to what that means for Kubernetes
 Responsible for Roadmap and Features Process
  Understanding the release notes is difficult sometimes since the docs aren&amp;rsquo;t always done by the time the release team is compiling the notes.</description>
    </item>
    
    <item>
      <title>feature-workflow</title>
      <link>/events/2017/12-contributor-summit/feature-workflow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/12-contributor-summit/feature-workflow/</guid>
      <description>Feature Workflow Notes by @jberkus
TSC: Getting something done in Kubernetes is byzantine. You need to know someone, who to ask, where to go. If you aren&amp;rsquo;t already involved in the Kubernetes community, it&amp;rsquo;s really hard to get involved. Vendors don&amp;rsquo;t know where to go.
Jeremy: we had to watch the bug tracker to figure out what sig owned the thing we wanted to change.
TSC: so you create a proposal.</description>
    </item>
    
    <item>
      <title>federated-api-servers</title>
      <link>/contributors/design-proposals/multicluster/federated-api-servers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/multicluster/federated-api-servers/</guid>
      <description>Federated API Servers Moved to aggregated-api-servers.md since cluster federation stole the word &amp;ldquo;federation&amp;rdquo; from this effort and it was very confusing.</description>
    </item>
    
    <item>
      <title>federated-hpa</title>
      <link>/contributors/design-proposals/multicluster/federated-hpa/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/multicluster/federated-hpa/</guid>
      <description>Federated Pod Autoscaler Requirements &amp;amp; Design Document irfan.rehman@huawei.com, quinton.hoole@huawei.com
Use cases 1 – Users can schedule replicas of same application, across the federated clusters, using replicaset (or deployment). Users however further might need to let the replicas be scaled independently in each cluster, depending on the current usage metrics of the replicas; including the CPU, memory and application defined custom metrics.
2 - As stated in the previous use case, a federation user schedules replicas of same application, into federated clusters and subsequently creates a horizontal pod autoscaler targeting the object responsible for the replicas.</description>
    </item>
    
    <item>
      <title>federated-ingress</title>
      <link>/contributors/design-proposals/multicluster/federated-ingress/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/multicluster/federated-ingress/</guid>
      <description>Kubernetes Federated Ingress  Requirements and High Level Design Quinton Hoole July 17, 2016  Overview/Summary Kubernetes Ingress provides an abstraction for sophisticated L7 load balancing through a single IP address (and DNS name) across multiple pods in a single Kubernetes cluster. Multiple alternative underlying implementations are provided, including one based on GCE L7 load balancing and another using an in-cluster nginx/HAProxy deployment (for non-GCE environments). An AWS implementation, based on Elastic Load Balancers and Route53 is under way by the community.</description>
    </item>
    
    <item>
      <title>federated-placement-policy</title>
      <link>/contributors/design-proposals/multicluster/federated-placement-policy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/multicluster/federated-placement-policy/</guid>
      <description>Policy-based Federated Resource Placement This document proposes a design for policy-based control over placement of Federated resources.
Tickets:
 https://github.com/kubernetes/kubernetes/issues/39982  Authors:
 Torin Sandall (torin@styra.com, tsandall@github) and Tim Hinrichs (tim@styra.com). Based on discussions with Quinton Hoole (quinton.hoole@huawei.com, quinton-hoole@github), Nikhil Jindal (nikhiljindal@github).  Background Resource placement is a policy-rich problem affecting many deployments. Placement may be based on company conventions, external regulation, pricing and performance requirements, etc. Furthermore, placement policies evolve over time and vary across organizations.</description>
    </item>
    
    <item>
      <title>federated-replicasets</title>
      <link>/contributors/design-proposals/multicluster/federated-replicasets/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/multicluster/federated-replicasets/</guid>
      <description>Federated ReplicaSets Requirements &amp;amp; Design Document This document is a markdown version converted from a working Google Doc. Please refer to the original for extended commentary and discussion.
Author: Marcin Wielgus mwielgus@google.com Based on discussions with Quinton Hoole quinton@google.com, Wojtek Tyczyński wojtekt@google.com
Overview Summary &amp;amp; Vision When running a global application on a federation of Kubernetes clusters the owner currently has to start it in multiple clusters and control whether he has both enough application replicas running locally in each of the clusters (so that, for example, users are handled by a nearby cluster, with low latency) and globally (so that there is always enough capacity to handle all traffic).</description>
    </item>
    
    <item>
      <title>federated-services</title>
      <link>/contributors/design-proposals/multicluster/federated-services/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/multicluster/federated-services/</guid>
      <description>Kubernetes Cluster Federation (previously nicknamed &amp;ldquo;Ubernetes&amp;rdquo;) Cross-cluster Load Balancing and Service Discovery Requirements and System Design by Quinton Hoole, Dec 3 2015 Requirements Discovery, Load-balancing and Failover  Internal discovery and connection: Pods/containers (running in a Kubernetes cluster) must be able to easily discover and connect to endpoints for Kubernetes services on which they depend in a consistent way, irrespective of whether those services exist in a different kubernetes cluster within the same cluster federation.</description>
    </item>
    
    <item>
      <title>federation</title>
      <link>/contributors/design-proposals/multicluster/federation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/multicluster/federation/</guid>
      <description>Kubernetes Cluster Federation (previously nicknamed &amp;ldquo;Ubernetes&amp;rdquo;) Requirements Analysis and Product Proposal by Quinton Hoole (quinton@google.com) Initial revision: 2015-03-05 Last updated: 2015-08-20 This doc: tinyurl.com/ubernetesv2 Original slides: tinyurl.com/ubernetes-slides Updated slides: tinyurl.com/ubernetes-whereto
Introduction Today, each Kubernetes cluster is a relatively self-contained unit, which typically runs in a single &amp;ldquo;on-premise&amp;rdquo; data centre or single availability zone of a cloud provider (Google&amp;rsquo;s GCE, Amazon&amp;rsquo;s AWS, etc).
Several current and potential Kubernetes users and customers have expressed a keen interest in tying together (&amp;ldquo;federating&amp;rdquo;) multiple clusters in some sensible way in order to enable the following kinds of use cases (intentionally vague):</description>
    </item>
    
    <item>
      <title>federation-clusterselector</title>
      <link>/contributors/design-proposals/multicluster/federation-clusterselector/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/multicluster/federation-clusterselector/</guid>
      <description>ClusterSelector Federated Resource Placement This document proposes a design for label based control over placement of Federated resources.
Tickets:
 https://github.com/kubernetes/kubernetes/issues/29887  Authors:
 Dan Wilson (emaildanwilson@github.com). Nikhil Jindal (nikhiljindal@github).  Background End users will often need a simple way to target a subset of clusters for deployment of resources. In some cases this will be for a specific cluster in other cases it will be groups of clusters. A few examples&amp;hellip;</description>
    </item>
    
    <item>
      <title>federation-lite</title>
      <link>/contributors/design-proposals/multicluster/federation-lite/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/multicluster/federation-lite/</guid>
      <description>Kubernetes Multi-AZ Clusters (previously nicknamed &amp;ldquo;Ubernetes-Lite&amp;rdquo;) Introduction Full Cluster Federation will offer sophisticated federation between multiple kubernetes clusters, offering true high-availability, multiple provider support &amp;amp; cloud-bursting, multiple region support etc. However, many users have expressed a desire for a &amp;ldquo;reasonably&amp;rdquo; high-available cluster, that runs in multiple zones on GCE or availability zones in AWS, and can tolerate the failure of a single zone without the complexity of running multiple clusters.</description>
    </item>
    
    <item>
      <title>federation-phase-1</title>
      <link>/contributors/design-proposals/multicluster/federation-phase-1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/multicluster/federation-phase-1/</guid>
      <description>Ubernetes Design Spec (phase one) Huawei PaaS Team
INTRODUCTION In this document we propose a design for the “Control Plane” of Kubernetes (K8S) federation (a.k.a. “Ubernetes”). For background of this work please refer to this proposal. The document is arranged as following. First we briefly list scenarios and use cases that motivate K8S federation work. These use cases drive the design and they also verify the design. We summarize the functionality requirements from these use cases, and define the “in scope” functionalities that will be covered by this design (phase one).</description>
    </item>
    
    <item>
      <title>fixit201606</title>
      <link>/events/2016/fixit201606/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2016/fixit201606/</guid>
      <description>#Fixit Event June 2016
Google runs internal fixit weeks. During these weeks the teams set aside all critical deadlines, showstopper bugs, regular development &amp;ndash; everything &amp;ndash; to pull together to achieve a common goal. And, we invite the Kubernetes community to join the June 2016 fixit.
Please take a look at anything that is kind/flake or, with our 1.4 focus on &amp;ldquo;mean time to dopamine&amp;rdquo; for our users, help team/ux or spend time triaging, de-duplicating, or closing issues that were opened before 20160101 or check out the issues in our docs repository.</description>
    </item>
    
    <item>
      <title>flakiness-sla</title>
      <link>/contributors/design-proposals/testing/flakiness-sla/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/testing/flakiness-sla/</guid>
      <description>Kubernetes Testing Flakiness SLA This document captures the expectations of the community about flakiness in our tests and our test infrastructure. It sets out an SLA (Service Level Agreement) for flakiness in our tests, as well as actions that we will take when we are out of SLA.
Definition of &amp;ldquo;We&amp;rdquo; Throughout the document the term we is used. This is intended to refer to the Kubernetes project as a whole, and any governance structures the project puts in place.</description>
    </item>
    
    <item>
      <title>flaky-tests</title>
      <link>/contributors/devel/flaky-tests/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/flaky-tests/</guid>
      <description>Flaky tests Any test that fails occasionally is &amp;ldquo;flaky&amp;rdquo;. Since our merges only proceed when all tests are green, and we have a number of different CI systems running the tests in various combinations, even a small percentage of flakes results in a lot of pain for people waiting for their PRs to merge.
Therefore, it&amp;rsquo;s very important that we write tests defensively. Situations that &amp;ldquo;almost never happen&amp;rdquo; happen with some regularity when run thousands of times in resource-constrained environments.</description>
    </item>
    
    <item>
      <title>flannel-integration</title>
      <link>/contributors/design-proposals/network/flannel-integration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/network/flannel-integration/</guid>
      <description>Flannel integration with Kubernetes Why?  Networking works out of the box. Cloud gateway configuration is regulated by quota. Consistent bare metal and cloud experience. Lays foundation for integrating with networking backends and vendors.  How? Thus:
Master | Node1 ---------------------------------------------------------------------- {192.168.0.0/16, 256 /24} | docker | | | restart with podcidr apiserver &amp;lt;------------------ kubelet (sends podcidr) | | | here&#39;s podcidr, mtu flannel-server:10253 &amp;lt;------------------ flannel-daemon Allocates a /24 ------------------&amp;gt; [config iptables, VXLan] &amp;lt;------------------ [watch subnet leases] I just allocated ------------------&amp;gt; [config VXLan] another /24 |  Proposal Explaining vxlan is out of the scope of this document, however it does take some basic understanding to grok the proposal.</description>
    </item>
    
    <item>
      <title>flex-volumes-drivers-psp</title>
      <link>/contributors/design-proposals/auth/flex-volumes-drivers-psp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/auth/flex-volumes-drivers-psp/</guid>
      <description>Allow Pod Security Policy to manage access to the Flexvolumes Current state Cluster admins can control the usage of specific volume types by using Pod Security Policy (PSP). Admins can allow the use of Flexvolumes by listing the flexVolume type in the volumes field. The only thing that can be managed is allowance or disallowance of Flexvolumes.
Technically, Flexvolumes are implemented as vendor drivers. They are executable files that must be placed on every node at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/&amp;lt;vendor~driver&amp;gt;/&amp;lt;driver&amp;gt;.</description>
    </item>
    
    <item>
      <title>flexvolume</title>
      <link>/contributors/devel/flexvolume/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/flexvolume/</guid>
      <description>Flexvolume Flexvolume enables users to write their own drivers and add support for their volumes in Kubernetes. Vendor drivers should be installed in the volume plugin path on every Kubelet node and on master node(s) if --enable-controller-attach-detach Kubelet option is enabled.
Flexvolume is a GA feature from Kubernetes 1.8 release onwards.
Prerequisites Install the vendor driver on all nodes (also on master nodes if &amp;ldquo;&amp;ndash;enable-controller-attach-detach&amp;rdquo; Kubelet option is enabled) in the plugin path.</description>
    </item>
    
    <item>
      <title>flexvolume-deployment</title>
      <link>/contributors/design-proposals/storage/flexvolume-deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/flexvolume-deployment/</guid>
      <description>Dynamic Flexvolume Plugin Discovery Objective Kubelet and controller-manager do not need to be restarted manually in order for new Flexvolume plugins to be recognized.
Background Beginning in version 1.8, the Kubernetes Storage SIG is putting a stop to accepting in-tree volume plugins and advises all storage providers to implement out-of-tree plugins. Currently, there are two recommended implementations: Container Storage Interface (CSI) and Flexvolume.
CSI provides a single interface that storage vendors can implement in order for their storage solutions to work across many different container orchestrators, and volume plugins are out-of-tree by design.</description>
    </item>
    
    <item>
      <title>formal-scalability-processes</title>
      <link>/sig-scalability/processes/formal-scalability-processes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-scalability/processes/formal-scalability-processes/</guid>
      <description>Formal Scalability Processes by Shyam JVS, Google Inc
February 2018
Introduction Scalability is a very crucial aspect of kubernetes and has allowed many customers to adopt it with confidence. K8s started scaling to 5000 nodes beginning from release 1.6. Building and maintaining a performant and scalable system needs conscious efforts from the whole developer community. Lack of solid measures have caused problems (both scalability and release-related) in the past - for e.</description>
    </item>
    
    <item>
      <title>garbage-collection</title>
      <link>/contributors/design-proposals/api-machinery/garbage-collection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/garbage-collection/</guid>
      <description>Table of Contents
 Overview Cascading deletion with Garbage Collector Orphaning the descendants with &amp;ldquo;orphan&amp;rdquo; finalizer  Part I. The finalizer framework Part II. The &amp;ldquo;orphan&amp;rdquo; finalizer  Related issues  Orphan adoption Upgrading a cluster to support cascading deletion  End-to-End Examples  Life of a Deployment and its descendants  Open Questions Considered and Rejected Designs 1. Tombstone + GC 2. Recovering from abnormal cascading deletion  Overview Currently most cascading deletion logic is implemented at client-side.</description>
    </item>
    
    <item>
      <title>gce-l4-loadbalancer-healthcheck</title>
      <link>/contributors/design-proposals/gcp/gce-l4-loadbalancer-healthcheck/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/gcp/gce-l4-loadbalancer-healthcheck/</guid>
      <description>GCE L4 load-balancers&amp;rsquo; health checks for nodes Goal Set up health checks for GCE L4 load-balancer to ensure it is only targeting healthy nodes.
Motivation On cloud providers which support external load balancers, setting the type field to &amp;ldquo;LoadBalancer&amp;rdquo; will provision a L4 load-balancer for the service (doc), which load-balances traffic to k8s nodes. As of k8s 1.6, we don&amp;rsquo;t create health check for L4 load-balancer by default, which means all traffic will be forwarded to any one of the nodes blindly.</description>
    </item>
    
    <item>
      <title>generating-clientset</title>
      <link>/contributors/devel/generating-clientset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/generating-clientset/</guid>
      <description>Generation and release cycle of clientset Client-gen is an automatic tool that generates clientset based on API types. This doc introduces the use the client-gen, and the release cycle of the generated clientsets.
Using client-gen The workflow includes three steps:
1. Marking API types with tags: in pkg/apis/${GROUP}/${VERSION}/types.go, mark the types (e.g., Pods) that you want to generate clients for with the // +genclient tag. If the resource associated with the type is not namespace scoped (e.</description>
    </item>
    
    <item>
      <title>get-describe-apiserver-extensions</title>
      <link>/contributors/design-proposals/cli/get-describe-apiserver-extensions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/cli/get-describe-apiserver-extensions/</guid>
      <description>Provide open-api extensions for kubectl get / kubectl describe columns Status: Pending
Version: Alpha
Motivation kubectl get and kubectl describe do not provide a rich experience for resources retrieved through federated apiservers and types not compiled into the kubectl binary. Kubectl should support printing columns configured per-type without having the types compiled in.
Proposal Allow the apiserver to define the type specific columns that will be printed using the open-api swagger.</description>
    </item>
    
    <item>
      <title>getting-builds</title>
      <link>/contributors/devel/getting-builds/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/getting-builds/</guid>
      <description>Getting Kubernetes Builds You can use hack/get-build.sh to get a build or to use as a reference on how to get the most recent builds with curl. With get-build.sh you can grab the most recent stable build, the most recent release candidate, or the most recent build to pass our ci and gce e2e tests (essentially a nightly build).
Run ./hack/get-build.sh -h for its usage.
To get a build at a specific version (v1.</description>
    </item>
    
    <item>
      <title>github-workflow</title>
      <link>/contributors/guide/github-workflow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/guide/github-workflow/</guid>
      <description>Workflow 1 Fork in the cloud  Visit https://github.com/kubernetes/kubernetes Click Fork button (top right) to establish a cloud-based fork.  2 Clone fork to local storage Per Go&amp;rsquo;s workspace instructions, place Kubernetes&amp;rsquo; code on your GOPATH using the following cloning procedure.
Define a local working directory:
# If your GOPATH has multiple paths, pick # just one and use it instead of $GOPATH here. # You must follow exactly this pattern, # neither `$GOPATH/src/github.</description>
    </item>
    
    <item>
      <title>goals</title>
      <link>/sig-scalability/goals/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-scalability/goals/</guid>
      <description>Kubernetes Scaling and Performance Goals by Quinton Hoole and Wojciech Tyczynski, Google Inc
April 2016
This document is a markdown version converted from a working Google Doc. Please refer to the original for extended commentary and discussion.
Introduction What size clusters do we think that we should support with Kubernetes in the short to medium term? How performant do we think that the control system should be at scale? What resource overhead should the Kubernetes control system reasonably consume?</description>
    </item>
    
    <item>
      <title>godep</title>
      <link>/contributors/devel/godep/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/godep/</guid>
      <description>Using godep to manage dependencies This document is intended to show a way for managing vendor/ tree dependencies in Kubernetes. If you do not need to manage vendored dependencies, you probably do not need to read this.
Background As a tool, godep leaves much to be desired. It builds on go get, and adds the ability to pin dependencies to exact git version. The go get tool itself doesn&amp;rsquo;t have any concept of versions, and tends to blow up if it finds a git repo synced to anything but master, but that is exactly the state that godep leaves repos.</description>
    </item>
    
    <item>
      <title>google-summer-of-code</title>
      <link>/mentoring/google-summer-of-code/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/mentoring/google-summer-of-code/</guid>
      <description>Google Summer of Code Kubernetes is happy to announce that we have applied to participate in the Google Summer of Code (GSoC) program under the Cloud Native Computing Foundation (CNCF), running from January 2018 to August 2018.
Please see the main program page for general information about the program, such as its purpose, timeline, eligibility requirements, and how to apply.
Schedule The following schedule is taken from the official timeline.</description>
    </item>
    
    <item>
      <title>governance</title>
      <link>/governance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/governance/</guid>
      <description>This is a Work in Progress, documenting approximately how we have been operating up to this point.
Principles The Kubernetes community adheres to the following principles: * Open: Kubernetes is open source. See repository guidelines and CLA, below. * Welcoming and respectful: See Code of Conduct, below. * Transparent and accessible: Work and collaboration should be done in public. See SIG governance, below. * Merit: Ideas and contributions are accepted according to their technical merit and alignment with project objectives, scope, and design principles.</description>
    </item>
    
    <item>
      <title>gpu-support</title>
      <link>/contributors/design-proposals/resource-management/gpu-support/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/resource-management/gpu-support/</guid>
      <description>GPU support  Objective Background Detailed discussion Inventory Scheduling The runtime  NVIDIA support  Event flow Too complex for now: nvidia-docker Implementation plan V0  Scheduling Runtime Other  Future work V1 V2 V3 Undetermined Security considerations   GPU support Author: @therc
Date: Apr 2016
Status: Design in progress, early implementation of requirements
Objective Users should be able to request GPU resources for their workloads, as easily as for CPU or memory.</description>
    </item>
    
    <item>
      <title>group-mentee-guide</title>
      <link>/mentoring/group-mentee-guide/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/mentoring/group-mentee-guide/</guid>
      <description>Mentee Guide for Group Mentoring This doc is a work in progress
Welcome and Summary Welcome prospective mentee! This is an experimental group mentoring program where individuals will work self-paced in a semi-structured learning environment over the course of three months. Want to get lgtm rights? Be a decision maker in an OWNERS file? This could be a great way to get the knowledge you need, some accountability to do it, and interact with active contributors who may on the same path as you and our experienced Kubernetes developers.</description>
    </item>
    
    <item>
      <title>group-mentoring</title>
      <link>/mentoring/group-mentoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/mentoring/group-mentoring/</guid>
      <description>Group Mentoring for Contributor Ladder Growth This is a work in progress
Summary This is an experimental group mentoring program where individuals will work self-paced in a semi-structured learning environment over the course of three months. Want to get /lgtm rights? Be a decision maker in an OWNERS file? This could be a great way to get the knowledge you need, some accountability to do it. Through this program you will get the opportunity to interact with active contributors who may be on the same path as you as well very experienced Kubernetes contributors (Pilots).</description>
    </item>
    
    <item>
      <title>grow-flexvolume-size</title>
      <link>/contributors/design-proposals/storage/grow-flexvolume-size/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/grow-flexvolume-size/</guid>
      <description>Proposal for Growing FlexVolume Size Authors: xingzhou
Goals Since PVC resizing is introduced in Kubernetes v1.8, several volume plugins have already supported this feature, e.g. GlusterFS, AWS EBS. In this proposal, we are proposing to support FlexVolume expansion. So when user uses FlexVolume and corresponding volume driver to connect to his/her backend storage system, he/she can expand the PV size by updating PVC in Kubernetes.
Non Goals  We only consider expanding FlexVolume size in this proposal.</description>
    </item>
    
    <item>
      <title>grow-volume-size</title>
      <link>/contributors/design-proposals/storage/grow-volume-size/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/grow-volume-size/</guid>
      <description>Growing Persistent Volume size Goals Enable users to increase size of PVs that their pods are using. The user will update PVC for requesting a new size. Underneath we expect that - a controller will apply the change to PV which is bound to the PVC.
Non Goals  Reducing size of Persistent Volumes: We realize that, reducing size of PV is way riskier than increasing it. Reducing size of a PV could be a destructive operation and it requires support from underlying file system and volume type.</description>
    </item>
    
    <item>
      <title>gubernator</title>
      <link>/contributors/devel/gubernator/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/gubernator/</guid>
      <description>Gubernator This document is oriented at developers who want to use Gubernator to debug while developing for Kubernetes.
 Gubernator  What is Gubernator? Gubernator Features Test Failures list Log Filtering Gubernator for Local Tests Future Work   What is Gubernator? Gubernator is a webpage for viewing and filtering Kubernetes test results.
Gubernator simplifies the debugging process and makes it easier to track down failures by automating many steps commonly taken in searching through logs, and by offering tools to filter through logs to find relevant lines.</description>
    </item>
    
    <item>
      <title>ha_master</title>
      <link>/contributors/design-proposals/cluster-lifecycle/ha_master/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/cluster-lifecycle/ha_master/</guid>
      <description>Automated HA master deployment Author: filipg@, jsz@
Introduction We want to allow users to easily replicate kubernetes masters to have highly available cluster, initially using kube-up.sh and kube-down.sh.
This document describes technical design of this feature. It assumes that we are using aforementioned scripts for cluster deployment. All of the ideas described in the following sections should be easy to implement on GCE, AWS and other cloud providers.
It is a non-goal to design a specific setup for bare-metal environment, which might be very different.</description>
    </item>
    
    <item>
      <title>help-wanted</title>
      <link>/contributors/devel/help-wanted/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/help-wanted/</guid>
      <description>Overview We use two labels help wanted and good first issue to identify issues that have been specially groomed for new contributors. The good first issue label is a subset of help wanted label, indicating that members have committed to providing extra assistance for new contributors. All good first issue items also have the help wanted label.
We also have some suggestions for using these labels to help grow and improve our community.</description>
    </item>
    
    <item>
      <title>high-availability</title>
      <link>/contributors/design-proposals/cluster-lifecycle/high-availability/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/cluster-lifecycle/high-availability/</guid>
      <description>High Availability of Scheduling and Controller Components in Kubernetes This document is deprecated. For more details about running a highly available cluster master, please see the admin instructions document.</description>
    </item>
    
    <item>
      <title>horizontal-pod-autoscaler</title>
      <link>/contributors/design-proposals/autoscaling/horizontal-pod-autoscaler/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/autoscaling/horizontal-pod-autoscaler/</guid>
      <description>Warning! This document might be outdated. Horizontal Pod Autoscaling Preface This document briefly describes the design of the horizontal autoscaler for pods. The autoscaler (implemented as a Kubernetes API resource and controller) is responsible for dynamically controlling the number of replicas of some collection (e.g. the pods of a ReplicationController) to meet some objective(s), for example a target per-pod CPU utilization.
This design supersedes autoscaling.md.
Overview The resource usage of a serving application usually varies over time: sometimes the demand for the application rises, and sometimes it drops.</description>
    </item>
    
    <item>
      <title>how-to-doc</title>
      <link>/contributors/devel/how-to-doc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/how-to-doc/</guid>
      <description>Document Conventions Updated: 11/3/2017
Users and developers who want to write documents for Kubernetes can get started here.</description>
    </item>
    
    <item>
      <title>hpa-external-metrics</title>
      <link>/contributors/design-proposals/autoscaling/hpa-external-metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/autoscaling/hpa-external-metrics/</guid>
      <description>HPA v2 API extension proposal Objective Horizontal Pod Autoscaler v2 API allows users to autoscale based on custom metrics. However, there are some use-cases that are not well supported by the current API. The goal of this document is to propose the following changes to the API:
 Allow autoscaling based on metrics coming from outside of Kubernetes. Example use-case is autoscaling based on a hosted cloud service used by a pod.</description>
    </item>
    
    <item>
      <title>hpa-status-conditions</title>
      <link>/contributors/design-proposals/autoscaling/hpa-status-conditions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/autoscaling/hpa-status-conditions/</guid>
      <description>Horizontal Pod Autoscaler Status Conditions Currently, the HPA status conveys the last scale time, current and desired replicas, and the last-retrieved values of the metrics used to autoscale.
However, the status field conveys no information about whether or not the HPA controller encountered difficulties while attempting to fetch metrics, or to scale. While this information is generally conveyed via events, events are difficult to use to determine the current state of the HPA.</description>
    </item>
    
    <item>
      <title>hpa-v2</title>
      <link>/contributors/design-proposals/autoscaling/hpa-v2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/autoscaling/hpa-v2/</guid>
      <description>Horizontal Pod Autoscaler with Arbitrary Metrics The current Horizontal Pod Autoscaler object only has support for CPU as a percentage of requested CPU. While this is certainly a common case, one of the most frequently sought-after features for the HPA is the ability to scale on different metrics (be they custom metrics, memory, etc).
The current HPA controller supports targeting &amp;ldquo;custom&amp;rdquo; metrics (metrics with a name prefixed with &amp;ldquo;custom/&amp;rdquo;) via an annotation, but this is suboptimal for a number of reasons: it does not allow for arbitrary &amp;ldquo;non-custom&amp;rdquo; metrics (e.</description>
    </item>
    
    <item>
      <title>hugepages</title>
      <link>/contributors/design-proposals/resource-management/hugepages/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/resource-management/hugepages/</guid>
      <description>HugePages support in Kubernetes Authors * Derek Carr (@derekwaynecarr) * Seth Jennings (@sjenning) * Piotr Prokop (@PiotrProkop)
Status: In progress
Abstract A proposal to enable applications running in a Kubernetes cluster to use huge pages.
A pod may request a number of huge pages. The scheduler is able to place the pod on a node that can satisfy that request. The kubelet advertises an allocatable number of huge pages to support scheduling decisions.</description>
    </item>
    
    <item>
      <title>identifiers</title>
      <link>/contributors/design-proposals/architecture/identifiers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/architecture/identifiers/</guid>
      <description>Identifiers and Names in Kubernetes A summarization of the goals and recommendations for identifiers in Kubernetes. Described in GitHub issue #199.
Definitions UID: A non-empty, opaque, system-generated value guaranteed to be unique in time and space; intended to distinguish between historical occurrences of similar entities.
Name: A non-empty string guaranteed to be unique within a given scope at a particular time; used in resource URLs; provided by clients at creation time and encouraged to be human friendly; intended to facilitate creation idempotence and space-uniqueness of singleton objects, distinguish distinct entities, and reference particular entities across operations.</description>
    </item>
    
    <item>
      <title>idvoretskyi_bio</title>
      <link>/events/elections/2017/idvoretskyi_bio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/elections/2017/idvoretskyi_bio/</guid>
      <description>Ihor Dvoretskyi Twitter: https://twitter.com/idvoretskyi
GitHub: https://github.com/idvoretskyi
Anywhere else: @idvoretskyi
About me I&amp;rsquo;m a Developer Advocate for Cloud Native Computing Foundation, open source lover, passionate about the technologies and open source adoption; with the deep technical background, together with program and product management experience in the open source world. In a previous life, I’ve been a System Administrator and DevOps Engineer, passionate about the Cloud technologies. Later, I’ve joined Mirantis, one of the largest players in the OpenStack world, where I’ve started my collaboration with OpenStack community.</description>
    </item>
    
    <item>
      <title>image-provenance</title>
      <link>/contributors/design-proposals/auth/image-provenance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/auth/image-provenance/</guid>
      <description>Overview Organizations wish to avoid running &amp;ldquo;unapproved&amp;rdquo; images.
The exact nature of &amp;ldquo;approval&amp;rdquo; is beyond the scope of Kubernetes, but may include reasons like:
 only run images that are scanned to confirm they do not contain vulnerabilities only run images that use a &amp;ldquo;required&amp;rdquo; base image only run images that contain binaries which were built from peer reviewed, checked-in source by a trusted compiler toolchain. only allow images signed by certain public keys.</description>
    </item>
    
    <item>
      <title>incubator</title>
      <link>/incubator/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/incubator/</guid>
      <description>IMPORTANT - The Kubernetes Incubator process is now deprecated and has been superseded by Kubernetes subprojects For information on creating a repository for a subproject see: kubernetes-repositories
Each SIG should define the process for sponsoring new subprojects in its charter. For information on SIG governance and charters see: SIG governance
Kubernetes Incubator Process Authors: Brandon Philips brandon.philips@coreos.com, Sarah Novotny sarahnovotny@google.com, Brian Grant briangrant@google.com
Kubernetes Incubator The Kubernetes Incubator is where all new Kubernetes projects should start.</description>
    </item>
    
    <item>
      <title>indexed-job</title>
      <link>/contributors/design-proposals/apps/indexed-job/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/apps/indexed-job/</guid>
      <description>Design: Indexed Feature of Job object Summary This design extends kubernetes with user-friendly support for running embarrassingly parallel jobs.
Here, parallel means on multiple nodes, which means multiple pods. By embarrassingly parallel, it is meant that the pods have no dependencies between each other. In particular, neither ordering between pods nor gang scheduling are supported.
Users already have two other options for running embarrassingly parallel Jobs (described in the next section), but both have ease-of-use issues.</description>
    </item>
    
    <item>
      <title>initial-resources</title>
      <link>/contributors/design-proposals/autoscaling/initial-resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/autoscaling/initial-resources/</guid>
      <description>Abstract Initial Resources is a data-driven feature that based on historical data tries to estimate resource usage of a container without Resources specified and set them before the container is run. This document describes design of the component.
Motivation Since we want to make Kubernetes as simple as possible for its users we don&amp;rsquo;t want to require setting Resources for container by its owner. On the other hand having Resources filled is critical for scheduling decisions.</description>
    </item>
    
    <item>
      <title>instrumentation</title>
      <link>/contributors/devel/instrumentation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/instrumentation/</guid>
      <description>Instrumenting Kubernetes The following references and outlines general guidelines for metric instrumentation in Kubernetes components. Components are instrumented using the Prometheus Go client library. For non-Go components. Libraries in other languages are available.
The metrics are exposed via HTTP in the Prometheus metric format, which is open and well-understood by a wide range of third party applications and vendors outside of the Prometheus eco-system.
The general instrumentation advice from the Prometheus documentation applies.</description>
    </item>
    
    <item>
      <title>issue-triage</title>
      <link>/contributors/guide/issue-triage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/guide/issue-triage/</guid>
      <description>Kubernetes Issue Triage Guidelines Purpose Speed up issue management.
The Kubernetes issues are listed at https://github.com/kubernetes/kubernetes/issues and are identified with labels. For example, an issue that belongs to SIG Network group will eventually be set to label sig/network. New issues will start out without any labels. The detailed list of labels can be found at https://github.com/kubernetes/kubernetes/labels. While working on triaging issues you may not have privilege to assign specific label (e.</description>
    </item>
    
    <item>
      <title>issues</title>
      <link>/contributors/devel/issues/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/issues/</guid>
      <description>+NOTE: This document has moved to a new location</description>
    </item>
    
    <item>
      <title>jaicesingerdumars_bio</title>
      <link>/events/elections/2017/jaicesingerdumars_bio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/elections/2017/jaicesingerdumars_bio/</guid>
      <description>Jaice Singer DuMars Contact Information  GitHub: @jdumars Twitter: @jaydumars LinkedIn: Jaice Singer DuMars Works for: Microsoft  Why me? The steering committee represents a unique convergence of my passion for servant leadership, organizational dynamics, inclusion, advocacy, and process optimization. It&amp;rsquo;s always my highest purpose to be a voice and advocate for those I serve. And, if elected, I will faithfully serve the ever-changing, sometimes divergent, and always important needs of the Kubernetes community.</description>
    </item>
    
    <item>
      <title>job</title>
      <link>/contributors/design-proposals/apps/job/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/apps/job/</guid>
      <description>Job Controller Abstract A proposal for implementing a new controller - Job controller - which will be responsible for managing pod(s) that require running once to completion even if the machine the pod is running on fails, in contrast to what ReplicationController currently offers.
Several existing issues and PRs were already created regarding that particular subject: * Job Controller #1624 * New Job resource #7380
Use Cases  Be able to start one or several pods tracked as a single entity.</description>
    </item>
    
    <item>
      <title>k8s-services-scalability-issues</title>
      <link>/sig-scalability/blogs/k8s-services-scalability-issues/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-scalability/blogs/k8s-services-scalability-issues/</guid>
      <description>Known Scalability Issues with Kubernetes Services by Shyam JVS, Google Inc (with inputs from Brian Grant, Wojciech Tyczynski &amp;amp; Dan Winship)
June 2018
This document serves as a catalog of issues we&amp;rsquo;ve known/discovered with kubernetes services as of June 2018, focusing on their scalability/performance. The purpose of the document is to make the information common knowledge for the community, so we can work together towards improving it. Listing them below in no particular order.</description>
    </item>
    
    <item>
      <title>kms-plugin-grpc-api</title>
      <link>/contributors/design-proposals/auth/kms-plugin-grpc-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/auth/kms-plugin-grpc-api/</guid>
      <description>KMS Plugin API for secrets encryption Background Since v1.7, Kubernetes allows encryption of resources. It supports 3 kinds of encryptions: aescbc, aesgcm and secretbox. They are implemented as value transformer. This feature currently only supports encryption using keys in the configuration file (plain text, encoded with base64).
Using an external trusted service to manage the keys separates the responsibility of key management from operating and managing a Kubernetes cluster. So a new transformer, “Envelope Transformer”, was introduced in 1.</description>
    </item>
    
    <item>
      <title>kris-nova_bio</title>
      <link>/events/elections/2017/kris-nova_bio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/elections/2017/kris-nova_bio/</guid>
      <description>Kris Nova What is important to me Mental, philosophical, and design diversity. Period.
I truly believe that the best teams and projects are those that embrace unique and sometimes conflicting ways of thinking.
Having a leadership board of people who all think the same way, and all are moving in the right direction is stale and stagnant.
 I want to help foster diversity in Kubernetes in both our design process, but also our goals and philosophies.</description>
    </item>
    
    <item>
      <title>kubeadm-1.6</title>
      <link>/sig-cluster-lifecycle/postmortems/kubeadm-1.6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-cluster-lifecycle/postmortems/kubeadm-1.6/</guid>
      <description>Kubernetes Postmortem: kubeadm 1.6.0 release Incident Date: 2017-03-28
Owners: Jacob Beacham (@pipejakob)
Collaborators: Joe Beda (@jbeda), Mike Danese (@mikedanese), Robert Bailey (@roberthbailey)
Status: [draft | pending feedback | final]
Summary: kubeadm 1.6.0 consistently hangs while trying to initialize new clusters. A fix required creating the 1.6.1 patch release six days after 1.6.0.
Impact: Initialization of a new 1.6.0 master using kubeadm.
Root Causes: kubelet’s behavior was changed to report NotReady instead of Ready when CNI was unconfigured (#43474), which caused kubeadm to hang indefinitely on initialization while waiting for the master node to become Ready (and then schedule a dummy deployment) in order to validate the control plane’s health, which was intended to happen before a CNI provider gets added.</description>
    </item>
    
    <item>
      <title>kubectl-conventions</title>
      <link>/contributors/devel/kubectl-conventions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/kubectl-conventions/</guid>
      <description>Kubectl Conventions Updated: 3/23/2017
Table of Contents
 Kubectl Conventions  Principles Command conventions Create commands Rules for extending special resource alias - &amp;ldquo;all&amp;rdquo; Flag conventions Output conventions Documentation conventions kubectl Factory conventions Command implementation conventions Exit code conventions Generators   Principles  Strive for consistency across commands
 Explicit should always override implicit
 Environment variables should override default values
 Command-line flags should override default values and environment variables</description>
    </item>
    
    <item>
      <title>kubectl-create-from-env-file</title>
      <link>/contributors/design-proposals/cli/kubectl-create-from-env-file/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/cli/kubectl-create-from-env-file/</guid>
      <description>Kubectl create configmap/secret &amp;ndash;env-file Goals Allow a Docker environment file (.env) to populate an entire ConfigMap or Secret. The populated ConfigMap or Secret can be referenced by a pod to load all the data contained within.
Design The create configmap subcommand would add a new option called --from-env-file. The option will accept a single file. The option may not be used in conjunction with --from-file or --from-literal.
The create secret generic subcommand would add a new option called --from-env-file.</description>
    </item>
    
    <item>
      <title>kubectl-exec-plugins</title>
      <link>/contributors/design-proposals/auth/kubectl-exec-plugins/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/auth/kubectl-exec-plugins/</guid>
      <description>Out-of-tree client authentication providers Author: @ericchiang
Objective This document describes a credential rotation strategy for client-go using an exec-based plugin mechanism.
Motivation Kubernetes clients can provide three kinds of credentials: bearer tokens, TLS client certs, and basic authentication username and password. Kubeconfigs can either in-line the credential, load credentials from a file, or can use an AuthProvider to actively fetch and rotate credentials. AuthProviders are compiled into client-go and target specific providers (GCP, Keystone, Azure AD) or implement a specification supported but a subset of vendors (OpenID Connect).</description>
    </item>
    
    <item>
      <title>kubectl-extension</title>
      <link>/contributors/design-proposals/cli/kubectl-extension/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/cli/kubectl-extension/</guid>
      <description>Kubectl Extension Abstract Allow kubectl to be extended to include other commands that can provide new functionality without recompiling Kubectl
Motivation and Background Kubernetes is designed to be a composable and extensible system, with the ability to add new APIs and features via Third Party Resources or API federation, by making the server provide functionality that eases writing generic clients, and by supporting other authentication systems. Given that kubectl is the primary method for interacting with the server, some new extensions are difficult to make usable for end users without recompiling that command.</description>
    </item>
    
    <item>
      <title>kubectl-login</title>
      <link>/contributors/design-proposals/cli/kubectl-login/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/cli/kubectl-login/</guid>
      <description>Kubectl Login Subcommand Authors: Eric Chiang (@ericchiang)
Goals kubectl login is an entrypoint for any user attempting to connect to an existing server. It should provide a more tailored experience than the existing kubectl config including config validation, auth challenges, and discovery.
Short term the subcommand should recognize and attempt to help:
 New users with an empty configuration trying to connect to a server. Users with no credentials, by prompt for any required information.</description>
    </item>
    
    <item>
      <title>kubectl_apply_getsetdiff_last_applied_config</title>
      <link>/contributors/design-proposals/cli/kubectl_apply_getsetdiff_last_applied_config/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/cli/kubectl_apply_getsetdiff_last_applied_config/</guid>
      <description>Kubectl apply subcommands for last-config Abstract kubectl apply uses the last-applied-config annotation to compute the removal of fields from local object configuration files and then send patches to delete those fields from the live object. Reading or updating the last-applied-config is complex as it requires parsing out and writing to the annotation. Here we propose a set of porcelain commands for users to better understand what is going on in the system and make updates.</description>
    </item>
    
    <item>
      <title>kubelet-auth</title>
      <link>/contributors/design-proposals/node/kubelet-auth/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/kubelet-auth/</guid>
      <description>Kubelet Authentication / Authorization Author: Jordan Liggitt (jliggitt@redhat.com)
Overview The kubelet exposes endpoints which give access to data of varying sensitivity, and allow performing operations of varying power on the node and within containers. There is no built-in way to limit or subdivide access to those endpoints, so deployers must secure the kubelet API using external, ad-hoc methods.
This document proposes a method for authenticating and authorizing access to the kubelet API, using interfaces and methods that complement the existing authentication and authorization used by the API server.</description>
    </item>
    
    <item>
      <title>kubelet-authorizer</title>
      <link>/contributors/design-proposals/node/kubelet-authorizer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/kubelet-authorizer/</guid>
      <description>Scoped Kubelet API Access Author: Jordan Liggitt (jliggitt@redhat.com)
Overview Kubelets are primarily responsible for: * creating and updating status of their Node API object * running and updating status of Pod API objects bound to their node * creating/deleting &amp;ldquo;mirror pod&amp;rdquo; API objects for statically-defined pods running on their node
To run a pod, a kubelet must have read access to the following objects referenced by the pod spec: * Secrets * ConfigMaps * PersistentVolumeClaims (and any bound PersistentVolume or referenced StorageClass object)</description>
    </item>
    
    <item>
      <title>kubelet-cri-logging</title>
      <link>/contributors/design-proposals/node/kubelet-cri-logging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/kubelet-cri-logging/</guid>
      <description>CRI: Log management for container stdout/stderr streams Goals and non-goals Container Runtime Interface (CRI) is an ongoing project to allow container runtimes to integrate with kubernetes via a newly-defined API. The goal of this proposal is to define how container&amp;rsquo;s stdout/stderr log streams should be handled in CRI.
The explicit non-goal is to define how (non-stdout/stderr) application logs should be handled. Collecting and managing arbitrary application logs is a long-standing issue [1] in kubernetes and is worth a proposal of its own.</description>
    </item>
    
    <item>
      <title>kubelet-cri-networking</title>
      <link>/contributors/devel/kubelet-cri-networking/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/kubelet-cri-networking/</guid>
      <description>Container Runtime Interface (CRI) Networking Specifications Introduction Container Runtime Interface (CRI) is an ongoing project to allow container runtimes to integrate with kubernetes via a newly-defined API. This document specifies the network requirements for container runtime interface (CRI). CRI networking requirements expand upon kubernetes pod networking requirements. This document does not specify requirements from upper layers of kubernetes network stack, such as Service. More background on k8s networking could be found here</description>
    </item>
    
    <item>
      <title>kubelet-eviction</title>
      <link>/contributors/design-proposals/node/kubelet-eviction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/kubelet-eviction/</guid>
      <description>Kubelet - Eviction Policy Authors: Derek Carr (@derekwaynecarr), Vishnu Kannan (@vishh)
Status: Proposed (memory evictions WIP)
This document presents a specification for how the kubelet evicts pods when compute resources are too low.
Goals The node needs a mechanism to preserve stability when available compute resources are low.
This is especially important when dealing with incompressible compute resources such as memory or disk. If either resource is exhausted, the node would become unstable.</description>
    </item>
    
    <item>
      <title>kubelet-hypercontainer-runtime</title>
      <link>/contributors/design-proposals/node/kubelet-hypercontainer-runtime/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/kubelet-hypercontainer-runtime/</guid>
      <description>Kubelet HyperContainer Container Runtime Authors: Pengfei Ni (@feiskyer), Harry Zhang (@resouer)
Abstract This proposal aims to support HyperContainer container runtime in Kubelet.
Motivation HyperContainer is a Hypervisor-agnostic Container Engine that allows you to run Docker images using hypervisors (KVM, Xen, etc.). By running containers within separate VM instances, it offers a hardware-enforced isolation, which is required in multi-tenant environments.
Goals  Complete pod/container/image lifecycle management with HyperContainer. Setup network by network plugins.</description>
    </item>
    
    <item>
      <title>kubelet-rkt-runtime</title>
      <link>/contributors/design-proposals/node/kubelet-rkt-runtime/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/kubelet-rkt-runtime/</guid>
      <description>Next generation rkt runtime integration Authors: Euan Kemp (@euank), Yifan Gu (@yifan-gu)
Abstract This proposal describes the design and road path for integrating rkt with kubelet with the new container runtime interface.
Background Currently, the Kubernetes project supports rkt as a container runtime via an implementation under pkg/kubelet/rkt package.
This implementation, for historical reasons, has required implementing a large amount of logic shared by the original Docker implementation.
In order to make additional container runtime integrations easier, more clearly defined, and more consistent, a new Container Runtime Interface (CRI) is being designed.</description>
    </item>
    
    <item>
      <title>kubelet-rootfs-distribution</title>
      <link>/contributors/design-proposals/node/kubelet-rootfs-distribution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/kubelet-rootfs-distribution/</guid>
      <description>Running Kubelet in a Chroot Authors: Vishnu Kannan &amp;lt;vishh@google.com&amp;gt;, Euan Kemp &amp;lt;euan.kemp@coreos.com&amp;gt;, Brandon Philips &amp;lt;brandon.philips@coreos.com&amp;gt;
Introduction The Kubelet is a critical component of Kubernetes that must be run on every node in a cluster.
However, right now it&amp;rsquo;s not always easy to run it correctly. The Kubelet has a number of dependencies that must exist in its filesystem, including various mount and network utilities. Missing any of these can lead to unexpected differences between Kubernetes hosts.</description>
    </item>
    
    <item>
      <title>kubelet-systemd</title>
      <link>/contributors/design-proposals/node/kubelet-systemd/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/kubelet-systemd/</guid>
      <description>Kubelet and systemd interaction Author: Derek Carr (@derekwaynecarr)
Status: Proposed
Motivation Many Linux distributions have either adopted, or plan to adopt systemd as their init system.
This document describes how the node should be configured, and a set of enhancements that should be made to the kubelet to better integrate with these distributions independent of container runtime.
Scope of proposal This proposal does not account for running the kubelet in a container.</description>
    </item>
    
    <item>
      <title>kubelet-tls-bootstrap</title>
      <link>/contributors/design-proposals/cluster-lifecycle/kubelet-tls-bootstrap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/cluster-lifecycle/kubelet-tls-bootstrap/</guid>
      <description>Kubelet TLS bootstrap Author: George Tankersley (george.tankersley@coreos.com)
Preface This document describes a method for a kubelet to bootstrap itself into a TLS-secured cluster. Crucially, it automates the provision and distribution of signed certificates.
Overview When a kubelet runs for the first time, it must be given TLS assets or generate them itself. In the first case, this is a burden on the cluster admin and a significant logistical barrier to secure Kubernetes rollouts.</description>
    </item>
    
    <item>
      <title>kubemark</title>
      <link>/contributors/design-proposals/scalability/kubemark/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/scalability/kubemark/</guid>
      <description>Kubemark proposal Goal of this document This document describes a design of Kubemark - a system that allows performance testing of a Kubernetes cluster. It describes the assumption, high level design and discusses possible solutions for lower-level problems. It is supposed to be a starting point for more detailed discussion.
Current state and objective Currently performance testing happens on ‘live’ clusters of up to 100 Nodes. It takes quite a while to start such cluster or to push updates to all Nodes, and it uses quite a lot of resources.</description>
    </item>
    
    <item>
      <title>kubemark-guide</title>
      <link>/contributors/devel/kubemark-guide/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/kubemark-guide/</guid>
      <description>Kubemark User Guide Introduction Kubemark is a performance testing tool which allows users to run experiments on simulated clusters. The primary use case is scalability testing, as simulated clusters can be much bigger than the real ones. The objective is to expose problems with the master components (API server, controller manager or scheduler) that appear only on bigger clusters (e.g. small memory leaks).
This document serves as a primer to understand what Kubemark is, what it is not, and how to use it.</description>
    </item>
    
    <item>
      <title>kubernetes-client-libraries-open-space</title>
      <link>/events/2017/12-contributor-summit/kubernetes-client-libraries-open-space/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/12-contributor-summit/kubernetes-client-libraries-open-space/</guid>
      <description>Notes from the open space discussion at the Kubernetes Contributor Summit 2017 lead by @brendandburns
Three high-level topics:
 Client libraries and autogeneration OpenAPI description status Fluent libraries (ie. those with native sensibilities) - is this scalable given need for lots of hand-written code?  Early shout-out for the Common LISP client :)
Currently Java, Python, .Net, Javascript clients are all silver. Reference to the badge and capabilities descriptions Python still outside the clients org - should this move from incubator?</description>
    </item>
    
    <item>
      <title>kubernetes-on-aws</title>
      <link>/sig-aws/kubernetes-on-aws/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-aws/kubernetes-on-aws/</guid>
      <description>Kubernetes on Amazon Web Services This page lists different options, in alphabetic order, of starting a Kubernetes cluster on Amazon Web Services.
 Clocker: http://www.clocker.io/tutorials/kubernetes-cluster.html Heptio: https://github.com/aws-quickstart/quickstart-heptio Juju Charms: https://jujucharms.com/canonical-kubernetes/ KAT - Kubernetes cluster on AWS with Terraform: https://github.com/xuwang/kube-aws-terraform Kargo: https://github.com/kubernetes-incubator/kargo Kismatic Enterprise Toolkit: https://github.com/apprenda/kismatic Kraken 2: https://github.com/samsung-cnct/k2 kube-aws: https://github.com/kubernetes-incubator/kube-aws Kubeadm Quickstart: https://github.com/upmc-enterprises/kubeadm-aws Kubernetes on AWS: https://github.com/zalando-incubator/kubernetes-on-aws/ Kubernetes Operations (kops): https://github.com/kubernetes/kops OpenShift: https://access.redhat.com/articles/2623521 Stackpoint.io: https://stackpoint.io Tack: https://github.com/kz8s/tack Tectonic: http://github.com/coreos/tectonic-installer Weaveworks AMI: https://github.</description>
    </item>
    
    <item>
      <title>kubernetes-repositories</title>
      <link>/github-management/kubernetes-repositories/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/github-management/kubernetes-repositories/</guid>
      <description>Kubernetes Repository Guidelines This document attempts to outline a structure for creating and associating GitHub repositories with the Kubernetes project. It also describes how and when repositories are removed.
The document presents a tiered system of repositories with increasingly strict requirements in an attempt to provide the right level of oversight and flexibility for a variety of different projects.
Associated Repositories Associated repositories conform to the Kubernetes community standards for a repository, but otherwise have no restrictions.</description>
    </item>
    
    <item>
      <title>local-cluster-ux</title>
      <link>/contributors/design-proposals/cluster-lifecycle/local-cluster-ux/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/cluster-lifecycle/local-cluster-ux/</guid>
      <description>Kubernetes Local Cluster Experience This proposal attempts to improve the existing local cluster experience for kubernetes. The current local cluster experience is sub-par and often not functional. There are several options to setup a local cluster (docker, vagrant, linux processes, etc) and we do not test any of them continuously. Here are some highlighted issues: - Docker based solution breaks with docker upgrades, does not support DNS, and many kubelet features are not functional yet inside a container.</description>
    </item>
    
    <item>
      <title>local-storage-overview</title>
      <link>/contributors/design-proposals/storage/local-storage-overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/local-storage-overview/</guid>
      <description>Local Storage Management Authors: vishh@, msau42@
This document presents a strawman for managing local storage in Kubernetes. We expect to provide a UX and high level design overview for managing most user workflows. More detailed design and implementation will be added once the community agrees with the high level design presented here.
Goals  Enable ephemeral &amp;amp; durable access to local storage Support storage requirements for all workloads supported by Kubernetes Provide flexibility for users/vendors to utilize various types of storage devices Define a standard partitioning scheme for storage drives for all Kubernetes nodes Provide storage usage isolation for shared partitions Support random access storage devices only (e.</description>
    </item>
    
    <item>
      <title>local-storage-pv</title>
      <link>/contributors/design-proposals/storage/local-storage-pv/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/local-storage-pv/</guid>
      <description>Local Storage Persistent Volumes Authors: @msau42, @vishh, @dhirajh, @ianchakeres
This document presents a detailed design for supporting persistent local storage, as outlined in Local Storage Overview. Supporting all the use cases for persistent local storage will take many releases, so this document will be extended for each new release as we add more features.
Goals  Allow pods to mount any local block or filesystem based volume. Allow pods to mount dedicated local disks, or channeled partitions as volumes for IOPS isolation.</description>
    </item>
    
    <item>
      <title>logging</title>
      <link>/contributors/devel/logging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/logging/</guid>
      <description>Logging Conventions The following conventions for the glog levels to use. glog is globally preferred to log for better runtime control.
 glog.Errorf() - Always an error
 glog.Warningf() - Something unexpected, but probably not an error
 glog.Infof() has multiple levels:
 glog.V(0) - Generally useful for this to ALWAYS be visible to an operator Programmer errors Logging extra info about a panic CLI argument handling glog.V(1) - A reasonable default log level if you don&amp;rsquo;t want verbosity.</description>
    </item>
    
    <item>
      <title>markdown-link-style-guide</title>
      <link>/sig-contributor-experience/markdown-link-style-guide/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-contributor-experience/markdown-link-style-guide/</guid>
      <description>Markdown Link Style Guide Markdown provides for a number of different ways to link between documents. There are advantages and disadvantages of each method.
 If you are linking between documents in the same folder, the easiest way to do it is use a relative link.
See this other document [here](document-2).  If you are linking to a file in the same repo, but not in the same directory, it&amp;rsquo;s usually best to use an absolute link from the root of the repo.</description>
    </item>
    
    <item>
      <title>mattfarina_bio</title>
      <link>/events/elections/2017/mattfarina_bio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/elections/2017/mattfarina_bio/</guid>
      <description>Matt Farina Technical leader, coder, author, presenter
Governance and Organization The steering committee is chartered with figuring out the governance and organization of the Kubernetes community. Matt has years of experience in both non-profit and open source organizations. He has served on the board for two non-profits along with being involved in open source projects with mature governance models (e.g., Drupal and OpenStack).
Matt believes that governance should enable people to easily navigate an organization and be empowered to be part of the community.</description>
    </item>
    
    <item>
      <title>meet-our-contributors</title>
      <link>/mentoring/meet-our-contributors/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/mentoring/meet-our-contributors/</guid>
      <description>Meet Our Contributors - Ask Us Anything! When Slack seems like it’s going too fast, and you just need a quick answer from a human&amp;hellip;
Meet Our Contributors gives you a monthly one-hour opportunity to ask questions about our upstream community, watch interviews with our contributors, and participate in peer code reviews.
When: Every first Wednesday of the month at the following times. Grab a copy of the calendar to yours from kubernetes.</description>
    </item>
    
    <item>
      <title>mentor-guide</title>
      <link>/mentoring/mentor-guide/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/mentoring/mentor-guide/</guid>
      <description>Mentor Guidelines With over 37 special interest and working groups (SIGs/WGs), the Kubernetes ecosystem is vast and no one has all of the answers. Contributors rely on our docs, commenting on PRs/Issues, finding a mentor on their own, and any time they can interact with others to get up to speed. Whether it’s Slack, update meetings, or KubeCons, contributors want to discuss technical specifics, the landscape, or how to be a valuable member.</description>
    </item>
    
    <item>
      <title>mentoring-events</title>
      <link>/mentoring/mentoring-events/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/mentoring/mentoring-events/</guid>
      <description>Mentoring Activities at Events During events like KubeCon and other related events that arise, there will be the opportunity for mentoring activities. This doc will list those and their respective details. The KubeCon events are run by CNCF/LF and not Contributor Experience although we may advise and members could help out.
CNCF is hosting another speed networking and mentoring session during KubeCon + CloudNativeCon Europe 2018.
Speed Networking &amp;amp; Mentoring Event Details:</description>
    </item>
    
    <item>
      <title>mesos-style</title>
      <link>/contributors/devel/mesos-style/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/mesos-style/</guid>
      <description>Building Mesos/Omega-style frameworks on Kubernetes Introduction We have observed two different cluster management architectures, which can be categorized as &amp;ldquo;Borg-style&amp;rdquo; and &amp;ldquo;Mesos/Omega-style.&amp;rdquo; In the remainder of this document, we will abbreviate the latter as &amp;ldquo;Mesos-style.&amp;rdquo; Although out-of-the box Kubernetes uses a Borg-style architecture, it can also be configured in a Mesos-style architecture, and in fact can support both styles at the same time. This document describes the two approaches and describes how to deploy a Mesos-style architecture on Kubernetes.</description>
    </item>
    
    <item>
      <title>metadata-policy</title>
      <link>/contributors/design-proposals/api-machinery/metadata-policy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/metadata-policy/</guid>
      <description>MetadataPolicy and its use in choosing the scheduler in a multi-scheduler system Status: Not Implemented
Introduction This document describes a new API resource, MetadataPolicy, that configures an admission controller to take one or more actions based on an object&amp;rsquo;s metadata. Initially the metadata fields that the predicates can examine are labels and annotations, and the actions are to add one or more labels and/or annotations, or to reject creation/update of the object.</description>
    </item>
    
    <item>
      <title>metrics-server</title>
      <link>/contributors/design-proposals/instrumentation/metrics-server/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/instrumentation/metrics-server/</guid>
      <description>Metrics Server Resource Metrics API is an effort to provide a first-class Kubernetes API (stable, versioned, discoverable, available through apiserver and with client support) that serves resource usage metrics for pods and nodes. The use cases were discussed and the API was proposed a while ago in another proposal. This document describes the architecture and the design of the second part of this effort: making the mentioned API available in the same way as the other Kubernetes APIs.</description>
    </item>
    
    <item>
      <title>michaelrubin_bio</title>
      <link>/events/elections/2017/michaelrubin_bio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/elections/2017/michaelrubin_bio/</guid>
      <description>Michael Rubin Bio GitHub: @matchstick
About Me I have been a software engineer, technical manager, and leader for ~20 years. I am passionate about helping projects and people thrive. Today I lead a number of Kubernetes teams at Google, including Storage, Networking, Multi-cluster, Node, and more.
I love coding and technology, but even more than that I love mentoring others and helping them to have impact and grow professionally. This is why I choose to support teams and projects as a technical manager.</description>
    </item>
    
    <item>
      <title>michellenoorali_bio</title>
      <link>/events/elections/2017/michellenoorali_bio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/elections/2017/michellenoorali_bio/</guid>
      <description>Michelle Noorali GitHub: @michelleN
Twitter: @michellenoorali
LinkedIn: Michelle Noorali
History &amp;amp; Roles Held  Working with Kubernetes and have been part of the community since mid-2015 Co-founded and Co-lead Kubernetes SIG Apps for focusing on defining and managing applications in Kubernetes Core Maintainer of the Kubernetes Helm project Co-chair CloudNativeCon &amp;amp; KubeCon 2017  Why I&amp;rsquo;m here The Kubernetes community has been a special place for me, and I&amp;rsquo;m glad to be part of it.</description>
    </item>
    
    <item>
      <title>moderation</title>
      <link>/communication/moderation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/communication/moderation/</guid>
      <description>Moderation on Kubernetes Communications Channels This page describes the rules and best practices for people chosen to moderate Kubernetes communications channels. This includes, Slack and the mailing lists and any communication tool used in an official manner by the project.
Roles and Responsibilities As part of volunteering to become a moderator you are now representative of the Kubernetes community and it is your responsibility to remain aware of your contributions in this space.</description>
    </item>
    
    <item>
      <title>monitoring_architecture</title>
      <link>/contributors/design-proposals/instrumentation/monitoring_architecture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/instrumentation/monitoring_architecture/</guid>
      <description>Kubernetes monitoring architecture Executive Summary Monitoring is split into two pipelines:
 A core metrics pipeline consisting of Kubelet, a resource estimator, a slimmed-down Heapster called metrics-server, and the API server serving the master metrics API. These metrics are used by core system components, such as scheduling logic (e.g. scheduler and horizontal pod autoscaling based on system metrics) and simple out-of-the-box UI components (e.g. kubectl top). This pipeline is not intended for integration with third-party monitoring systems.</description>
    </item>
    
    <item>
      <title>mount-options</title>
      <link>/contributors/design-proposals/storage/mount-options/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/mount-options/</guid>
      <description>Mount options for mountable volume types Goal Enable Kubernetes admins to specify mount options with mountable volumes such as - nfs, glusterfs or aws-ebs etc.
Motivation We currently support network filesystems: NFS, Glusterfs, Ceph FS, SMB (Azure file), Quobytes, and local filesystems such as ext[3|4] and XFS.
Mount time options that are operationally important and have no security implications should be supported. Examples are NFS&amp;rsquo;s TCP mode, versions, lock mode, caching mode; Glusterfs&amp;rsquo;s caching mode; SMB&amp;rsquo;s version, locking, id mapping; and more.</description>
    </item>
    
    <item>
      <title>multi-fields-merge-key</title>
      <link>/contributors/design-proposals/cli/multi-fields-merge-key/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/cli/multi-fields-merge-key/</guid>
      <description>Multi-fields Merge Key in Strategic Merge Patch Abstract Support multi-fields merge key in Strategic Merge Patch.
Background Strategic Merge Patch is covered in this doc. In Strategic Merge Patch, we use Merge Key to identify the entries in the list of non-primitive types. It must always be present and unique to perform the merge on the list of non-primitive types, and will be preserved.
The merge key exists in the struct tag (e.</description>
    </item>
    
    <item>
      <title>multi-platform</title>
      <link>/contributors/design-proposals/multi-platform/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/multi-platform/</guid>
      <description>Kubernetes for multiple platforms Author: Lucas Käldström (@luxas)
Status (25th of August 2016): Some parts are already implemented; but still there quite a lot of work to be done.
Abstract We obviously want Kubernetes to run on as many platforms as possible, in order to make Kubernetes an even more powerful system. This is a proposal that explains what should be done in order to achieve a true cross-platform container management system.</description>
    </item>
    
    <item>
      <title>multicluster-reserved-namespaces</title>
      <link>/contributors/design-proposals/multicluster/multicluster-reserved-namespaces/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/multicluster/multicluster-reserved-namespaces/</guid>
      <description>Multicluster reserved namespaces @perotinus
06/06/2018
Background sig-multicluster has identified the need for a canonical set of namespaces that can be used for supporting multicluster applications and use cases. Initially, an issue was filed in the cluster-registry repository describing the need for a namespace that would be used for public, global cluster records. This topic was further discussed at the SIG meeting on June 5, 2018 and in a thread on the SIG mailing list.</description>
    </item>
    
    <item>
      <title>multiple-schedulers</title>
      <link>/contributors/design-proposals/scheduling/multiple-schedulers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/scheduling/multiple-schedulers/</guid>
      <description>Multi-Scheduler in Kubernetes Status: Design &amp;amp; Implementation in progress.
 Contact @HaiyangDING for questions &amp;amp; suggestions.
 Motivation In current Kubernetes design, there is only one default scheduler in a Kubernetes cluster. However it is common that multiple types of workload, such as traditional batch, DAG batch, streaming and user-facing production services, are running in the same cluster and they need to be scheduled in different ways. For example, in Omega batch workload and service workload are scheduled by two types of schedulers: the batch workload is scheduled by a scheduler which looks at the current usage of the cluster to improve the resource usage rate and the service workload is scheduled by another one which considers the reserved resources in the cluster and many other constraints since their performance must meet some higher SLOs.</description>
    </item>
    
    <item>
      <title>namespaces</title>
      <link>/contributors/design-proposals/architecture/namespaces/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/architecture/namespaces/</guid>
      <description>Namespaces Abstract A Namespace is a mechanism to partition resources created by users into a logically named group.
Motivation A single cluster should be able to satisfy the needs of multiple user communities.
Each user community wants to be able to work in isolation from other communities.
Each user community has its own:
 resources (pods, services, replication controllers, etc.) policies (who can or cannot perform actions in their community) constraints (this community is allowed this much quota, etc.</description>
    </item>
    
    <item>
      <title>network-policy</title>
      <link>/contributors/design-proposals/network/network-policy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/network/network-policy/</guid>
      <description>NetworkPolicy Abstract A proposal for implementing a new resource - NetworkPolicy - which will enable definition of ingress policies for selections of pods.
The design for this proposal has been created by, and discussed extensively within the Kubernetes networking SIG. It has been implemented and tested using Kubernetes API extensions by various networking solutions already.
In this design, users can create various NetworkPolicy objects which select groups of pods and define how those pods should be allowed to communicate with each other.</description>
    </item>
    
    <item>
      <title>networking</title>
      <link>/contributors/design-proposals/network/networking/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/network/networking/</guid>
      <description>Networking There are 4 distinct networking problems to solve:
 Highly-coupled container-to-container communications Pod-to-Pod communications Pod-to-Service communications External-to-internal communications  Model and motivation Kubernetes deviates from the default Docker networking model (though as of Docker 1.8 their network plugins are getting closer). The goal is for each pod to have an IP in a flat shared networking namespace that has full communication with other physical computers and containers across the network.</description>
    </item>
    
    <item>
      <title>networking-notes</title>
      <link>/events/2018/05-contributor-summit/networking-notes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2018/05-contributor-summit/networking-notes/</guid>
      <description>Networking Lead: thockin
Slides: here
Thanks to our notetakers: onyiny-ang, mrbobbytales, tpepper
This session is not declaring what&amp;rsquo;s being implemented next, but rather laying out the problems that loom.
Coming soon  kube-proxy with IPVS  currently beta  core DNS replacing kube DNS  currently beta  pod &amp;ldquo;ready++&amp;rdquo;  allow external systems to participate in rolling updates. Say your load-balancer takes 5-10 seconds to program, when you bring up new pod and take down old pod the load balancer has lost old backends but hasn&amp;rsquo;t yet added new backends.</description>
    </item>
    
    <item>
      <title>new-contributor-notes</title>
      <link>/events/2018/05-contributor-summit/new-contributor-notes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2018/05-contributor-summit/new-contributor-notes/</guid>
      <description>Kubernetes New Contributor Workshop - KubeCon EU 2018 - Notes Joining in the beginning was onboarding on a yacht Now is more onboarding a BIG cruise ship.
Will be a Hard schedule, and let&amp;rsquo;s hope we can achieve everything Sig-contributor-experience -&amp;gt; from Non-member contributors to Owner
SIG presentation  SIG-docs &amp;amp; SIG-contributor-experience: Docs and website contribution SIG-testing: Testing contribution SIG-* (depends on the area to contribute on): Code contribution  =&amp;gt; Find your first topics: bug, feature, learning, community development and documentation</description>
    </item>
    
    <item>
      <title>new-contributor-workshop</title>
      <link>/events/2018/05-contributor-summit/new-contributor-workshop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2018/05-contributor-summit/new-contributor-workshop/</guid>
      <description>Kubernetes Summit: New Contributor Workshop
This was presented as one continuous 3-hour training with a break. For purposes of live coding exercises, participants were asked to bring a laptop with git installed.
This course was captured on video, and the playlist can be found here.
Course Playlist Part One: * Opening * Welcome contributors * Who this is for * Program * The contributor ladder * CLA signing * Why we have a CLA * Going through the signing process * Choose Your Own Adventure: Figuring out where to contribute * Docs &amp;amp; Website * Testing * Community management * Code * Main code * Drivers, platforms, plugins, subprojects * Finding your first topic * Things that fit into your work at work * Interest match * Skills match * Choose your own adventure exercise * Let&amp;rsquo;s talk: Communication * Importance of communication * Community standards and courtesy * Mailing Lists (esp Kube-dev) * Slack * Github Issues &amp;amp; PRs * Zoom meetings &amp;amp; calendar * Office hours, MoC, other events * Meetups * Communication exercise * The SIG system * What are SIGs and WGs * Finding the right SIG * Most active SIGs * SIG Membership, governance * WGs and Subprojects * Repositories * Tour de Repo * Core Repo * Website/docs * Testing * Other core repos * Satellite Repos * Owners files * Repo membership * BREAK (20min)</description>
    </item>
    
    <item>
      <title>no-new-privs</title>
      <link>/contributors/design-proposals/auth/no-new-privs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/auth/no-new-privs/</guid>
      <description>No New Privileges  Description  Interactions with other Linux primitives  Current Implementations  Support in Docker Support in rkt Support in OCI runtimes  Existing SecurityContext objects Changes of SecurityContext objects Pod Security Policy changes  Description In Linux, the execve system call can grant more privileges to a newly-created process than its parent process. Considering security issues, since Linux kernel v3.5, there is a new flag named no_new_privs added to prevent those new privileges from being granted to the processes.</description>
    </item>
    
    <item>
      <title>no_custom_content</title>
      <link>/generator/testdata/no_custom_content/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/generator/testdata/no_custom_content/</guid>
      <description> Auth SIG Covers improvements to Kubernetes authorization, authentication, and cluster security policy.
Meetings  Wednesdays at 18:00 UTC (biweekly). Convert to your timezone.  Meeting notes and Agenda can be found here.
Leads  Eric Chiang, CoreOS Jordan Liggitt, Red Hat David Eads, Red Hat  Contact  Slack Mailing list  </description>
    </item>
    
    <item>
      <title>node-allocatable</title>
      <link>/contributors/design-proposals/node/node-allocatable/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/node-allocatable/</guid>
      <description>Node Allocatable Resources Authors: timstclair@, vishh@ Overview Kubernetes nodes typically run many OS system daemons in addition to kubernetes daemons like kubelet, runtime, etc. and user pods. Kubernetes assumes that all the compute resources available, referred to as Capacity, in a node are available for user pods. In reality, system daemons use non-trivial amount of resources and their availability is critical for the stability of the system. To address this issue, this proposal introduces the concept of Allocatable which identifies the amount of compute resources available to user pods.</description>
    </item>
    
    <item>
      <title>node-performance-testing</title>
      <link>/contributors/devel/node-performance-testing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/node-performance-testing/</guid>
      <description>Measuring Node Performance This document outlines the issues and pitfalls of measuring Node performance, as well as the tools available.
Cluster Set-up There are lots of factors which can affect node performance numbers, so care must be taken in setting up the cluster to make the intended measurements. In addition to taking the following steps into consideration, it is important to document precisely which setup was used. For example, performance can vary wildly from commit-to-commit, so it is very important to document which commit or version of Kubernetes was used, which Docker version was used, etc.</description>
    </item>
    
    <item>
      <title>node-usernamespace-remapping</title>
      <link>/contributors/design-proposals/node/node-usernamespace-remapping/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/node-usernamespace-remapping/</guid>
      <description>Support Node-Level User Namespaces Remapping  Summary Motivation Goals Non-Goals Use Stories Proposal Future Work Risks and Mitigations Graduation Criteria Alternatives  Authors:
 Mrunal Patel &amp;lt;mpatel@redhat.com&amp;gt; Jan Pazdziora &amp;lt;jpazdziora@redhat.com&amp;gt; Vikas Choudhary &amp;lt;vichoudh@redhat.com&amp;gt;  Summary Container security consists of many different kernel features that work together to make containers secure. User namespaces is one such feature that enables interesting possibilities for containers by allowing them to be root inside the container while not being root on the host.</description>
    </item>
    
    <item>
      <title>nodeaffinity</title>
      <link>/contributors/design-proposals/scheduling/nodeaffinity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/scheduling/nodeaffinity/</guid>
      <description>Node affinity and NodeSelector Introduction This document proposes a new label selector representation, called NodeSelector, that is similar in many ways to LabelSelector, but is a bit more flexible and is intended to be used only for selecting nodes.
In addition, we propose to replace the map[string]string in PodSpec that the scheduler currently uses as part of restricting the set of nodes onto which a pod is eligible to schedule, with a field of type Affinity that contains one or more affinity specifications.</description>
    </item>
    
    <item>
      <title>nodeport-ip-range</title>
      <link>/contributors/design-proposals/network/nodeport-ip-range/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/network/nodeport-ip-range/</guid>
      <description>Support specifying NodePort IP range Author: @m1093782566
Objective This document proposes creating a option for kube-proxy to specify NodePort IP range.
Background NodePort type service gives developers the freedom to set up their own load balancers, to expose one or more nodes’ IPs directly. The service will be visible as the nodes&amp;rsquo;s IPs. For now, the NodePort addresses are the IPs from all available interfaces.
With iptables magic, all the IPs whose ADDRTYPE matches dst-type LOCAL will be taken as the address of NodePort, which might look like,</description>
    </item>
    
    <item>
      <title>office-hours</title>
      <link>/events/office-hours/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/office-hours/</guid>
      <description>Kubernetes Community Office Hours Office Hours is a live stream where we answer live questions about Kubernetes from users on the YouTube channel. Office hours are a regularly scheduled meeting where people can bring topics to discuss with the greater community. They are great for answering questions, getting feedback on how you’re using Kubernetes, or to just passively learn by following along.
When and Where Third Wednesday of every month, there are two sessions:</description>
    </item>
    
    <item>
      <title>on-call-federation-build-cop</title>
      <link>/contributors/devel/on-call-federation-build-cop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/on-call-federation-build-cop/</guid>
      <description>Federation Buildcop Guide and Playbook Federation runs two classes of tests: CI and Pre-submits.
CI  These tests run on the HEADs of master and release branches (starting from Kubernetes v1.7). As a result, they run on code that&amp;rsquo;s already merged. As the name suggests, they run continuously. Currently, they are configured to run at least once every 30 minutes. Federation CI tests run as periodic jobs on prow. CI jobs always run sequentially.</description>
    </item>
    
    <item>
      <title>onboarding-new-developers-through-better-docs</title>
      <link>/events/2017/12-contributor-summit/onboarding-new-developers-through-better-docs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/12-contributor-summit/onboarding-new-developers-through-better-docs/</guid>
      <description>Onboarding Developers through Better Documentation
@ K8s Contributor Summit, 12&amp;frasl;5
Note Taker: Jared Bhatti (jaredbhatti), Andrew Chen (chenopis), Solly Ross (directxman12)
[[TOC]]
Goal  Understand how we’re currently onboarding developers now
 Understand the &amp;ldquo;rough edges&amp;rdquo; of the onboarding experience for new users
 Understand how we can better target the needs of our audience
 Understand if our contributor process will meet the documentation needs.
  Note: the focus is on documentation, so we won’t be able to act on suggestions outside of that (but we will consider them!</description>
    </item>
    
    <item>
      <title>optional-configmap</title>
      <link>/contributors/design-proposals/node/optional-configmap/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/optional-configmap/</guid>
      <description>Optional ConfigMaps and Secrets Goal Allow the ConfigMaps or Secrets that are used to populate the environment variables of a container and files within a Volume to be optional.
Use Cases When deploying an application to multiple environments like development, test, and production, there may be certain environment variables that must reflect the values that are relevant to said environment. One way to do so would be to have a well named ConfigMap which contains all the environment variables needed.</description>
    </item>
    
    <item>
      <title>org-owners-guide</title>
      <link>/github-management/org-owners-guide/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/github-management/org-owners-guide/</guid>
      <description>Kubernetes GitHub Organization Guide The Kubernetes project leverages multiple GitHub organizations to store and organize code. This guide contains the details on how to run those organizations for CNCF compliance and for the guidelines of the community.
Organization Naming Kubernetes managed organizations should be in the form of kubernetes-[thing]. For example, kubernetes-client where the API clients are housed.
Prior to creating an organization please contact the steering committee for direction and approval.</description>
    </item>
    
    <item>
      <title>outreachy</title>
      <link>/sig-cli/outreachy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-cli/outreachy/</guid>
      <description>Outreachy Kubernetes, specifically the SIG-CLI, is happy to announce our participation in the Outreachy program, running from December 2017 to March 2018. Please see the main program page for general information about the program, such as its purpose, timeline, eligibility requirements, and how to apply.
Schedule
 October 23: application deadline for other Outreachy communities October 30: application deadline for Kubernetes Outreachy applications November 9: selection decisions are made December 5 - March 5: internship  What is Kubernetes?</description>
    </item>
    
    <item>
      <title>owners</title>
      <link>/contributors/guide/owners/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/guide/owners/</guid>
      <description>OWNERS files Overview OWNERS files are used to designate responsibility over different parts of the Kubernetes codebase. Today, we use them to assign the reviewer and approver roles used in our two-phase code review process. Our OWNERS files were inspired by Chromium OWNERS files, which in turn inspired GitHub&amp;rsquo;s CODEOWNERS files.
The velocity of a project that uses code review is limited by the number of people capable of reviewing code.</description>
    </item>
    
    <item>
      <title>performance-related-monitoring</title>
      <link>/contributors/design-proposals/instrumentation/performance-related-monitoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/instrumentation/performance-related-monitoring/</guid>
      <description>Performance Monitoring Reason for this document This document serves as a place to gather information about past performance regressions, their reason and impact and discuss ideas to avoid similar regressions in the future. Main reason behind doing this is to understand what kind of monitoring needs to be in place to keep Kubernetes fast.
Known past and present performance issues Higher logging level causing scheduler stair stepping Issue https://github.com/kubernetes/kubernetes/issues/14216 was opened because @spiffxp observed a regression in scheduler performance in 1.</description>
    </item>
    
    <item>
      <title>permissions</title>
      <link>/github-management/permissions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/github-management/permissions/</guid>
      <description>GitHub Permissions GitHub provides a limited permissions model for organizations and repositories. It lacks granularity, and for the most part is &amp;ldquo;all or nothing&amp;rdquo;. This doesn&amp;rsquo;t scale well with the size and velocity of the Kubernetes project.
We have created a number of automated systems/bots to allow us to work around these limitations. Authorized users can issue bot commands to execute actions against PRs and issues without having direct GitHub access to run these actions.</description>
    </item>
    
    <item>
      <title>persistent-storage</title>
      <link>/contributors/design-proposals/storage/persistent-storage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/persistent-storage/</guid>
      <description>Persistent Storage This document proposes a model for managing persistent, cluster-scoped storage for applications requiring long lived data.
Abstract Two new API kinds:
A PersistentVolume (PV) is a storage resource provisioned by an administrator. It is analogous to a node. See Persistent Volume Guide for how to use it.
A PersistentVolumeClaim (PVC) is a user&amp;rsquo;s request for a persistent volume to use in a pod. It is analogous to a pod.</description>
    </item>
    
    <item>
      <title>pod-lifecycle-event-generator</title>
      <link>/contributors/design-proposals/node/pod-lifecycle-event-generator/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/pod-lifecycle-event-generator/</guid>
      <description>Kubelet: Pod Lifecycle Event Generator (PLEG) In Kubernetes, Kubelet is a per-node daemon that manages the pods on the node, driving the pod states to match their pod specifications (specs). To achieve this, Kubelet needs to react to changes in both (1) pod specs and (2) the container states. For the former, Kubelet watches the pod specs changes from multiple sources; for the latter, Kubelet polls the container runtime periodically (e.</description>
    </item>
    
    <item>
      <title>pod-pid-namespace</title>
      <link>/contributors/design-proposals/node/pod-pid-namespace/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/pod-pid-namespace/</guid>
      <description>Shared PID Namespace  Status: Pending Version: Alpha Implementation Owner: @verb  Motivation Pods share namespaces where possible, but support for sharing the PID namespace had not been defined due to lack of support in Docker. This created an implicit API on which certain container images now rely. This document proposes adding support for sharing a process namespace between containers in a pod while maintaining backwards compatibility with the existing implicit API.</description>
    </item>
    
    <item>
      <title>pod-preemption</title>
      <link>/contributors/design-proposals/scheduling/pod-preemption/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/scheduling/pod-preemption/</guid>
      <description>Pod Preemption in Kubernetes Status: Draft
Author: @bsalamat
 Pod Preemption in Kubernetes Objectives  Non-Goals  Background  Terminology  Overview Detailed Design  Preemption scenario Scheduler performs preemption Preemption order Preemption - Eviction workflow Race condition in multi-scheduler clusters Starvation Problem Supporting PodDisruptionBudget Supporting Inter-Pod Affinity on Lower Priority Pods Supporting Cross Node Preemption  Interactions with Cluster Autoscaler Alternatives Considered  Rescheduler or Kubelet performs preemption Preemption order  References  Objectives  Define the concept of preemption in Kubernetes.</description>
    </item>
    
    <item>
      <title>pod-preset</title>
      <link>/contributors/design-proposals/service-catalog/pod-preset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/service-catalog/pod-preset/</guid>
      <description>Pod Preset  Abstract Motivation Constraints and Assumptions Use Cases  Summary Prior Art Objectives  Proposed Changes  PodPreset API object  Validations  AdmissionControl Plug-in: PodPreset  Behavior PodPreset Exclude Annotation   Examples  Simple Pod Spec Example Pod Spec with ConfigMap Example ReplicaSet with Pod Spec Example Multiple PodPreset Example Conflict Example   Abstract Describes a policy resource that allows for the loose coupling of a Pod&amp;rsquo;s definition from additional runtime requirements for that Pod.</description>
    </item>
    
    <item>
      <title>pod-priority-api</title>
      <link>/contributors/design-proposals/scheduling/pod-priority-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/scheduling/pod-priority-api/</guid>
      <description>Priority in Kubernetes API @bsalamat
May 2017 * Objective * Non-Goals * Background * Overview * Detailed Design * Effect of priority on scheduling * Effect of priority on preemption * Priority in PodSpec * Priority Classes * Resolving priority class names * Ordering of priorities * System Priority Class Names * Modifying Priority Classes * Drawbacks of changing priority names * Priority and QoS classes
Objective  How to specify priority for workloads in Kubernetes API.</description>
    </item>
    
    <item>
      <title>pod-priority-resourcequota</title>
      <link>/contributors/design-proposals/scheduling/pod-priority-resourcequota/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/scheduling/pod-priority-resourcequota/</guid>
      <description>Priority in ResourceQuota Authors:
Harry Zhang @resouer Vikas Choudhary @vikaschoudhary16
Main Reviewers:
Bobby @bsalamat Derek @derekwaynecarr
Dec 2017
 Objective  Non-Goals  Background Overview Detailed Design  Changes in ResourceQuota Changes in Admission Controller configuration Expected behavior of ResourceQuota admission controller and Quota system  Backward Compatibility   Sample user story 1 Sample user story 2  Objective This feature is designed to make ResourceQuota become priority aware, several sub-tasks are included.</description>
    </item>
    
    <item>
      <title>pod-resolv-conf</title>
      <link>/contributors/design-proposals/network/pod-resolv-conf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/network/pod-resolv-conf/</guid>
      <description>Custom /etc/resolv.conf  Status: pending Version: alpha Implementation owner: Bowei Du &amp;lt;bowei@google.com&amp;gt;, Zihong Zheng &amp;lt;zihongz@google.com&amp;gt;  Overview The /etc/resolv.conf in a pod is managed by Kubelet and its contents are generated based on pod.dnsPolicy. For dnsPolicy: Default, the search and nameserver fields are taken from the resolve.conf on the node where the pod is running. If the dnsPolicy is ClusterFirst, the search contents of the resolv.conf is the hosts resolv.</description>
    </item>
    
    <item>
      <title>pod-resource-management</title>
      <link>/contributors/design-proposals/node/pod-resource-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/pod-resource-management/</guid>
      <description>Kubelet pod level resource management Authors:
 Buddha Prakash (@dubstack) Vishnu Kannan (@vishh) Derek Carr (@derekwaynecarr)  Last Updated: 02/21/2017
Status: Implementation planned for Kubernetes 1.6
This document proposes a design for introducing pod level resource accounting to Kubernetes. It outlines the implementation and associated rollout plan.
Introduction Kubernetes supports container level isolation by allowing users to specify compute resource requirements via requests and limits on individual containers. The kubelet delegates creation of a cgroup sandbox for each container to its associated container runtime.</description>
    </item>
    
    <item>
      <title>pod-safety</title>
      <link>/contributors/design-proposals/storage/pod-safety/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/pod-safety/</guid>
      <description>Pod Safety, Consistency Guarantees, and Storage Implications @smarterclayton @bprashanth
October 2016
Proposal and Motivation A pod represents the finite execution of one or more related processes on the cluster. In order to ensure higher level consistent controllers can safely build on top of pods, the exact guarantees around its lifecycle on the cluster must be clarified, and it must be possible for higher order controllers and application authors to correctly reason about the lifetime of those processes and their access to cluster resources in a distributed computing environment.</description>
    </item>
    
    <item>
      <title>pod-security-context</title>
      <link>/contributors/design-proposals/auth/pod-security-context/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/auth/pod-security-context/</guid>
      <description>Abstract A proposal for refactoring SecurityContext to have pod-level and container-level attributes in order to correctly model pod- and container-level security concerns.
Motivation Currently, containers have a SecurityContext attribute which contains information about the security settings the container uses. In practice, many of these attributes are uniform across all containers in a pod. Simultaneously, there is also a need to apply the security context pattern at the pod level to correctly model security attributes that apply only at a pod level.</description>
    </item>
    
    <item>
      <title>pod-security-policy</title>
      <link>/contributors/design-proposals/auth/pod-security-policy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/auth/pod-security-policy/</guid>
      <description>Abstract PodSecurityPolicy allows cluster administrators to control the creation and validation of a security context for a pod and containers. The intent of PodSecurityPolicy is to protect the cluster from the pod and containers, not to protect a pod or containers from a user.
Motivation Administration of a multi-tenant cluster requires the ability to provide varying sets of permissions among the tenants, the infrastructure components, and end users of the system who may themselves be administrators within their own isolated namespace.</description>
    </item>
    
    <item>
      <title>pod_startup_latency</title>
      <link>/sig-scalability/slos/pod_startup_latency/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-scalability/slos/pod_startup_latency/</guid>
      <description>Pod startup latency SLI/SLO details User stories  As a user of vanilla Kubernetes, I want some guarantee how quickly my pods will be started.  Other notes  Only schedulable and stateless pods contribute to the SLI:  If there is no space in the cluster to place the pod, there is not much we can do about it (it is task for Cluster Autoscaler which should have separate SLIs/SLOs).</description>
    </item>
    
    <item>
      <title>podaffinity</title>
      <link>/contributors/design-proposals/scheduling/podaffinity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/scheduling/podaffinity/</guid>
      <description>Inter-pod topological affinity and anti-affinity Introduction NOTE: It is useful to read about node affinity first.
This document describes a proposal for specifying and implementing inter-pod topological affinity and anti-affinity. By that we mean: rules that specify that certain pods should be placed in the same topological domain (e.g. same node, same rack, same zone, same power domain, etc.) as some other pods, or, conversely, should not be placed in the same topological domain as some other pods.</description>
    </item>
    
    <item>
      <title>postpone-pv-deletion</title>
      <link>/contributors/design-proposals/storage/postpone-pv-deletion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/postpone-pv-deletion/</guid>
      <description>Postpone deletion of a Persistent Volume if it is bound by a PVC Status: Pending
Version: Beta
Implementation Owner: NickrenREN@
Motivation Admin can delete a Persistent Volume (PV) that is being used by a PVC. It may result in data loss.
Proposal Postpone the PV deletion until the PV is not used by any PVC.
User Experience Use Cases  Admin deletes a PV that is being used by a PVC and a pod referring that PVC is not aware of this.</description>
    </item>
    
    <item>
      <title>postpone-pvc-deletion-if-used-in-a-pod</title>
      <link>/contributors/design-proposals/storage/postpone-pvc-deletion-if-used-in-a-pod/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/postpone-pvc-deletion-if-used-in-a-pod/</guid>
      <description>Postpone Deletion of a Persistent Volume Claim in case It Is Used by a Pod Status: Proposal
Version: GA
Implementation Owner: @pospispa
Motivation User can delete a Persistent Volume Claim (PVC) that is being used by a pod. This may have negative impact on the pod and it may result in data loss.
For more details see issue https://github.com/kubernetes/kubernetes/issues/45143
Proposal Postpone the PVC deletion until the PVC is not used by any pod.</description>
    </item>
    
    <item>
      <title>predicates-ordering</title>
      <link>/contributors/design-proposals/scheduling/predicates-ordering/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/scheduling/predicates-ordering/</guid>
      <description>predicates ordering Status: proposal
Author: yastij Approvers: * gmarek * bsalamat * k82cn
Abstract This document describes how and why reordering predicates helps to achieve performance for the kubernetes scheduler. We will expose the motivations behind this proposal, The two steps/solution we see to tackle this problem and the timeline decided to implement these.
Motivation While working on a Pull request related to a proposal, we saw that the order of running predicates isn’t defined.</description>
    </item>
    
    <item>
      <title>preserve-order-in-strategic-merge-patch</title>
      <link>/contributors/design-proposals/cli/preserve-order-in-strategic-merge-patch/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/cli/preserve-order-in-strategic-merge-patch/</guid>
      <description>Preserve Order in Strategic Merge Patch Author: @mengqiy
Motivation Background of the Strategic Merge Patch is covered here.
The Kubernetes API may apply semantic meaning to the ordering of items within a list, however the strategic merge patch does not keeping the ordering of elements. Ordering has semantic meaning for Environment variables, as later environment variables may reference earlier environment variables, but not the other way around.
One use case is the environment variables.</description>
    </item>
    
    <item>
      <title>principles</title>
      <link>/contributors/design-proposals/architecture/principles/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/architecture/principles/</guid>
      <description>Design Principles Principles to follow when extending Kubernetes.
API See also the API conventions.
 All APIs should be declarative. API objects should be complementary and composable, not opaque wrappers. The control plane should be transparent &amp;ndash; there are no hidden internal APIs. The cost of API operations should be proportional to the number of objects intentionally operated upon. Therefore, common filtered lookups must be indexed. Beware of patterns of multiple API calls that would incur quadratic behavior.</description>
    </item>
    
    <item>
      <title>proc-mount-type</title>
      <link>/contributors/design-proposals/auth/proc-mount-type/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/auth/proc-mount-type/</guid>
      <description>ProcMount/ProcMountType Option Background Currently the way docker and most other container runtimes work is by masking and setting as read-only certain paths in /proc. This is to prevent data from being exposed into a container that should not be. However, there are certain use-cases where it is necessary to turn this off.
Motivation For end-users who would like to run unprivileged containers using user namespaces nested inside CRI containers, we need an option to have a ProcMount.</description>
    </item>
    
    <item>
      <title>profiling</title>
      <link>/contributors/devel/profiling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/profiling/</guid>
      <description>Profiling Kubernetes This document explain how to plug in profiler and how to profile Kubernetes services. To get familiar with the tools mentioned below, it is strongly recommended to read Profiling Go Programs.
Profiling library Go comes with inbuilt &amp;lsquo;net/http/pprof&amp;rsquo; profiling library and profiling web service. The way service works is binding debug/pprof/ subtree on a running webserver to the profiler. Reading from subpages of debug/pprof returns pprof-formatted profiles of the running binary.</description>
    </item>
    
    <item>
      <title>project-design-and-plan</title>
      <link>/contributors/design-proposals/multicluster/cluster-registry/project-design-and-plan/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/multicluster/cluster-registry/project-design-and-plan/</guid>
      <description>Cluster registry design and plan @perotinus
Updated: 11/2/17
REVIEWED in SIG-multicluster meeting on 10&amp;frasl;24
This doc is a Markdown conversion of the original Cluster registry design and plan Google doc. That doc is deprecated, and this one is canonical; however, the old doc will be preserved so as not to lose comment and revision history that it contains.
Table of Contents  Background Goal Technical requirements  Alpha Beta Later  Implementation design  Alternatives Using a CRD  Tooling design  User tooling  Repository process Release strategy  Version skew  Test strategy Milestones and timelines  Alpha (targeting late Q4 &amp;lsquo;17) Beta (targeting mid Q1 &amp;lsquo;18) Stable (targeting mid Q2 &amp;lsquo;18) Later   Background SIG-multicluster has identified a cluster registry as being a key enabling component for multi-cluster use cases.</description>
    </item>
    
    <item>
      <title>projects</title>
      <link>/sig-contributor-experience/projects/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-contributor-experience/projects/</guid>
      <description>Projects and Goals note - this is a temporary file until we can figure out a better project management solution.
This is a list of the projects and goals currently underway with Contributor Experience. Please submit a PR if you are adding your project to this list. To introduce a new project, attend a weekly meeting or drop a note to us on the mailing list - details can be found on our [README]().</description>
    </item>
    
    <item>
      <title>propagation</title>
      <link>/contributors/design-proposals/node/propagation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/propagation/</guid>
      <description>HostPath Volume Propagation Abstract A proposal to add support for propagation mode in HostPath volume, which allows mounts within containers to visible outside the container and mounts after pods creation visible to containers. Propagation modes contains &amp;ldquo;shared&amp;rdquo;, &amp;ldquo;slave&amp;rdquo;, &amp;ldquo;private&amp;rdquo;, &amp;ldquo;unbindable&amp;rdquo;. Out of them, docker supports &amp;ldquo;shared&amp;rdquo; / &amp;ldquo;slave&amp;rdquo; / &amp;ldquo;private&amp;rdquo;.
Several existing issues and PRs were already created regarding that particular subject: * Capability to specify mount propagation mode of per volume with docker #20698 * Set propagation to &amp;ldquo;shared&amp;rdquo; for hostPath volume #31504</description>
    </item>
    
    <item>
      <title>protobuf</title>
      <link>/contributors/design-proposals/api-machinery/protobuf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/protobuf/</guid>
      <description>Protobuf serialization and internal storage @smarterclayton
March 2016
Proposal and Motivation The Kubernetes API server is a &amp;ldquo;dumb server&amp;rdquo; which offers storage, versioning, validation, update, and watch semantics on API resources. In a large cluster the API server must efficiently retrieve, store, and deliver large numbers of coarse-grained objects to many clients. In addition, Kubernetes traffic is heavily biased towards intra-cluster traffic - as much as 90% of the requests served by the APIs are for internal cluster components like nodes, controllers, and proxies.</description>
    </item>
    
    <item>
      <title>provider-configs</title>
      <link>/sig-scalability/configs-and-limits/provider-configs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-scalability/configs-and-limits/provider-configs/</guid>
      <description>Scalability Testing/Analysis Environment and Goals Project practice is to perform baseline scalability testing and analysis on a large single machine (VM or server) with all control plane processing on that single node. The single large machine provides sufficient scalability to scale to 5000 node density tests. The typical machine for testing at this scale is at the larger end of the VM scale available on public cloud providers, but is by no means the largest available.</description>
    </item>
    
    <item>
      <title>pull-requests</title>
      <link>/contributors/guide/pull-requests/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/guide/pull-requests/</guid>
      <description>Pull Request Process This doc explains the process and best practices for submitting a pull request to the Kubernetes project and its associated subrepositories. It should serve as a reference for all contributors, and be useful especially to new and infrequent submitters.
 Before You Submit a Pull Request  Run Local Verifications  The Pull Request Submit Process  The Testing and Merge Workflow Marking Unfinished Pull Requests Comment Commands Reference Automation How the e2e Tests Work  Why was my Pull Request closed?</description>
    </item>
    
    <item>
      <title>pv-to-rbd-mapping</title>
      <link>/contributors/design-proposals/storage/pv-to-rbd-mapping/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/pv-to-rbd-mapping/</guid>
      <description>RBD Volume to PV Mapping Authors: krmayankk@
Problem The RBD Dynamic Provisioner currently generates rbd volume names which are random. The current implementation generates a UUID and the rbd image name becomes image := fmt.Sprintf(&amp;ldquo;kubernetes-dynamic-pvc-%s&amp;rdquo;, uuid.NewUUID()). This RBD image name is stored in the PV. The PV also has a reference to the PVC to which it binds. The problem with this approach is that if there is a catastrophic etcd data loss and all PV&amp;rsquo;s are gone, there is no way to recover the mapping from RBD to PVC.</description>
    </item>
    
    <item>
      <title>pwittrock_bio</title>
      <link>/events/elections/2017/pwittrock_bio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/elections/2017/pwittrock_bio/</guid>
      <description>Phillip Wittrock Bio GitHub: @pwittrock
Problem solving style  Incremental approach to problem solving Try new things - build and iterate on minimalist solutions  What I find important I care deeply about helping people feel positive about what they are doing and empowered to do more. Within the Kubernetes community, I would like to focus on helping ensure contributors&amp;rsquo;:
 time and contributions are impactful time and contributions are recognized and appreciated opinions are listened to and valued  Where I see myself contributing I believe it is important for the processes for contribution and maintenance of the project to be well communicated and understood within the community.</description>
    </item>
    
    <item>
      <title>quintonhoole_bio</title>
      <link>/events/elections/2017/quintonhoole_bio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/elections/2017/quintonhoole_bio/</guid>
      <description>Quinton Hoole  Github: quinton-hoole  About Me Currently I&amp;rsquo;m Technical Vice President of Cloud Computing At Huawei Technologies (185,000 employees globally, US$75BN annual revenue (2016), 32% annual revenue growth).
I was the founding engineer of Amazon EC2 (2005-2010).
I was the lead engineer at Nimbula.com (2010-2012), a Cloud IaaS startup which was acquired by Oracle in 2013.
I was at Google from 2012-2016, first as Technical Lead and Manager of Ads Serving SRE, and then Engineering Lead on the Kubernetes team (2014-2016).</description>
    </item>
    
    <item>
      <title>raw-block-pv</title>
      <link>/contributors/design-proposals/storage/raw-block-pv/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/raw-block-pv/</guid>
      <description>Raw Block Consumption in Kubernetes Authors: erinboyd@, screeley44@, mtanino@
This document presents a proposal for managing raw block storage in Kubernetes using the persistent volume source API as a consistent model of consumption.
Terminology  Raw Block Device - a physically attached device devoid of a filesystem Raw Block Volume - a logical abstraction of the raw block device as defined by a path Filesystem on Block - a formatted (ie xfs) filesystem on top of a raw block device  Goals  Enable durable access to block storage Provide flexibility for users/vendors to utilize various types of storage devices Agree on API changes for block Provide a consistent security model for block devices Provide a means for running containerized block storage offerings as non-privileged container  Non Goals  Support all storage devices natively in upstream Kubernetes.</description>
    </item>
    
    <item>
      <title>release-notes</title>
      <link>/contributors/design-proposals/release/release-notes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/release/release-notes/</guid>
      <description>Kubernetes Release Notes djmm@google.com Last Updated: 2016-04-06
 Kubernetes Release Notes  Objective Background The Problem The (general) Solution  Then why not just list every change that was submitted, CHANGELOG-style?  Options Collection Design Publishing Design Location Layout  Alpha/Beta/Patch Releases Major/Minor Releases  Work estimates Caveats / Considerations   Objective Define a process and design tooling for collecting, arranging and publishing release notes for Kubernetes releases, automating as much of the process as possible.</description>
    </item>
    
    <item>
      <title>release-notes</title>
      <link>/contributors/guide/release-notes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/guide/release-notes/</guid>
      <description>Adding Release Notes On the kubernetes/kubernetes repository, release notes are required for any pull request with user-visible changes, such as bug-fixes, feature additions, and output format changes.
To meet this requirement, do one of the following: - Add notes in the release notes block, or - Update the release note label
If you don&amp;rsquo;t add release notes in the pull request template, the do-not-merge/release-note-label-needed label is added to your pull request automatically after you create it.</description>
    </item>
    
    <item>
      <title>release-test-signal</title>
      <link>/contributors/design-proposals/release/release-test-signal/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/release/release-test-signal/</guid>
      <description>Overview Describes the process and tooling (find_green_build) used to find a binary signal from the Kubernetes testing framework for the purposes of selecting a release candidate. Currently this process is used to gate all Kubernetes releases.
Motivation Previously, the guidance in the (now deprecated) release document was to &amp;ldquo;look for green tests&amp;rdquo;. That is, of course, decidedly insufficient.
Software releases should have the goal of being primarily automated and having a gating binary test signal is a key component to that ultimate goal.</description>
    </item>
    
    <item>
      <title>rescheduler</title>
      <link>/contributors/design-proposals/scheduling/rescheduler/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/scheduling/rescheduler/</guid>
      <description>Rescheduler design space @davidopp, @erictune, @briangrant
July 2015
Introduction and definition A rescheduler is an agent that proactively causes currently-running Pods to be moved, so as to optimize some objective function for goodness of the layout of Pods in the cluster. (The objective function doesn&amp;rsquo;t have to be expressed mathematically; it may just be a collection of ad-hoc rules, but in principle there is an objective function. Implicitly an objective function is described by the scheduler&amp;rsquo;s predicate and priority functions.</description>
    </item>
    
    <item>
      <title>rescheduling</title>
      <link>/contributors/design-proposals/scheduling/rescheduling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/scheduling/rescheduling/</guid>
      <description>Controlled Rescheduling in Kubernetes Overview Although the Kubernetes scheduler(s) try to make good placement decisions for pods, conditions in the cluster change over time (e.g. jobs finish and new pods arrive, nodes are removed due to failures or planned maintenance or auto-scaling down, nodes appear due to recovery after a failure or re-joining after maintenance or auto-scaling up or adding new hardware to a bare-metal cluster), and schedulers are not omniscient (e.</description>
    </item>
    
    <item>
      <title>rescheduling-for-critical-pods</title>
      <link>/contributors/design-proposals/scheduling/rescheduling-for-critical-pods/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/scheduling/rescheduling-for-critical-pods/</guid>
      <description>Rescheduler: guaranteed scheduling of critical addons Motivation In addition to Kubernetes core components like api-server, scheduler, controller-manager running on a master machine there is a bunch of addons which due to various reasons have to run on a regular cluster node, not the master. Some of them are critical to have fully functional cluster: Heapster, DNS, UI. Users can break their cluster by evicting a critical addon (either manually or as a side effect of another operation like upgrade) which possibly can become pending (for example when the cluster is highly utilized).</description>
    </item>
    
    <item>
      <title>resource-management</title>
      <link>/contributors/design-proposals/architecture/resource-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/architecture/resource-management/</guid>
      <description>Kubernetes Resource Management  This article was authored by Brian Grant (bgrant0607) on 2/20/2018. The original Google Doc can be found here.
 Kubernetes is not just API-driven, but is API-centric.
At the center of the Kubernetes control plane is the apiserver, which implements common functionality for all of the system’s APIs. Both user clients and components implementing the business logic of Kubernetes, called controllers, interact with the same APIs.</description>
    </item>
    
    <item>
      <title>resource-metrics-api</title>
      <link>/contributors/design-proposals/instrumentation/resource-metrics-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/instrumentation/resource-metrics-api/</guid>
      <description>Resource Metrics API This document describes API part of MVP version of Resource Metrics API effort in Kubernetes. Once the agreement will be made the document will be extended to also cover implementation details. The shape of the effort may be also a subject of changes once we will have more well-defined use cases.
Goal The goal for the effort is to provide resource usage metrics for pods and nodes through the API server.</description>
    </item>
    
    <item>
      <title>resource-qos</title>
      <link>/contributors/design-proposals/node/resource-qos/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/resource-qos/</guid>
      <description>Resource Quality of Service in Kubernetes Author(s): Vishnu Kannan (vishh@), Ananya Kumar (@AnanyaKumar) Last Updated: 5/17/2016
Status: Implemented
This document presents the design of resource quality of service for containers in Kubernetes, and describes use cases and implementation details.
Introduction This document describes the way Kubernetes provides different levels of Quality of Service to pods depending on what they request. Pods that need to stay up reliably can request guaranteed resources, while pods with less stringent requirements can use resources with weaker or no guarantee.</description>
    </item>
    
    <item>
      <title>resource-quota-scoping</title>
      <link>/contributors/design-proposals/resource-management/resource-quota-scoping/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/resource-management/resource-quota-scoping/</guid>
      <description>Resource Quota - Scoping resources Problem Description Ability to limit compute requests and limits The existing ResourceQuota API object constrains the total amount of compute resource requests. This is useful when a cluster-admin is interested in controlling explicit resource guarantees such that there would be a relatively strong guarantee that pods created by users who stay within their quota will find enough free resources in the cluster to be able to schedule.</description>
    </item>
    
    <item>
      <title>resources</title>
      <link>/contributors/design-proposals/scheduling/resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/scheduling/resources/</guid>
      <description>Note: this is a design doc, which describes features that have not been completely implemented. User documentation of the current state is here. The tracking issue for implementation of this model is #168. Currently, both limits and requests of memory and cpu on containers (not pods) are supported. &amp;ldquo;memory&amp;rdquo; is in bytes and &amp;ldquo;cpu&amp;rdquo; is in milli-cores.
The Kubernetes resource model To do good pod placement, Kubernetes needs to know how big pods are, as well as the sizes of the nodes onto which they are being placed.</description>
    </item>
    
    <item>
      <title>resources</title>
      <link>/sig-big-data/resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-big-data/resources/</guid>
      <description> Resources Spark  Spark on Kubernetes Design Proposal Spark Dynamic Allocation Proposal SPARK-JIRA Kubernetes Issue #34377 External Repository  HDFS  Data Locality Doc External Repository  Airflow  Airflow roadmap  </description>
    </item>
    
    <item>
      <title>rhirschfeld_bio</title>
      <link>/events/elections/2017/rhirschfeld_bio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/elections/2017/rhirschfeld_bio/</guid>
      <description>Rob Hirschfeld Bio  GitHub: @zehicle Twitter: @zehicle Blog: RobHirschfeld.com I work at RackN.com and live in Austin, Texas.  Governance Perspective: it&amp;rsquo;s about people, not tech Open source infrastructure automation is a critical foundation for the Internet and, thus, advancing society as a whole. In a very practical way, protecting open software is essential to building a better world. I am seeking a seat on the Kubernetes Steering Committee because I bring special perspectives to governing the project.</description>
    </item>
    
    <item>
      <title>roadmap-cluster-deployment</title>
      <link>/sig-cluster-lifecycle/migrated-from-wiki/roadmap-cluster-deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-cluster-lifecycle/migrated-from-wiki/roadmap-cluster-deployment/</guid>
      <description>OBSOLETE
Cluster lifecycle includes deployment (infrastructure provisioning and bootstrapping Kubernetes), scaling, upgrades, and turndown.
Owner: @kubernetes/sig-cluster-lifecycle (kubernetes-sig-cluster-lifecycle at googlegroups.com, sig-cluster-lifecycle on slack)
There is no one-size-fits-all solution for cluster deployment and management (e.g., upgrades). There&amp;rsquo;s a spectrum of possible solutions, each with different tradeoffs: * opinionated solution (easier to use for a narrower solution space) vs. toolkit (easier to adapt and extend) * understandability (easier to modify) vs. configurability (addresses a broader solution space without coding)</description>
    </item>
    
    <item>
      <title>roadmap-docs</title>
      <link>/sig-docs/migrated-from-wiki/roadmap-docs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-docs/migrated-from-wiki/roadmap-docs/</guid>
      <description> Docs and examples roadmap If you&amp;rsquo;d like to help with documentation, please read the kubernetes.io site instructions.
If you&amp;rsquo;d like to contribute an example, please read the guidelines.
Owners: @kubernetes/docs, @kubernetes/examples
Labels: kind/documentation, kind/example
We&amp;rsquo;re currently planning a documentation site overhaul. Join kubernetes-dev@googlegroups.com to gain access to the proposals.
 Overhaul proposal Mocks API documentation improvements  </description>
    </item>
    
    <item>
      <title>roadmap-kubectl</title>
      <link>/sig-cli/migrated-from-wiki/roadmap-kubectl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-cli/migrated-from-wiki/roadmap-kubectl/</guid>
      <description>kubectl roadmap kubectl is the Kubernetes CLI.
If you&amp;rsquo;d like to contribute, please read the conventions and familiarize yourself with existing commands.
Owner: @kubernetes/kubectl
Label: component/kubectl
Motivation: kubectl brainstorm
Add new commands / subcommands / flags  Simplify support for multiple files  Manifest that can specify multiple files / http(s) URLs Default manifest manifest (ala Dockerfile or Makefile) Unpack archive (tgz, zip) and then invoke “-f” on that directory URL shortening via default URL prefix  Imperative set commands view commands Support run --edit and create --edit More kubectl create &amp;lt;sub-command&amp;gt; Support --dry-run for every mutation kubectl commands aliases  Allow user defined aliases for resources and commands Suggest possibly matching kubectl commands  Improve kubectl run  Make generated objects more discoverable: suggest the user to do kubectl get all to see what&amp;rsquo;s generated (extend all to more resources) Make it optional to specify name (auto generate name from image) Make kubectl run --restart=Never creates Pods (instead of Jobs)  Create commands/flags for common get + template patterns (e.</description>
    </item>
    
    <item>
      <title>role-of-sig-lead</title>
      <link>/events/2017/12-contributor-summit/role-of-sig-lead/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/12-contributor-summit/role-of-sig-lead/</guid>
      <description>What is the role of a sig lead Notetaker: @MHBauer
Lots of sig leads are present in the room
If there is a doc it’s years out of date
Joe wants to redefine the session Power and role of sig leads is defined in how we pick these people. Voting? Membership? What do we do for governance in general
Paul: Roles that are valuable that are not sig lead. Svc cat, moderator of discussions and took notes.</description>
    </item>
    
    <item>
      <title>rules-review-api</title>
      <link>/contributors/design-proposals/rules-review-api/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/rules-review-api/</guid>
      <description>&amp;ldquo;What can I do?&amp;rdquo; API Author: Eric Chiang (eric.chiang@coreos.com)
Overview Currently, to determine if a user is authorized to perform a set of actions, that user has to query each action individually through a SelfSubjectAccessReview.
Beyond making the authorization layer hard to reason about, it means web interfaces such as the OpenShift Web Console, Tectonic Console, and Kubernetes Dashboard, have to perform individual calls for every resource a page displays.</description>
    </item>
    
    <item>
      <title>runas-groupid</title>
      <link>/contributors/design-proposals/auth/runas-groupid/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/auth/runas-groupid/</guid>
      <description>RunAsGroup Proposal Author: krmayankk@
Status: Proposal
Abstract As a Kubernetes User, we should be able to specify both user id and group id for the containers running inside a pod on a per Container basis, similar to how docker allows that using docker run options -u, --user=&amp;quot;&amp;quot; Username or UID (format: &amp;lt;name|uid&amp;gt;[:&amp;lt;group|gid&amp;gt;]) format.
PodSecurityContext allows Kubernetes users to specify RunAsUser which can be overridden by RunAsUser in SecurityContext on a per Container basis.</description>
    </item>
    
    <item>
      <title>running-locally</title>
      <link>/contributors/devel/running-locally/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/running-locally/</guid>
      <description>Getting started locally Table of Contents
 Requirements  Linux Docker etcd go OpenSSL CFSSL  Clone the repository Starting the cluster Running a container Running a user defined pod Troubleshooting  I cannot reach service IPs on the network. I cannot create a replication controller with replica size greater than 1! What gives? I changed Kubernetes code, how do I run it? kubectl claims to start a container but get pods and docker ps don&amp;rsquo;t show it.</description>
    </item>
    
    <item>
      <title>runtime-client-server</title>
      <link>/contributors/design-proposals/node/runtime-client-server/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/runtime-client-server/</guid>
      <description>Client/Server container runtime Abstract A proposal of client/server implementation of kubelet container runtime interface.
Motivation Currently, any container runtime has to be linked into the kubelet. This makes experimentation difficult, and prevents users from landing an alternate container runtime without landing code in core kubernetes.
To facilitate experimentation and to enable user choice, this proposal adds a client/server implementation of the new container runtime interface. The main goal of this proposal is:</description>
    </item>
    
    <item>
      <title>runtime-pod-cache</title>
      <link>/contributors/design-proposals/node/runtime-pod-cache/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/runtime-pod-cache/</guid>
      <description>Kubelet: Runtime Pod Cache This proposal builds on top of the Pod Lifecycle Event Generator (PLEG) proposed in #12802. It assumes that Kubelet subscribes to the pod lifecycle event stream to eliminate periodic polling of pod states. Please see #12802. for the motivation and design concept for PLEG.
Runtime pod cache is an in-memory cache which stores the status of all pods, and is maintained by PLEG. It serves as a single source of truth for internal pod status, freeing Kubelet from querying the container runtime.</description>
    </item>
    
    <item>
      <title>runtimeconfig</title>
      <link>/contributors/design-proposals/cluster-lifecycle/runtimeconfig/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/cluster-lifecycle/runtimeconfig/</guid>
      <description>Overview Proposes adding a --feature-config to core kube system components: apiserver , scheduler, controller-manager, kube-proxy, and selected addons. This flag will be used to enable/disable alpha features on a per-component basis.
Motivation Motivation is enabling/disabling features that are not tied to an API group. API groups can be selectively enabled/disabled in the apiserver via existing --runtime-config flag on apiserver, but there is currently no mechanism to toggle alpha features that are controlled by e.</description>
    </item>
    
    <item>
      <title>scalability-good-practices</title>
      <link>/contributors/guide/scalability-good-practices/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/guide/scalability-good-practices/</guid>
      <description>Scalability Good Practices for Kubernetes contributors This document is written for contributors who would like to avoid their code being reverted for performance reasons
Table of Contents
 Who should read this document and what is in it? What does it mean to &amp;ldquo;break scalability&amp;rdquo;? Examples  Inefficient use of memory Explicit lists from the API server Superfluous API calls Complex and expensive computations on a critical path Big dependency changes  Summary Closing remarks  Who should read this document and what is in it?</description>
    </item>
    
    <item>
      <title>scalability-regressions-case-studies</title>
      <link>/sig-scalability/governance/scalability-regressions-case-studies/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-scalability/governance/scalability-regressions-case-studies/</guid>
      <description>Kubernetes Scalability/Performance Regressions - Case Studies &amp;amp; Insights by Shyam JVS, Google Inc
February 2018
Overview This document is a compilation of some interesting scalability/performance regression stories from the past. These were identified/studied/fixed largely by sig-scalability. We begin by listing them down, along with their succinct explanations, features/components that were involved, and relevant SIGs (besides sig-scalability). We also accompany them with data on what was the smallest scale, both for real and simulated (i.</description>
    </item>
    
    <item>
      <title>scalability-testing</title>
      <link>/contributors/design-proposals/scalability/scalability-testing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/scalability/scalability-testing/</guid>
      <description>Background We have a goal to be able to scale to 1000-node clusters by end of 2015. As a result, we need to be able to run some kind of regression tests and deliver a mechanism so that developers can test their changes with respect to performance.
Ideally, we would like to run performance tests also on PRs - although it might be impossible to run them on every single PR, we may introduce a possibility for a reviewer to trigger them if the change has non obvious impact on the performance (something like &amp;ldquo;k8s-bot run scalability tests please&amp;rdquo; should be feasible).</description>
    </item>
    
    <item>
      <title>scaling-new-contributors</title>
      <link>/events/2017/12-contributor-summit/scaling-new-contributors/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/12-contributor-summit/scaling-new-contributors/</guid>
      <description>Scaling New Contributors Lead: Vishnu Kannan
Notetaker: Chris Love
New pull requests are plateauing in k/k Takes much longer for a PR to get through
Contributor Experience 3 mo for PR merged - minor PR that was a fix Nobody reviewed - was ignored PRs are lacking process to get them on milestones Lack of organization and understanding PR process There is a need for a sig buyin We have a explicate state machine, instead of a implicate state machine We do have a well defined process for new contributors to follow We need to have committed resources to a PR, File an issue first, then file a PR When the needs sig is there, the issue will be processed and you can find out which sig</description>
    </item>
    
    <item>
      <title>scaling-up-and-scaling-down-and-addon-management</title>
      <link>/events/2017/12-contributor-summit/scaling-up-and-scaling-down-and-addon-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/12-contributor-summit/scaling-up-and-scaling-down-and-addon-management/</guid>
      <description>Scaling Up &amp;amp; Scaling Down &amp;amp; Addon Management Session… Notes by @justinsb
Scaling Up &amp;amp; Scaling Down Lots of users that want to run on a single node cluster - one node, one core thockin’s position: 1 node 1 core should continue to work (with 3.75GB, but ideally less than 2GB) Works today but only just - e.g. you only have 0.2 cores left today Addons / core components written with a different target in mind Some of the choices not optimal for single node (e.</description>
    </item>
    
    <item>
      <title>schedule-DS-pod-by-scheduler</title>
      <link>/contributors/design-proposals/scheduling/schedule-ds-pod-by-scheduler/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/scheduling/schedule-ds-pod-by-scheduler/</guid>
      <description>Schedule DaemonSet Pods by default scheduler, not DaemonSet controller @k82cn, Feb 2018, #42002.
Motivation A DaemonSet ensures that all (or some) nodes run a copy of a pod. As nodes are added to the cluster, pods are added to them. As nodes are removed from the cluster, those pods are garbage collected. Normally, the machine that a pod runs on is selected by the Kubernetes scheduler; however, pods of DaemonSet are created and scheduled by DaemonSet controller who leveraged kube-scheduler’s predicates policy.</description>
    </item>
    
    <item>
      <title>scheduler</title>
      <link>/contributors/devel/scheduler/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/scheduler/</guid>
      <description>The Kubernetes Scheduler The Kubernetes scheduler runs as a process alongside the other master components such as the API server. Its interface to the API server is to watch for Pods with an empty PodSpec.NodeName, and for each Pod, it posts a binding indicating where the Pod should be scheduled.
Exploring the code We are dividing scheduler into three layers from high level: - cmd/kube-scheduler/scheduler.go: This is the main() entry that does initialization before calling the scheduler framework.</description>
    </item>
    
    <item>
      <title>scheduler-equivalence-class</title>
      <link>/contributors/design-proposals/scheduling/scheduler-equivalence-class/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/scheduling/scheduler-equivalence-class/</guid>
      <description>Equivalence class based scheduling in Kubernetes Authors:
@resouer @wojtek-t @davidopp
Guideline  Objectives  Goals Non-Goals  Background  Terminology  Overview Detailed Design  Define equivalence class Equivalence class in predicate phase Keep equivalence class cache up-to-date  Notes for scheduler developers References  Objectives Goals  Define the equivalence class for pods during predicate phase in Kubernetes. Define how to use equivalence class to speed up predicate process.</description>
    </item>
    
    <item>
      <title>scheduler_algorithm</title>
      <link>/contributors/devel/scheduler_algorithm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/scheduler_algorithm/</guid>
      <description>Scheduler Algorithm in Kubernetes For each unscheduled Pod, the Kubernetes scheduler tries to find a node across the cluster according to a set of rules. A general introduction to the Kubernetes scheduler can be found at scheduler.md. In this document, the algorithm of how to select a node for the Pod is explained. There are two steps before a destination node of a Pod is chosen. The first step is filtering all the nodes and the second is ranking the remaining nodes to find a best fit for the Pod.</description>
    </item>
    
    <item>
      <title>scheduler_extender</title>
      <link>/contributors/design-proposals/scheduling/scheduler_extender/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/scheduling/scheduler_extender/</guid>
      <description>Scheduler extender There are three ways to add new scheduling rules (predicates and priority functions) to Kubernetes: (1) by adding these rules to the scheduler and recompiling, described here, (2) implementing your own scheduler process that runs instead of, or alongside of, the standard Kubernetes scheduler, (3) implementing a &amp;ldquo;scheduler extender&amp;rdquo; process that the standard Kubernetes scheduler calls out to as a final pass when making scheduling decisions.
This document describes the third approach.</description>
    </item>
    
    <item>
      <title>sebastiengoasguen_bio</title>
      <link>/events/elections/2017/sebastiengoasguen_bio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/elections/2017/sebastiengoasguen_bio/</guid>
      <description>Sebastien Goasguen Contact  GitHub: @sebgoa Twitter: @sebgoa Linkedin Profile Employer: Bitnami  Vision Being a member of the steering committee is not a technical role, it is a temporary position to help the community grow and ensure that the project (at large) succeeds in the long term.
The steering committee already has a long backlog of things to do and I submitted a PR some time ago to show my take on it.</description>
    </item>
    
    <item>
      <title>seccomp</title>
      <link>/contributors/design-proposals/node/seccomp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/seccomp/</guid>
      <description>Abstract A proposal for adding alpha support for seccomp to Kubernetes. Seccomp is a system call filtering facility in the Linux kernel which lets applications define limits on system calls they may make, and what should happen when system calls are made. Seccomp is used to reduce the attack surface available to applications.
Motivation Applications use seccomp to restrict the set of system calls they can make. Recently, container runtimes have begun adding features to allow the runtime to interact with seccomp on behalf of the application, which eliminates the need for applications to link against libseccomp directly.</description>
    </item>
    
    <item>
      <title>secret-configmap-downwardapi-file-mode</title>
      <link>/contributors/design-proposals/node/secret-configmap-downwardapi-file-mode/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/secret-configmap-downwardapi-file-mode/</guid>
      <description>Secrets, configmaps and downwardAPI file mode bits Author: Rodrigo Campos (@rata), Tim Hockin (@thockin)
Date: July 2016
Status: Design in progress
Goal Allow users to specify permission mode bits for a secret/configmap/downwardAPI file mounted as a volume. For example, if a secret has several keys, a user should be able to specify the permission mode bits for any file, and they may all have different modes.
Let me say that with &amp;ldquo;permission&amp;rdquo; I only refer to the file mode here and I may use them interchangeably.</description>
    </item>
    
    <item>
      <title>secrets</title>
      <link>/contributors/design-proposals/auth/secrets/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/auth/secrets/</guid>
      <description>Abstract A proposal for the distribution of secrets (passwords, keys, etc) to the Kubelet and to containers inside Kubernetes using a custom volume type. See the secrets example for more information.
Motivation Secrets are needed in containers to access internal resources like the Kubernetes master or external resources such as git repositories, databases, etc. Users may also want behaviors in the kubelet that depend on secret data (credentials for image pull from a docker registry) associated with pods.</description>
    </item>
    
    <item>
      <title>security</title>
      <link>/contributors/design-proposals/auth/security/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/auth/security/</guid>
      <description>Security in Kubernetes Kubernetes should define a reasonable set of security best practices that allows processes to be isolated from each other, from the cluster infrastructure, and which preserves important boundaries between those who manage the cluster, and those who use the cluster.
While Kubernetes today is not primarily a multi-tenant system, the long term evolution of Kubernetes will increasingly rely on proper boundaries between users and administrators. The code running on the cluster must be appropriately isolated and secured to prevent malicious parties from affecting the entire cluster.</description>
    </item>
    
    <item>
      <title>security_context</title>
      <link>/contributors/design-proposals/auth/security_context/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/auth/security_context/</guid>
      <description>Security Contexts Abstract A security context is a set of constraints that are applied to a container in order to achieve the following goals (from security design):
 Ensure a clear isolation between container and the underlying host it runs on Limit the ability of the container to negatively impact the infrastructure or other containers  Background The problem of securing containers in Kubernetes has come up before and the potential problems with container security are well known.</description>
    </item>
    
    <item>
      <title>selector-generation</title>
      <link>/contributors/design-proposals/apps/selector-generation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/apps/selector-generation/</guid>
      <description>Design Goals Make it really hard to accidentally create a job which has an overlapping selector, while still making it possible to chose an arbitrary selector, and without adding complex constraint solving to the APIserver.
Use Cases  user can leave all label and selector fields blank and system will fill in reasonable ones: non-overlappingness guaranteed. user can put on the pod template some labels that are useful to the user, without reasoning about non-overlappingness.</description>
    </item>
    
    <item>
      <title>self-hosted-kubelet</title>
      <link>/contributors/design-proposals/cluster-lifecycle/self-hosted-kubelet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/cluster-lifecycle/self-hosted-kubelet/</guid>
      <description>Proposal: Self-hosted kubelet Abstract In a self-hosted Kubernetes deployment (see this comment for background on self hosted kubernetes), we have the initial bootstrap problem. When running self-hosted components, there needs to be a mechanism for pivoting from the initial bootstrap state to the kubernetes-managed (self-hosted) state. In the case of a self-hosted kubelet, this means pivoting from the initial kubelet defined and run on the host, to the kubelet pod which has been scheduled to the node.</description>
    </item>
    
    <item>
      <title>self-hosted-kubernetes</title>
      <link>/contributors/design-proposals/cluster-lifecycle/self-hosted-kubernetes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/cluster-lifecycle/self-hosted-kubernetes/</guid>
      <description>Proposal: Self-hosted Control Plane Author: Brandon Philips brandon.philips@coreos.com
Motivations  Running our components in pods would solve many problems, which we&amp;rsquo;ll otherwise need to implement other, less portable, more brittle solutions to, and doesn&amp;rsquo;t require much that we don&amp;rsquo;t need to do for other reasons. Full self-hosting is the eventual goal.
 Brian Grant (ref)   What is self-hosted? Self-hosted Kubernetes runs all required and optional components of a Kubernetes cluster on top of Kubernetes itself.</description>
    </item>
    
    <item>
      <title>selinux</title>
      <link>/contributors/design-proposals/node/selinux/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/selinux/</guid>
      <description>Abstract A proposal for enabling containers in a pod to share volumes using a pod level SELinux context.
Motivation Many users have a requirement to run pods on systems that have SELinux enabled. Volume plugin authors should not have to explicitly account for SELinux except for volume types that require special handling of the SELinux context during setup.
Currently, each container in a pod has an SELinux context. This is not an ideal factoring for sharing resources using SELinux.</description>
    </item>
    
    <item>
      <title>selinux-enhancements</title>
      <link>/contributors/design-proposals/node/selinux-enhancements/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/selinux-enhancements/</guid>
      <description>Abstract Presents a proposal for enhancing the security of Kubernetes clusters using SELinux and simplifying the implementation of SELinux support within the Kubelet by removing the need to label the Kubelet directory with an SELinux context usable from a container.
Motivation The current Kubernetes codebase relies upon the Kubelet directory being labeled with an SELinux context usable from a container. This means that a container escaping namespace isolation will be able to use any file within the Kubelet directory without defeating kernel MAC (mandatory access control).</description>
    </item>
    
    <item>
      <title>server-get</title>
      <link>/contributors/design-proposals/api-machinery/server-get/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/server-get/</guid>
      <description>Expose get output from the server Today, all clients must reproduce the tabular and describe output implemented in kubectl to perform simple lists of objects. This logic in many cases is non-trivial and condenses multiple fields into succinct output. It also requires that every client provide rendering logic for every possible type, including those provided by API aggregation or third party resources which may not be known at compile time.</description>
    </item>
    
    <item>
      <title>service-discovery</title>
      <link>/contributors/design-proposals/network/service-discovery/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/network/service-discovery/</guid>
      <description>Service Discovery Proposal Goal of this document To consume a service, a developer needs to know the full URL and a description of the API. Kubernetes contains the host and port information of a service, but it lacks the scheme and the path information needed if the service is not bound at the root. In this document we propose some standard kubernetes service annotations to fix these gaps. It is important that these annotations are a standard to allow for standard service discovery across Kubernetes implementations.</description>
    </item>
    
    <item>
      <title>service-external-name</title>
      <link>/contributors/design-proposals/network/service-external-name/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/network/service-external-name/</guid>
      <description>Service externalName Author: Tim Hockin (@thockin), Rodrigo Campos (@rata), Rudi C (@therc)
Date: August 2016
Status: Implementation in progress
Goal Allow a service to have a CNAME record in the cluster internal DNS service. For example, the lookup for a db service could return a CNAME that points to the RDS resource something.rds.aws.amazon.com. No proxying is involved.
Motivation There were many related issues, but we&amp;rsquo;ll try to summarize them here.</description>
    </item>
    
    <item>
      <title>service_accounts</title>
      <link>/contributors/design-proposals/auth/service_accounts/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/auth/service_accounts/</guid>
      <description>Service Accounts Motivation Processes in Pods may need to call the Kubernetes API. For example: - scheduler - replication controller - node controller - a map-reduce type framework which has a controller that then tries to make a dynamically determined number of workers and watch them - continuous build and push system - monitoring system
They also may interact with services other than the Kubernetes API, such as: - an image repository, such as docker &amp;ndash; both when the images are pulled to start the containers, and for writing images in the case of pods that generate images.</description>
    </item>
    
    <item>
      <title>session-notes</title>
      <link>/events/2017/05-leadership-summit/session-notes/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/05-leadership-summit/session-notes/readme/</guid>
      <description>Per the Template &amp;amp; Instruction Document, upload your session notes here with the name format:
HHMM-HHMM_SESSIONTITLE.md
where HHMM is the time in 24-hour format.</description>
    </item>
    
    <item>
      <title>setting-up-cla-check</title>
      <link>/github-management/setting-up-cla-check/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/github-management/setting-up-cla-check/</guid>
      <description>Setting up the CNCF CLA check If you are trying to sign the CLA so your PR&amp;rsquo;s can be merged, please read the CLA docs
If you are a Kubernetes GitHub organization or repo owner, and would like to setup the Linux Foundation CNCF CLA check for your repositories, please read on.
Setup the webhook  Go to the settings for your organization or webhook, and choose Webhooks from the menu, then &amp;ldquo;Add webhook&amp;rdquo;  Payload URL: https://identity.</description>
    </item>
    
    <item>
      <title>should-k8s-use-feature-branches</title>
      <link>/events/2017/12-contributor-summit/should-k8s-use-feature-branches/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/12-contributor-summit/should-k8s-use-feature-branches/</guid>
      <description>Should Kubernetes use feature branches? Lead: Brian Grant
Note Taker(s): Aaron Crickenberger
Focusing on our current release cadence:
 We spend several weeks putting features in We spend several weeks stabilizing the week kinda/sortof  The people who are managing the release process don’t have the ability to push back and say this can’t go in and we don’t want to slip the release so they’re left powerless
In earlier session today over breaking up monolith:</description>
    </item>
    
    <item>
      <title>sig-charter-template</title>
      <link>/committee-steering/governance/sig-charter-template/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/committee-steering/governance/sig-charter-template/</guid>
      <description>SIG YOURSIG Charter This charter adheres to the conventions described in the Kubernetes Charter README and uses the Roles and Organization Management outlined in sig-governance.
Scope Include a 2-3 sentence summary of what work SIG TODO does. Imagine trying to explain your work to a colleague who is familiar with Kubernetes but not necessarily all of the internals.
In scope Link to SIG section in sigs.yaml
Code, Binaries and Services  list of what qualifies a piece of code, binary or service as falling into the scope of this SIG e.</description>
    </item>
    
    <item>
      <title>sig-creation-procedure</title>
      <link>/sig-creation-procedure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-creation-procedure/</guid>
      <description>SIG creation procedure Moved to sig governance</description>
    </item>
    
    <item>
      <title>sig-governance</title>
      <link>/committee-steering/governance/sig-governance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/committee-steering/governance/sig-governance/</guid>
      <description>SIG Roles and Organizational Governance This charter adheres to the conventions described in the Kubernetes Charter README.
This document will be updated as needed to meet the current needs of the Kubernetes project.
Roles Notes on Roles Unless otherwise stated, individuals are expected to be responsive and active within their roles. Within this section &amp;ldquo;member&amp;rdquo; refers to a member of a Chair, Tech Lead or Subproject Owner Role. (this different from a SIG or Organization Member).</description>
    </item>
    
    <item>
      <title>sig-governance</title>
      <link>/sig-governance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-governance/</guid>
      <description>SIG Governance In order to standardize Special Interest Group efforts, create maximum transparency, and route contributors to the appropriate SIG, SIGs should follow the guidelines stated below:
 Meet regularly, at least for 30 minutes every 3 weeks, except November and December Keep up-to-date meeting notes, linked from the SIG&amp;rsquo;s page in the community repo Announce meeting agenda and minutes after each meeting, on their SIG mailing list Record SIG meeting and make it publicly available Ensure the SIG&amp;rsquo;s mailing list and slack channel are archived Report activity in the weekly community meeting at least once every 6 weeks Participate in release planning meetings and retrospectives, and burndown meetings, as needed Ensure related work happens in a project-owned github org and repository, with code and tests explicitly owned and supported by the SIG, including issue triage, PR reviews, test-failure response, bug fixes, etc.</description>
    </item>
    
    <item>
      <title>sig-governance-requirements</title>
      <link>/committee-steering/governance/sig-governance-requirements/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/committee-steering/governance/sig-governance-requirements/</guid>
      <description>SIG Governance Requirements Goals This document outlines the recommendations and requirements for defining SIG and subproject governance.
This doc uses rfc2119 to indicate keyword requirement levels. Sub elements of a list inherit the requirements of the parent by default unless overridden.
Checklist Following is the checklist of items that should be considered as part of defining governance for any subarea of the Kubernetes project.
Roles  MUST enumerate any roles within the SIG and the responsibilities of each MUST define process for changing the membership of roles  When and how new members are chosen / added to each role When and how existing members are retired from each role  SHOULD define restrictions / requirements for membership of roles MAY define target staffing numbers of roles  Organizational management  MUST define when and how collaboration between members of the SIG is organized</description>
    </item>
    
    <item>
      <title>sig-list</title>
      <link>/sig-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-list/</guid>
      <description>SIGs and Working Groups Most community activity is organized into Special Interest Groups (SIGs), time bounded Working Groups, and the community meeting.
SIGs follow these guidelines although each of these groups may operate a little differently depending on their needs and workflow.
Each group&amp;rsquo;s material is in its subdirectory in this project.
When the need arises, a new SIG can be created
Master SIG List    Name Label Chairs Contact Meetings     API Machinery api-machinery * Daniel Smith, Google</description>
    </item>
    
    <item>
      <title>simple-rolling-update</title>
      <link>/contributors/design-proposals/cli/simple-rolling-update/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/cli/simple-rolling-update/</guid>
      <description>Simple rolling update This is a lightweight design document for simple rolling update in kubectl.
Complete execution flow can be found here. See the example of rolling update for more information.
Lightweight rollout Assume that we have a current replication controller named foo and it is running image image:v1
kubectl rolling-update foo [foo-v2] --image=myimage:v2
If the user doesn&amp;rsquo;t specify a name for the &amp;lsquo;next&amp;rsquo; replication controller, then the &amp;lsquo;next&amp;rsquo; replication controller is renamed to the name of the original replication controller.</description>
    </item>
    
    <item>
      <title>slack-guidelines</title>
      <link>/communication/slack-guidelines/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/communication/slack-guidelines/</guid>
      <description>SLACK GUIDELINES Slack is the main communication platform for Kubernetes outside of our mailing lists. It’s important that conversation stays on topic in each channel, and that everyone abides by the Code of Conduct. We have over 30,000 members who should all expect to have a positive experience.
Chat is searchable and public. Do not make comments that you would not say on a video recording or in another public space.</description>
    </item>
    
    <item>
      <title>slos</title>
      <link>/sig-scalability/slos/slos/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-scalability/slos/slos/</guid>
      <description>Kubernetes scalability and performance SLIs/SLOs What Kubernetes guarantees? One of the important aspects of Kubernetes is its scalability and performance characteristic. As Kubernetes user or operator/administrator of a cluster you would expect to have some guarantees in those areas.
The goal of this doc is to organize the guarantees that Kubernetes provides in these areas.
What do we require from SLIs/SLOs? We are going to define more SLIs and SLOs based on the most important indicators in the system.</description>
    </item>
    
    <item>
      <title>staging</title>
      <link>/contributors/devel/staging/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/staging/</guid>
      <description>Staging Directory and Publishing The staging/ directory of Kubernetes contains a number of pseudo repositories (&amp;ldquo;staging repos&amp;rdquo;). They are symlinked into Kubernetes&amp;rsquo; vendor/ directory for Golang to pick them up.
We publish the staging repos using the publishing bot. It uses git filter-branch essentially to cut the staging directories into separate git trees and pushing the new commits to the corresponding real repositories in the kubernetes organization on Github.</description>
    </item>
    
    <item>
      <title>stateful-apps</title>
      <link>/contributors/design-proposals/apps/stateful-apps/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/apps/stateful-apps/</guid>
      <description>StatefulSets: Running pods which need strong identity and storage Motivation Many examples of clustered software systems require stronger guarantees per instance than are provided by the Replication Controller (aka Replication Controllers). Instances of these systems typically require:
 Data per instance which should not be lost even if the pod is deleted, typically on a persistent volume  Some cluster instances may have tens of TB of stored data - forcing new instances to replicate data from other members over the network is onerous  A stable and unique identity associated with that instance of the storage - such as a unique member id A consistent network identity that allows other members to locate the instance even if the pod is deleted A predictable number of instances to ensure that systems can form a quorum  This may be necessary during initialization  Ability to migrate from node to node with stable network identity (DNS name) The ability to scale up in a controlled fashion, but are very rarely scaled down without human intervention  Kubernetes should expose a pod controller (a StatefulSet) that satisfies these requirements in a flexible manner.</description>
    </item>
    
    <item>
      <title>statefulset-update</title>
      <link>/contributors/design-proposals/apps/statefulset-update/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/apps/statefulset-update/</guid>
      <description>StatefulSet Updates Author: kow3ns@
Status: Proposal
Abstract Currently (as of Kubernetes 1.6), .Spec.Replicas and .Spec.Template.Containers are the only mutable fields of the StatefulSet API object. Updating .Spec.Replicas will scale the number of Pods in the StatefulSet. Updating .Spec.Template.Containers causes all subsequently created Pods to have the specified containers. In order to cause the StatefulSet controller to apply its updated .Spec, users must manually delete each Pod. This manual method of applying updates is error prone.</description>
    </item>
    
    <item>
      <title>statefulset_notes</title>
      <link>/events/2016/developer-summit-2016/statefulset_notes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2016/developer-summit-2016/statefulset_notes/</guid>
      <description>StatefulSets Session Topics to talk about: * local volumes * requests for the storage sig * reclaim policies * Filtering APIs for scheduler * Data locality * State of the StateFulSet * Portable IPs * Sticky Regions * Renaming Pods
State of the StatefulSet 1.5 will come out soon, we&amp;rsquo;ll go beta for StatefulSets in that one. One of the questions is what are the next steps for Statefulsets? One thing is a long beta, so that we know we can trust statefulsets and they&amp;rsquo;re safe.</description>
    </item>
    
    <item>
      <title>steering-committee-update</title>
      <link>/events/2017/12-contributor-summit/steering-committee-update/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/12-contributor-summit/steering-committee-update/</guid>
      <description>Steering Committee Update Notes by @jberkus
Showed list of Steering Committee backlog.
We had a meeting, and the two big items we&amp;rsquo;d been pushing on hard were:
 votining in the proposals in the bootstrap committee how we&amp;rsquo;re going to handle incubator and contrib etc.  Incubator/contrib: one of our big concerns are what the the consequences for projects and ecosystems. We&amp;rsquo;re still discussing it, please be patient. In the process of solving the incubator process, we have to answer what is kubernetes, which is probably SIGs, but what&amp;rsquo;s a SIG, and who decides, and &amp;hellip; we end up having to examine everything.</description>
    </item>
    
    <item>
      <title>steering-update</title>
      <link>/events/2018/05-contributor-summit/steering-update/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2018/05-contributor-summit/steering-update/</guid>
      <description>Steering Committee Update Leads: pwittrock, timothysc
Thanks to our notetaker: tpepper
 incubation is deprecated, &amp;ldquo;associated&amp;rdquo; projects are a thing WG are horizontal across SIGs and are ephemeral. Subprojects own a piece of code and relate to a SIG. Example: SIG-Cluster-Lifecycle with kubeadm, kops, etc. under it. SIG charters: PR a proposed new SIG with the draft charter. Discussion can then happen on GitHub around the evolving charter. This is cleaner and more efficient than discussing on mailing list.</description>
    </item>
    
    <item>
      <title>strategic-merge-patch</title>
      <link>/contributors/devel/strategic-merge-patch/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/strategic-merge-patch/</guid>
      <description>Strategic Merge Patch Background Kubernetes supports a customized version of JSON merge patch called strategic merge patch. This patch format is used by kubectl apply, kubectl edit and kubectl patch, and contains specialized directives to control how specific fields are merged.
In the standard JSON merge patch, JSON objects are always merged but lists are always replaced. Often that isn&amp;rsquo;t what we want. Let&amp;rsquo;s say we start with the following Pod:</description>
    </item>
    
    <item>
      <title>support_traffic_shaping_for_kubelet_cni</title>
      <link>/contributors/design-proposals/network/support_traffic_shaping_for_kubelet_cni/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/network/support_traffic_shaping_for_kubelet_cni/</guid>
      <description>Support traffic shaping for CNI network plugin Version: Alpha
Authors: @m1093782566
Motivation and background Currently the kubenet code supports applying basic traffic shaping during pod setup. This will happen if bandwidth-related annotations have been added to the pod&amp;rsquo;s metadata, for example:
{ &amp;#34;kind&amp;#34;: &amp;#34;Pod&amp;#34;, &amp;#34;metadata&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;iperf-slow&amp;#34;, &amp;#34;annotations&amp;#34;: { &amp;#34;kubernetes.io/ingress-bandwidth&amp;#34;: &amp;#34;10M&amp;#34;, &amp;#34;kubernetes.io/egress-bandwidth&amp;#34;: &amp;#34;10M&amp;#34; } } } Our current implementation uses the linux tc to add an download(ingress) and upload(egress) rate limiter using 1 root qdisc, 2 class(one for ingress and one for egress) and 2 filter(one for ingress and one for egress attached to the ingress and egress classes respectively).</description>
    </item>
    
    <item>
      <title>svcacct-token-volume-source</title>
      <link>/contributors/design-proposals/storage/svcacct-token-volume-source/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/svcacct-token-volume-source/</guid>
      <description>Service Account Token Volumes Authors: @smarterclayton @liggitt @mikedanese
Summary Kubernetes is able to provide pods with unique identity tokens that can prove the caller is a particular pod to a Kubernetes API server. These tokens are injected into pods as secrets. This proposal proposes a new mechanism of distribution with support for improved service account tokens and explores how to migrate from the existing mechanism backwards compatibly.
Motivation Many workloads running on Kubernetes need to prove to external parties who they are in order to participate in a larger application environment.</description>
    </item>
    
    <item>
      <title>synchronous-garbage-collection</title>
      <link>/contributors/design-proposals/api-machinery/synchronous-garbage-collection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/synchronous-garbage-collection/</guid>
      <description>Table of Contents
 Overview API Design  Standard Finalizers OwnerReference DeleteOptions  Components changes  API Server Garbage Collector Controllers  Handling circular dependencies Unhandled cases Implications to existing clients  Overview Users of the server-side garbage collection need to determine if the garbage collection is done. For example: * Currently kubectl delete rc blocks until all the pods are terminating. To convert to use server-side garbage collection, kubectl has to be able to determine if the garbage collection is done.</description>
    </item>
    
    <item>
      <title>sysctl</title>
      <link>/contributors/design-proposals/node/sysctl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/sysctl/</guid>
      <description>Setting Sysctls on the Pod Level This proposal aims at extending the current pod specification with support for namespaced kernel parameters (sysctls) set for each pod.
Roadmap v1.4 initial implementation for v1.4 https://github.com/kubernetes/kubernetes/pull/27180  node-level whitelist for safe sysctls: kernel.shm_rmid_forced, net.ipv4.ip_local_port_range, net.ipv4.tcp_max_syn_backlog, net.ipv4.tcp_syncookies (disabled by-default) unsafe sysctls: kernel.msg*, kernel.sem, kernel.shm*, fs.mqueue.*, net.* new kubelet flag: --experimental-allowed-unsafe-sysctls PSP default: *  document node-level whitelist with kubectl flags and taints/tolerations document host-level sysctls with daemon sets + taints/tolerations in parallel: kernel upstream patches to fix ipc accounting for 4.</description>
    </item>
    
    <item>
      <title>system_throughput</title>
      <link>/sig-scalability/slos/system_throughput/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-scalability/slos/system_throughput/</guid>
      <description>System throughput SLI/SLO details User stories  As a user, I want a guarantee that my workload of X pods can be started within a given time As a user, I want to understand how quickly I can react to a dramatic change in workload profile when my workload exhibits very bursty behavior (e.g. shop during Back Friday Sale) As a user, I want a guarantee how quickly I can recreate the whole setup in case of a serious disaster which brings the whole cluster down.</description>
    </item>
    
    <item>
      <title>taint-node-by-condition</title>
      <link>/contributors/design-proposals/scheduling/taint-node-by-condition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/scheduling/taint-node-by-condition/</guid>
      <description>Taints Node according to NodeConditions @k82cn, @gmarek, @jamiehannaford, Jul 15, 2017
Relevant issues:  https://github.com/kubernetes/kubernetes/issues/42001 https://github.com/kubernetes/kubernetes/issues/45717  Motivation In kubernetes 1.8 and before, there are six Node Conditions, each with three possible values: True, False or Unknown. Kubernetes components modify and check those node conditions without any consideration to pods and their specs. For example, the scheduler will filter out all nodes whose NetworkUnavailable condition is True, meaning that pods on the host network can not be scheduled to those nodes, even though a user might want that.</description>
    </item>
    
    <item>
      <title>taint-toleration-dedicated</title>
      <link>/contributors/design-proposals/scheduling/taint-toleration-dedicated/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/scheduling/taint-toleration-dedicated/</guid>
      <description>Taints, Tolerations, and Dedicated Nodes Introduction This document describes taints and tolerations, which constitute a generic mechanism for restricting the set of pods that can use a node. We also describe one concrete use case for the mechanism, namely to limit the set of users (or more generally, authorization domains) who can access a set of nodes (a feature we call dedicated nodes). There are many other uses&amp;ndash;for example, a set of nodes with a particular piece of hardware could be reserved for pods that require that hardware, or a node could be marked as unschedulable when it is being drained before shutdown, or a node could trigger evictions when it experiences hardware or software problems or abnormal node configurations; see issues #17190 and #3885 for more discussion.</description>
    </item>
    
    <item>
      <title>testing</title>
      <link>/contributors/devel/testing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/testing/</guid>
      <description>Testing guide Table of Contents
 Testing guide  Unit tests Run all unit tests Set go flags during unit tests Run unit tests from certain packages Run specific unit test cases in a package Stress running unit tests Unit test coverage Benchmark unit tests Integration tests Install etcd dependency Etcd test data Run integration tests Run a specific integration test End-to-End tests   This assumes you already read the development guide to install go, godeps, and configure your git client.</description>
    </item>
    
    <item>
      <title>the1-on-1hour</title>
      <link>/mentoring/the1-on-1hour/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/mentoring/the1-on-1hour/</guid>
      <description>The 1:1 Hour One mentor. One mentee. One Hour.
In this mentorship opportunity, you&amp;rsquo;ll be paired with someone in the ecosystem that can help drive your upstream Kubernetes experience forward with a quick burst of information. The mentee controls the content that will be served during that hour. This is open for new AND current contributors of all levels.
Activities that you can select from include: * pair programming * live code review how-to (for those wishing to be a reviewer or be a better reviewer) * live docs review how-to * live code review of a proposed solution you are working on * codebase tour of a certain area * guidance on your contributor path (advice on becoming an approver, etc.</description>
    </item>
    
    <item>
      <title>thirdpartyresources</title>
      <link>/contributors/design-proposals/api-machinery/thirdpartyresources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/api-machinery/thirdpartyresources/</guid>
      <description>Moving ThirdPartyResources to beta Background There are a number of important issues with the alpha version of ThirdPartyResources that we wish to address to move TPR to beta. The list is tracked here, and also includes feedback from existing Kubernetes ThirdPartyResource users. This proposal covers the steps we believe are necessary to move TPR to beta and to prevent future challenges in upgrading.
Goals  Ensure ThirdPartyResource APIs operate consistently with first party Kubernetes APIs.</description>
    </item>
    
    <item>
      <title>thresholds</title>
      <link>/sig-scalability/configs-and-limits/thresholds/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-scalability/configs-and-limits/thresholds/</guid>
      <description>Kubernetes Scalability thresholds Background Since 1.6 release Kubernetes officially supports 5000-node clusters. However, the question is what that actually means. As of early Q3 2017 we are in the process of defining set of performance-related SLIs (Service Level Indicators) and SLOs (Service Level Objectives).
However, no matter what SLIs and SLOs we have, there will always be some users coming and saying that their cluster is not meeting the SLOs.</description>
    </item>
    
    <item>
      <title>timothysc_bio</title>
      <link>/events/elections/2017/timothysc_bio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/elections/2017/timothysc_bio/</guid>
      <description>Timothy St. Clair Contact Information  GitHub/Slack: @timothysc Email: tstclair@heptio.com Twitter: @timothysc  Background I&amp;rsquo;ve been involved in the Kubernetes project since 2014, and prior to that I was an active member in the Mesos community, with over 10+ years of experience working on open source projects across a plethora of communities. During my tenure, I&amp;rsquo;ve been lucky enough to meet a number of great folks working on various SIGs:</description>
    </item>
    
    <item>
      <title>troubleshoot-running-pods</title>
      <link>/contributors/design-proposals/node/troubleshoot-running-pods/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/node/troubleshoot-running-pods/</guid>
      <description>Troubleshoot Running Pods  Status: Pending Version: Alpha Implementation Owner: @verb  This proposal seeks to add first class support for troubleshooting by creating a mechanism to execute a shell or other troubleshooting tools inside a running pod without requiring that the associated container images include such tools.
Motivation Development Many developers of native Kubernetes applications wish to treat Kubernetes as an execution platform for custom binaries produced by a build system.</description>
    </item>
    
    <item>
      <title>update-release-docs</title>
      <link>/contributors/devel/update-release-docs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/update-release-docs/</guid>
      <description>This document relates to the release process and can be found here.
This file is a redirect stub. It should be deleted within 3 months from the current date, or by the release date of k8s v1.12, whichever comes sooner.</description>
    </item>
    
    <item>
      <title>updating-docs-for-feature-changes</title>
      <link>/contributors/devel/updating-docs-for-feature-changes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/updating-docs-for-feature-changes/</guid>
      <description>This document relates to the release process and can be found here.
This file is a redirect stub. It should be deleted within 3 months from the current date, or by the release date of k8s v1.12, whichever comes sooner.</description>
    </item>
    
    <item>
      <title>vagrant</title>
      <link>/contributors/devel/vagrant/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/vagrant/</guid>
      <description>Getting started with Vagrant Running Kubernetes with Vagrant is an easy way to run/test/develop on your local machine in an environment using the same setup procedures when running on GCE or AWS cloud providers. This provider is not tested on a per PR basis, if you experience bugs when testing from HEAD, please open an issue.
Prerequisites  Install latest version &amp;gt;= 1.8.1 of vagrant from http://www.vagrantup.com/downloads.html
 Install a virtual machine host.</description>
    </item>
    
    <item>
      <title>vault-based-kms-provider</title>
      <link>/contributors/design-proposals/vault-based-kms-provider/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/vault-based-kms-provider/</guid>
      <description>Vault based KMS provider for envelope encryption of secrets in etcd3 Abstract Kubernetes, starting with the release 1.7, adds Alpha support ( via PRs 41939 and 46460) to encrypt secrets and resources in etcd3 via a configured Provider. This release supports three providers viz. aesgcm, aescbc, secretbox. These providers store the encryption key(s) locally in a server configuration file. The provider encrypts and decrypts secrets in-process. Building upon these, a KMS provider framework with an option to support different KMS providers like google cloud KMS is being added via PRs 48574 and 49350.</description>
    </item>
    
    <item>
      <title>versioning</title>
      <link>/contributors/design-proposals/release/versioning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/release/versioning/</guid>
      <description>Kubernetes Release Versioning Reference: Semantic Versioning
Legend:
 Kube X.Y.Z refers to the version (git tag) of Kubernetes that is released. This versions all components: apiserver, kubelet, kubectl, etc. (X is the major version, Y is the minor version, and Z is the patch version.)  Release versioning Minor version scheme and timeline  Kube X.Y.0-alpha.W, W &amp;gt; 0 (Branch: master)  Alpha releases are released roughly every two weeks directly from the master branch.</description>
    </item>
    
    <item>
      <title>vertical-pod-autoscaler</title>
      <link>/contributors/design-proposals/autoscaling/vertical-pod-autoscaler/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/autoscaling/vertical-pod-autoscaler/</guid>
      <description>Vertical Pod Autoscaler Authors: kgrygiel, mwielgus Contributors: DirectXMan12, fgrzadkowski, jszczepkowski, smarterclayton
Vertical Pod Autoscaler (#10782), later referred to as VPA (aka. &amp;ldquo;rightsizing&amp;rdquo; or &amp;ldquo;autopilot&amp;rdquo;) is an infrastructure service that automatically sets resource requirements of Pods and dynamically adjusts them in runtime, based on analysis of historical resource utilization, amount of resources available in the cluster and real-time events, such as OOMs.
 Introduction  Background Purpose Related features  Requirements  Functional Availability Extensibility  Design  Overview Architecture overview API Admission Controller Recommender Updater Recommendation model History Storage Open questions  Future work  Pods that require VPA to start Combining vertical and horizontal scaling Batch workloads  Alternatives considered  Pods point at VPA VPA points at Deployment Actuation using the Deployment update mechanism    Introduction Background  Compute resources Resource QoS Admission Controllers External Admission Webhooks  Purpose Vertical scaling has two objectives:</description>
    </item>
    
    <item>
      <title>volume-hostpath-qualifiers</title>
      <link>/contributors/design-proposals/storage/volume-hostpath-qualifiers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/volume-hostpath-qualifiers/</guid>
      <description>Support HostPath volume existence qualifiers Introduction A Host volume source is probably the simplest volume type to define, needing only a single path. However, that simplicity comes with many assumptions and caveats.
This proposal describes one of the issues associated with Host volumes &amp;mdash; their silent and implicit creation of directories on the host &amp;mdash; and proposes a solution.
Problem Right now, under Docker, when a bindmount references a hostPath, that path will be created as an empty directory, owned by root, if it does not already exist.</description>
    </item>
    
    <item>
      <title>volume-metrics</title>
      <link>/contributors/design-proposals/storage/volume-metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/volume-metrics/</guid>
      <description>Volume operation metrics Goal Capture high level metrics for various volume operations in Kubernetes.
Motivation Currently we don&amp;rsquo;t have high level metrics that captures time taken and success/failures rates of various volume operations.
This proposal aims to implement capturing of these metrics at a level higher than individual volume plugins.
Implementation Metric format and collection Volume metrics emitted will fall under category of service metrics as defined in Kubernetes Monitoring Architecture.</description>
    </item>
    
    <item>
      <title>volume-ownership-management</title>
      <link>/contributors/design-proposals/storage/volume-ownership-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/volume-ownership-management/</guid>
      <description>Volume plugins and idempotency Currently, volume plugins have a SetUp method which is called in the context of a higher-level workflow within the kubelet which has externalized the problem of managing the ownership of volumes. This design has a number of drawbacks that can be mitigated by completely internalizing all concerns of volume setup behind the volume plugin SetUp method.
Known issues with current externalized design  The ownership management is currently repeatedly applied, which breaks packages that require special permissions in order to work correctly There is a gap between files being mounted/created by volume plugins and when their ownership is set correctly; race conditions exist around this Solving the correct application of ownership management in an externalized model is difficult and makes it clear that the a transaction boundary is being broken by the externalized design  Additional issues with externalization Fully externalizing any one concern of volumes is difficult for a number of reasons:</description>
    </item>
    
    <item>
      <title>volume-plugin-faq</title>
      <link>/sig-storage/volume-plugin-faq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-storage/volume-plugin-faq/</guid>
      <description>Kubernetes Volume Plugin FAQ for Storage Vendors Last Updated: 02/08/2018
What is Kubernetes volume plugin?
A Kubernetes Volume plugin extends the Kubernetes volume interface to support a block and/or file storage system.
In-tree vs. Out-of-Tree Volume Plugins How do I implement a Volume plugin?
There are three methods to implement a volume plugin: 1. In-tree volume plugin 2. Out-of-tree FlexVolume driver 3. Out-of-tree CSI driver
The Kubernetes Storage SIG, which is responsible for all volume code in the Kubernetes core repository, is no longer accepting new in-tree volume plugins.</description>
    </item>
    
    <item>
      <title>volume-provisioning</title>
      <link>/contributors/design-proposals/storage/volume-provisioning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/volume-provisioning/</guid>
      <description>Abstract Real Kubernetes clusters have a variety of volumes which differ widely in size, iops performance, retention policy, and other characteristics. Administrators need a way to dynamically provision volumes of these different types to automatically meet user demand.
A new mechanism called &amp;lsquo;storage classes&amp;rsquo; is proposed to provide this capability.
Motivation In Kubernetes 1.2, an alpha form of limited dynamic provisioning was added that allows a single volume type to be provisioned in clouds that offer special volume types.</description>
    </item>
    
    <item>
      <title>volume-selectors</title>
      <link>/contributors/design-proposals/storage/volume-selectors/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/volume-selectors/</guid>
      <description>Abstract Real Kubernetes clusters have a variety of volumes which differ widely in size, iops performance, retention policy, and other characteristics. A mechanism is needed to enable administrators to describe the taxonomy of these volumes, and for users to make claims on these volumes based on their attributes within this taxonomy.
A label selector mechanism is proposed to enable flexible selection of volumes by persistent volume claims.
Motivation Currently, users of persistent volumes have the ability to make claims on those volumes based on some criteria such as the access modes the volume supports and minimum resources offered by a volume.</description>
    </item>
    
    <item>
      <title>volume-snapshotting</title>
      <link>/contributors/design-proposals/storage/volume-snapshotting/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/volume-snapshotting/</guid>
      <description>Kubernetes Snapshotting Proposal Authors: Cindy Wang
Background Many storage systems (GCE PD, Amazon EBS, etc.) provide the ability to create &amp;ldquo;snapshots&amp;rdquo; of a persistent volumes to protect against data loss. Snapshots can be used in place of a traditional backup system to back up and restore primary and critical data. Snapshots allow for quick data backup (for example, it takes a fraction of a second to create a GCE PD snapshot) and offer fast recovery time objectives (RTOs) and recovery point objectives (RPOs).</description>
    </item>
    
    <item>
      <title>volume-topology-scheduling</title>
      <link>/contributors/design-proposals/storage/volume-topology-scheduling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/volume-topology-scheduling/</guid>
      <description>Volume Topology-aware Scheduling Authors: @msau42, @lichuqiang
This document presents a detailed design for making the default Kubernetes scheduler aware of volume topology constraints, and making the PersistentVolumeClaim (PVC) binding aware of scheduling decisions.
Definitions  Topology: Rules to describe accessibility of an object with respect to location in a cluster. Domain: A grouping of locations within a cluster. For example, &amp;lsquo;node1&amp;rsquo;, &amp;lsquo;rack10&amp;rsquo;, &amp;lsquo;zone5&amp;rsquo;. Topology Key: A description of a general class of domains.</description>
    </item>
    
    <item>
      <title>volume_stats_pvc_ref</title>
      <link>/contributors/design-proposals/instrumentation/volume_stats_pvc_ref/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/instrumentation/volume_stats_pvc_ref/</guid>
      <description>Add PVC reference in Volume Stats Background Pod volume stats tracked by kubelet do not currently include any information about the PVC (if the pod volume was referenced via a PVC)
This prevents exposing (and querying) volume metrics labeled by PVC name which is preferable for users, given that PVC is a top-level API object.
Proposal Modify VolumeStats tracked in Kubelet and populate with PVC info:
// VolumeStats contains data about Volume filesystem usage.</description>
    </item>
    
    <item>
      <title>volumes</title>
      <link>/contributors/design-proposals/storage/volumes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/design-proposals/storage/volumes/</guid>
      <description>Abstract A proposal for sharing volumes between containers in a pod using a special supplemental group.
Motivation Kubernetes volumes should be usable regardless of the UID a container runs as. This concern cuts across all volume types, so the system should be able to handle them in a generalized way to provide uniform functionality across all volume types and lower the barrier to new plugins.
Goals of this design:</description>
    </item>
    
    <item>
      <title>vote_for_justinsb</title>
      <link>/events/elections/2017/vote_for_justinsb/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/elections/2017/vote_for_justinsb/</guid>
      <description>Vote for justinsb! You can see my full &amp;ldquo;manifesto&amp;rdquo; here; we&amp;rsquo;ve been asked to provide a shorter summary here.
This election is critical. The steering committee is not an honorary position; it has effectively unlimited powers. Serving is a duty, not a privilege, and I humbly ask for your vote.
I have been involved with Kubernetes since before 1.0, primarily on AWS support (I am a lead of sig-aws), but also contributed the original multi-zone &amp;amp; NodePort support.</description>
    </item>
    
    <item>
      <title>watch_latency</title>
      <link>/sig-scalability/slos/watch_latency/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/sig-scalability/slos/watch_latency/</guid>
      <description>Watch latency SLI details User stories  As an administrator, if Kubernetes is slow, I would like to know if the root cause of it is slow api-machinery (slow watch) or something farther the path (lack of network bandwidth, slow or cpu-starved controllers, &amp;hellip;)  Other notes  Pretty much all control loops in Kubernetes are watch-based. As a result slow watch means slow system in general. Note that how we measure it silently assumes no clock-skew in case of cluster with multiple masters.</description>
    </item>
    
    <item>
      <title>welcome-to-kubernetes-new-developer-guide</title>
      <link>/contributors/devel/welcome-to-kubernetes-new-developer-guide/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/welcome-to-kubernetes-new-developer-guide/</guid>
      <description>This page has moved to:
https://git.k8s.io/community/contributors/guide/</description>
    </item>
    
    <item>
      <title>whats-up-with-sig-cluster-lifecycle</title>
      <link>/events/2017/12-contributor-summit/whats-up-with-sig-cluster-lifecycle/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/events/2017/12-contributor-summit/whats-up-with-sig-cluster-lifecycle/</guid>
      <description>What&amp;rsquo;s Up with SIG-Cluster-Lifecycle Notes by @jberkus
Luxas talking: I can go over what we did last year, but I&amp;rsquo;d like to see your ideas about what we should be doing for the future, especially around hosting etc.
How can we make Kubeadm beta for next year? Opinions: * HA * etcd-multihost * some solution for apiserver, controller * Self-hosted Kubeadm
Q: Can someone write a statement on purpose &amp;amp; scope of Kubeadm?</description>
    </item>
    
    <item>
      <title>writing-a-getting-started-guide</title>
      <link>/contributors/devel/writing-a-getting-started-guide/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/writing-a-getting-started-guide/</guid>
      <description>Writing a Getting Started Guide This page gives some advice for anyone planning to write or update a Getting Started Guide for Kubernetes. It also gives some guidelines which reviewers should follow when reviewing a pull request for a guide.
A Getting Started Guide is instructions on how to create a Kubernetes cluster on top of a particular type(s) of infrastructure. Infrastructure includes: the IaaS provider for VMs; the node OS; inter-node networking; and node Configuration Management system.</description>
    </item>
    
    <item>
      <title>writing-good-e2e-tests</title>
      <link>/contributors/devel/writing-good-e2e-tests/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contributors/devel/writing-good-e2e-tests/</guid>
      <description>Writing good e2e tests for Kubernetes Patterns and Anti-Patterns Goals of e2e tests Beyond the obvious goal of providing end-to-end system test coverage, there are a few less obvious goals that you should bear in mind when designing, writing and debugging your end-to-end tests. In particular, &amp;ldquo;flaky&amp;rdquo; tests, which pass most of the time but fail intermittently for difficult-to-diagnose reasons are extremely costly in terms of blurring our regression signals and slowing down our automated merge queue.</description>
    </item>
    
    <item>
      <title>zoom-guidelines</title>
      <link>/communication/zoom-guidelines/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/communication/zoom-guidelines/</guid>
      <description>Zoom Guidelines Zoom is the main video communication platform for Kubernetes. It is used for running the community meeting and SIG meetings. Since the Zoom meetings are open to the general public, a Zoom host has to moderate a meeting if a person is in violation of the code of conduct.
These guidelines are meant as a tool to help Kubernetes members manage their Zoom resources. Check the main moderation page for more information on other tools and general moderation guidelines.</description>
    </item>
    
  </channel>
</rss>